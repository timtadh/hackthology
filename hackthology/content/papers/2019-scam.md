Title: Evaluating Automatic Fault Localization Using Markov Processes
Author: <a href="http://hackthology.com">Tim Henderson</a>, Yiğit Küçük, and <a href="http://engineering.case.edu/profiles/hap">Andy Podgurski</a>
Citation: <strong>Tim A. D. Henderson</strong>, Yiğit Küçük, and Andy Podgurski. <i>Evaluating Automatic Fault Localization Using Markov Processes</i>. ICST 2018.
CiteAuthor: <strong>Tim A. D. Henderson</strong>, Yiğit Küçük, and Andy Podgurski
Publication: SCAM 2019
Date: 2019-09-30
Category: Paper
MainPage: show


**Tim A. D. Henderson**, Yiğit Küçük, and Andy Podgurski
*Evaluating Automatic Fault Localization Using Markov Processes*.  [SCAM 2019](http://www.ieee-scam.org/2019/).
<br/>
[DOI](http://tba).
[PDF]({filename}/pdfs/scam-2019.pdf).
[SUPPLEMENT]({filename}/pdfs/scam-2019-supplement.pdf).
[WEB]({filename}/papers/2019-scam.md).

<h4>Note</h4>
> This is a conversion from a latex paper I wrote. If you want all formatting
> correct you should read the
> [pdf version]({filename}/pdfs/scam-2019.pdf).

#### Abstract

<p>
Statistical fault localization (SFL) techniques are commonly compared and
evaluated using a measure known as "Rank Score" and its associated evaluation
process.  In the latter process each SFL technique under comparison is used to
produce a list of program locations, ranked by their suspiciousness scores.
Each technique then receives a Rank Score for each faulty program it is applied
to, which is equal to the rank of the first faulty location in the
corresponding list. The SFL technique whose average Rank Score is lowest is
judged the best overall, based on the assumption that a programmer will examine
each location in rank order until a fault is found.  However, this assumption
*oversimplifies* how an SFL technique would be used in practice.
Programmers are likely to regard suspiciousness ranks as just one source of
information among several that are relevant to locating faults. This paper
provides a new evaluation approach using first-order Markov models of debugging
processes, which can incorporate multiple additional kinds of information,
e.g., about code locality, dependences, or even intuition.  Our approach,
<span><span class="math inline">\( \textrm{HT}_{\textrm{Rank}} \)</span></span>, scores SFL techniques based on the expected number of steps a
programmer would take through the Markov model before reaching a faulty
location. Unlike previous evaluation methods, <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> can compare techniques
even when they produce fault localization reports differing in structure or
information granularity.  To illustrate the approach, we present a case study
comparing two existing fault localization techniques that produce results
varying in form and granularity.
</p>




<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h1 id="introduction">Introduction</h1>
<p>Automatic fault localization is a software engineering technique to assist a programmer during the debugging process by suggesting "suspicious" locations that may contain or overlap with a fault (bug, defect) that is the root cause of observed failures. The big idea behind automatic fault localization (or just fault localization) is that pointing the programmer towards the right area of the program will enable them to find the relevant fault more quickly.</p>
<p>A much-investigated approach to fault localization is <em>Coverage-Based Statistical Fault Localization</em> (CBSFL), which is also known as <em>Spectrum-Based Fault Localization</em> <span class="citation">[<a href="#ref-Jones2002">1</a>]-[<a href="#ref-Sun2016">3</a>]</span>. This approach uses code-coverage profiles and success/failure information from testing or field use of software to rank statements or other generic program locations (e.g., basic blocks, methods, or classes) from most "suspicious" (likely to be faulty) to least suspicious. To perform CBSFL, each test case is run using a version of the program being debugged that has been instrumented to record which potential fault locations were actually executed on that test. A human or automated <em>test oracle</em> labels each test to indicate whether it passed or failed. The coverage profiles are also referred to as <em>coverage spectra</em>.</p>
<p>In the usage scenario typically envisioned for CBSFL, a programmer uses the ranked list of program locations to guide debugging. Starting at the top of the list and moving down, they examine each location to determine if it is faulty. If the location of a fault is near the top of the list, the programmer saves time by avoiding consideration of most of the non-faulty locations in the program. However, if there is no fault near the top of the list, the programmer instead wastes time examining many more locations than necessary. CBSFL techniques are typically evaluated empirically in terms of their ability to rank faulty locations near the top of the list <span class="citation">[<a href="#ref-Jones2005">4</a>], [<a href="#ref-Pearson2017">5</a>]</span>, as measured by each technique's "Rank Score", which is the location of the first faulty location in the list. A CBSFL technique that consistently ranks faulty statements from a wide range of faulty programs near the top of the corresponding lists is considered a good technique.</p>
<p>One pitfall of using the ranked-list evaluation regime outlined above is that it can be applied fairly only when the techniques being compared provide results as a prioritized list of program elements of the same granularity. This means that if technique A produces a prioritized list of basic blocks, technique B produces an unordered set of sub-expressions, and technique C produces a prioritized list of classes then it is not valid to use the Standard Rank Score to compare them. The Standard Rank Score can only be applied to ordered lists and thus cannot be used directly to evaluate technique B, and it requires the techniques being compared to have the same granularity. Our new evaluation metric (called <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>) accounts for these differences in granularity and report construction, allowing a direct comparison between different styles of fault localization. We present a case study in Section 27 comparing <em>behavioral</em> fault localization (which produces fragments of the dynamic control flow graph) to standard CBSFL.</p>
<p>Second, it is evident that the imagined usage scenario for CBSFL, in which a programmer examines each possible fault location in rank order until a fault is found, is an oversimplification of programmer behavior <span class="citation">[<a href="#ref-Parnin2011">6</a>]</span>. Programmers are likely to deviate from this scenario, e.g.: by choosing not to re-examine locations that have already been carefully examined and have not changed; by examining the locations around a highly ranked one, regardless of their ranks; by examining locations that a highly ranked location is dependent upon or that are dependent upon it <span class="citation">[<a href="#ref-Renieres2003">7</a>]</span>; by employing a conventional debugger (such as gdb, WinDB, or Visual Studio's debugger); or simply by using their knowledge and intuition about the program.</p>
<p>To support more flexible and nuanced evaluation criteria for CBSFL and other fault localization techniques, we present a new approach to evaluating them that is based on constructing and analyzing first-order Markov models of debugging processes and that uses a new evaluation metric (<span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>) based on the "hitting time" of a Markov process. This approach allows researchers to directly compare different fault localization techniques by incorporating their results and other relevant information into an appropriate model. To illustrate the approach, we present models for two classes of fault localization techniques: CBSFL and Suspicious Behavior Based Fault Localization (SBBFL) <span class="citation">[<a href="#ref-Cheng2009a">8</a>], [<a href="#ref-Henderson2018">9</a>]</span>. The models we present are also easy to update, allowing researchers to incorporate results of future studies of programmers' behavior during debugging.</p>
<p>Our new debugging model (and its <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> metric) can be thought of as a first-order simulation of the debugging process as conducted by a programmer. As such, we intend for it to be a practical alternative to conducting an expensive user study. The model is capable of incorporating a variety of behaviors a programmer may exhibit while debugging, allowing researchers to evaluate the performance of their tool against multiple debugging "styles."</p>
<h1 id="background-and-related-work">Background and Related Work</h1>
<p>Coverage Based Statistical Fault Localization (CBSFL) <span class="citation">[<a href="#ref-Jones2002">1</a>], [<a href="#ref-Jones2005">4</a>]</span> techniques attempt to quantify the likelihood that individual program locations are faulty using sample statistics, called <em>suspiciousness metrics</em> or <em>fault localization metrics</em>, which are computed from PASS/FAIL labels assigned to test executions and from coverage profiles (coverage spectra) collected from those executions. A CBSFL suspiciousness metric (of which there are a great many <span class="citation">[<a href="#ref-Lucia2014">2</a>], [<a href="#ref-Sun2016">3</a>]</span>) measures the statistical association between the occurrence of test failures and the coverage of individual program locations (program elements) of a certain kind.</p>
<p>Some statistical fault localization techniques use additional information beyond basic coverage information to either improve accuracy or provide more explainable results. For instance, work on <em>Causal Statistical Fault Localization</em> uses information about the execution of program dependence predecessors of a target statement to adjust for confounding bias that can distort suspiciousness scores <span class="citation">[<a href="#ref-Baah2010">10</a>]</span>. By contrast, <em>Suspicious-Behavior-Based Fault Localization</em> (SBBFL) techniques use runtime control-flow information (the behavior) to identify groups of "collaborating" suspicious elements <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span>. These techniques typically leverage data mining techniques <span class="citation">[<a href="#ref-Aggarwal2014">11</a>]</span> such as frequent pattern mining <span class="citation">[<a href="#ref-Agrawal1993">12</a>]-[<a href="#ref-Aggarwal2014a">14</a>]</span> or significant pattern mining <span class="citation">[<a href="#ref-Henderson2018">9</a>], [<a href="#ref-Yan2008">15</a>]</span>. Unlike CBSFL techniques, SBBFL techniques output a ranked list of <em>patterns</em> (subgraphs, itemsets, trees) which each contain multiple program locations. This makes it difficult to directly compare SBBFL and CBSFL techniques using traditional evaluation methods.</p>
<p>Finally, a variety of non-statistical (or hybrid) approaches to fault localization have been explored <span class="citation">[<a href="#ref-Abreu2006">16</a>]-[<a href="#ref-Wong2016">19</a>]</span>. These approaches range from delta debugging <span class="citation">[<a href="#ref-Zeller1999">20</a>]</span> to nearest neighbor queries <span class="citation">[<a href="#ref-Renieres2003">7</a>]</span> to program slicing <span class="citation">[<a href="#ref-Tip1995">21</a>], [<a href="#ref-Mao2014">22</a>]</span> to information retrieval <span class="citation">[<a href="#ref-Marcus2004">23</a>]-[<a href="#ref-Le2015">25</a>]</span> to test case generation <span class="citation">[<a href="#ref-Artzi2010">26</a>]-[<a href="#ref-Perez2014">28</a>]</span>. Despite technical and theoretical differences in these approaches, they all suggest locations (or groups of locations) for programmers to consider when debugging.</p>
<h2 id="the-tarantula-evaluation">The Tarantula Evaluation</h2>
<p>Some of the earliest papers on fault localization do not provide a quantitative method for evaluating performance (as is seen in later papers <span class="citation">[<a href="#ref-Pearson2017">5</a>]</span>). For instance, the earliest CBSFL paper <span class="citation">[<a href="#ref-Jones2002">1</a>]</span>, by Jones <em>et al.</em>, proposes a technique and evaluates it qualitatively using data visualization. At the time, this was entirely appropriate as Jones was proposing a technique for visualizing the relative suspiciousness of different statements, as estimated with what is now called a suspiciousness metric (Tarantula). The visualization used for evaluating this technique aggregated the visualizations for all of the subject programs included in the study.</p>
<p>While the evaluation method used in the Jones <em>et al.</em> paper <span class="citation">[<a href="#ref-Jones2002">1</a>]</span> effectively communicated the potential of CBSFL (and interested many researchers in the idea) it was not good way to compare multiple fault localization techniques. In 2005 Jones and Harrold <span class="citation">[<a href="#ref-Jones2005">4</a>]</span> published a study that compared their Tarantula technique to three other techniques: Set Union and Intersection <span class="citation">[<a href="#ref-Agrawal1995">29</a>]</span>, Nearest Neighbor <span class="citation">[<a href="#ref-Renieres2003">7</a>]</span>, and Cause-Transitions <span class="citation">[<a href="#ref-Cleve2005">30</a>]</span>. These techniques involved different approaches toward the fault localization problem and originally had been evaluated in different ways. Jones and Harrold re-evaluated all of the techniques under a new common evaluation framework.</p>
<p>In their 2005 paper, Jones and Harrold evaluated the effectiveness of each fault localization technique by using it to rank the statements in each subject program version from most likely to be the root cause of observed program failures to least likely. For their technique Tarantula, the statements were ranked using the Tarantula suspiciousness score.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> To compare the effectiveness of the techniques, another kind of score was assigned to each faulty version of each subject program. This score is based on the "rank score":</p>
<h4 id="definition.-tarantula-rank-score"><strong>Definition</strong>. Tarantula Rank Score <span class="citation">[<a href="#ref-Jones2005">4</a>]</span></h4>
<blockquote>
<p>Given a set of locations <span class="math inline">\(L\)</span> with their suspiciousness scores <span class="math inline">\(s(l)\)</span> for <span class="math inline">\(l
  \in L\)</span> the Rank Score <span class="math inline">\(r(l)\)</span> for a faulty location <span class="math inline">\(l \in L\)</span> is: <span class="math display">\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) \ge s(l) \right\} }\right|}}
  \end{aligned}\]</span></p>
</blockquote>
<p>For Set Union and Intersection, Nearest Neighbor, and Cause-Transitions, Jones and Harrold used an idea of Renieres and Reiss <span class="citation">[<a href="#ref-Renieres2003">7</a>]</span> and ranked a program's statements based on consideration of its System Dependence Graph (SDG) <span class="citation">[<a href="#ref-Horwitz1990">31</a>]</span>. The surrogate suspiciousness score of a program location <span class="math inline">\(L\)</span> is the inverse of the size of the smallest dependence sphere around <span class="math inline">\(L\)</span> that contains a faulty location. The surrogate scores are then used to calculate the Tarantula Rank Score (Def. 14).</p>
<p>In Jones's and Harrold's evaluation the authors did not use the Tarantula Rank Score directly but instead used a version of it that is normalized by program size:</p>
<h4 id="definition.-tarantula-effectiveness-score-expense"><strong>Definition</strong>. Tarantula Effectiveness Score (Expense) <span class="citation">[<a href="#ref-Jones2005">4</a>]</span></h4>
<blockquote>
<p>This score is the proportion of program locations that do <strong>not</strong> need to be examined to find a fault when the locations are examined in rank order. Formally, let <span class="math inline">\(n\)</span> be the total number of program locations, and let <span class="math inline">\(r(f)\)</span> be the Tarantula Rank Score (Def. 14) of the faulty location <span class="math inline">\(f\)</span>. Then the score is: <span class="math display">\[\begin{aligned}
    \frac{n-r(f)}{n}
  \end{aligned}\]</span></p>
</blockquote>
<p>Using the normalized effectiveness score, Jones and Harrold directly compared the fault localization effectiveness of the techniques they considered. They did this in two ways. First, they presented a table that bucketed all the buggy versions of all the programs by their Tarantula Effectiveness Scores (given as percentages). Second, they presented a figure that showed the same data as a cumulative curve.</p>
<p>The core ideas of Jones' and Harrold's Effectivness/Expense score now underlie most evaluations of CBSFL techniques. Faulty statements are scored, ranked, rank-scored, normalized, and then aggregated over all programs and versions to provide an overall representation of a fault localization method's performance (e.g., <span class="citation">[<a href="#ref-Steimann2013">53</a>]</span>, [<a href="#ref-Lucia2014">2</a>], [<a href="#ref-Sun2016">3</a>], [<a href="#ref-Wong2008">32</a>]-[<a href="#ref-Zheng2018">34</a>]</span>). However, some refinements have been made to both the Rank Score and the Effectiveness Score.</p>
<h2 id="the-implied-debugging-models">The Implied Debugging Models</h2>
<p>It is worth (re)stating here the debugging model implied in the Jones and Harrold evaluation <span class="citation">[<a href="#ref-Jones2005">4</a>]</span>. The programmer receives from the fault localization tool a ranked list of statements with the most suspicious statements at the top. The programmer then moves down the list examining each location in turn. If multiple statements have the same rank (the same suspiciousness score) all of those statements are examined before the programmer makes a determination on whether or not the bug has been located. This rule is captured in the mathematical definition of the Tarantula Rank Score (Definition 14).</p>
<p>For the non-CBSFL methods which Jones compared CBSFL against, the ranks of the program locations were once again compared using the method of Renieres and Reiss <span class="citation">[<a href="#ref-Renieres2003">7</a>], [<a href="#ref-Cleve2005">30</a>], [<a href="#ref-ChaoLiu2006">35</a>]</span> which is sometimes called <em>T-Score</em>. As a reminder, this method computes a surrogate suspiciousness score based on the size of smallest <em>dependence sphere</em><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> centered around the locations indicated in the fault localization report that contain the faulty code. This implies a debugging model in which the programmer examines each "shell" of the dependence sphere in turn before moving onto the next larger shell (see Figure 7 in <span class="citation">[<a href="#ref-Cleve2005">30</a>]</span> for a visualization).</p>
<p>Neither of these debugging models are realistic. Programmers may be reasonably expected to deviate from the ordering implied by the ranking. During the debugging process a programmer may use a variety of information sources — including intuition — to decide on the next element to examine. They may examine the same element multiple times. They may take a highly circuitous route to the buggy code or via intuition jump immediately to the fault. The models described above allow for none of these subtleties.</p>
<h2 id="refinements-to-the-evaluation-framework">Refinements to the Evaluation Framework</h2>
<p>Wong <em>et al.</em> <span class="citation">[<a href="#ref-Wong2008">32</a>]</span> introduced the most commonly used effectiveness score, which is called the <span class="math inline">\(\mathcal{EXAM}\)</span> score. This score is essentially the same as the Expense score (Def. 15) except that it gives the percentage of locations that need to be examined rather than those avoided.</p>
<h4 id="definition.-mathcalexam-score"><strong>Definition</strong>. <span class="math inline">\(\mathcal{EXAM}\)</span> Score <span class="citation">[<a href="#ref-Wong2008">32</a>]</span></h4>
<blockquote>
<p><span class="math display">\[\begin{aligned}
    \frac{r(f)}{n}
  \end{aligned}\]</span></p>
</blockquote>
<p>Ali <em>et al.</em> <span class="citation">[<a href="#ref-Ali2009">37</a>]</span> identified an important problem with Jones' and Harrold's evaluation method: some fault localization techniques always assign different locations distinct suspiciousness scores, but others do not. Ali <em>et al.</em> pointed out that when comparing techniques, the Tarantula Effectiveness Score may favor a technique that generates more distinct suspiciousness scores than the other techniques. The fix they propose is to assign to a location in a group of locations with the same suspiciousness score a rank score that reflects developers having to examine half the locations in the group on average.</p>
<h4 id="definition.-standard-rank-score"><strong>Definition</strong>. Standard Rank Score</h4>
<blockquote>
<p>This score is the expected number of locations a programmer would inspect before locating a fault. Formally, given a set of locations <span class="math inline">\(L\)</span> with their suspiciousness scores <span class="math inline">\(s(l)\)</span> for <span class="math inline">\(l \in L\)</span>, the Rank Score for a location <span class="math inline">\(l
  \in L\)</span> is <span class="citation">[<a href="#ref-Ali2009">37</a>]</span>: <span class="math display">\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &gt; s(l) \right\} }\right|}} +
    \frac{
      {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}}
    }{
      2
    }
  \end{aligned}\]</span> Note: when we refer to the "Standard Rank Score" this is the definition we are referring to.</p>
</blockquote>
<p>Parnin and Orso <span class="citation">[<a href="#ref-Parnin2011">6</a>]</span> conducted a study of programmers' actual use of a statistical fault localization tool (Tarantula <span class="citation">[<a href="#ref-Jones2002">1</a>]</span>). One of their findings was that programmers did not look deeply through the ranked list of locations and would instead only consider the first few locations. Consequently, they encouraged researchers to no longer report effectiveness scores as percentages. Most CBSFL studies now report absolute (non-percentage) rank scores. This is desirable for another reason: larger programs can have much larger absolute ranks than small programs, for the same percentage rank. Consider, for instance a program with 100,000 lines. If a fault's Rank Score is 10,000 its percentage Exam Score would be 10%. A 10% Exam Score might look like a reasonably good localization (and would be if the program had 100 lines) but no programmer will be willing to look through 10,000 lines. By themselves, percentage evaluation metrics (like Exam Score) produce inherently misleading results for large programs.</p>
<p>Steinmann <em>et al.</em> <span class="citation">[<a href="#ref-Steimann2013">53</a>]</span> identified a number of threats to validity in CBSFL studies, including: heterogeneous subject programs, poor test suites, small sample sizes, unclear sample spaces, flaky tests, total number of faults, and masked faults. For evaluation they used the Standard Rank Score of Definition 17 modified to deal with <span class="math inline">\(k\)</span> faults tied at the same rank.</p>
<h4 id="definition.-steinmann-rank-score"><strong>Definition</strong>. Steinmann Rank Score</h4>
<blockquote>
<p>This score is the expected number of locations a programmer would inspect before finding a fault when multiple faulty statements may have the same rank. Formally, given a set of locations <span class="math inline">\(L\)</span> with their suspiciousness scores <span class="math inline">\(s(l)\)</span> for <span class="math inline">\(l \in L\)</span>, the Rank Score for a location <span class="math inline">\(l \in L\)</span> is <span class="citation">[<a href="#ref-Steimann2013">53</a>]</span>: <span class="math display">\[\begin{aligned}
    &amp; {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &gt; s(l) \right\} }\right|}}\\
    &amp; + \frac{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}} + 1
        }{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \wedge x \text{ is a
          faulty location} \right\}}\right|}} + 1
        }
  \end{aligned}\]</span></p>
</blockquote>
<p>Moon <em>et al.</em> <span class="citation">[<a href="#ref-Moon2014">38</a>]</span> proposed Locality Information Loss (LIL) as an alternative evaluation framework. LIL models the localization result as a probability distribution constructed from the suspiciousness scores:</p>
<h4 id="definition.-lil-probability-distribution"><strong>Definition</strong>. LIL Probability Distribution</h4>
<blockquote>
<p>Let <span class="math inline">\(\tau\)</span> be a suspicious metric normalized to the <span class="math inline">\([0,1]\)</span> range of reals. Let <span class="math inline">\(n\)</span> be the number of locations in the program and let <span class="math inline">\(L = \{l_1,\ldots,
  l_n\}\)</span> be the set of locations. The constructed probability distribution is given by: <span class="math display">\[\begin{aligned}
    P_{\tau}(l_i) = \frac{\tau(l_i)}{\sum^{n}_{j=1} \tau(l_j)}
  \end{aligned}\]</span></p>
</blockquote>
<p>LIL uses the Kullback-Leibler measure of divergence between distributions to compute a score indicating how different the distribution constructed for a suspiciousness metric of interest is from the distribution constructed from an "ideal" metric, which gives a score of 1 to the faulty location(s) and gives negligible scores to every other location. The advantage of the LIL framework is that it does not depend on a list of ranked statements and can be applied to non-statistical methods (using a synthetic <span class="math inline">\(\tau\)</span>). The disadvantage of LIL is that it does not reflect programmer effort (as the Rank Scores do). However, it may be a better metric to use when evaluating fault localization systems as components of automated fault repair systems.</p>
<p>Pearson <em>et al.</em> <span class="citation">[<a href="#ref-Pearson2017">5</a>]</span> re-evaluated a number of previous results using new real world subject programs with real defects and test suites. In contrast to previous work they made use of statistical hypothesis testing and confidence intervals to characterize the uncertainty of the results. To evaluate the performance of each technique under study they used the <span class="math inline">\(\mathcal{EXAM}\)</span> score, reporting best, average, and worst case results for multi-statement faults.</p>
<h1 id="a-new-approach-to-evaluation">A New Approach to Evaluation</h1>
<p>Programmers consider multiple sources of information when performing debugging tasks and use them to guide their exploration of the source code. In our new approach to evaluating fault localization techniques, a model is constructed for each technique <span class="math inline">\(T\)</span> and each program <span class="math inline">\(P\)</span> of how a programmer using <span class="math inline">\(T\)</span> might move from examining one location in <span class="math inline">\(P\)</span> to examining another. The model for <span class="math inline">\(T\)</span> and <span class="math inline">\(P\)</span> is used to compute a statistical estimate of the expected number of moves a programmer using <span class="math inline">\(T\)</span> would make before encountering a fault in <span class="math inline">\(P\)</span>. This estimate is used to compute a "hitting-time rank score" for technique <span class="math inline">\(T\)</span>. The scores for all the techniques can then be compared to determine which performed best on program <span class="math inline">\(P\)</span>. This section presents the general approach and specific example models. The models make use of CBSFL reports and information about static program structure and dynamic control flow.</p>
<p>In order to support very flexible modeling and tractable analysis of debugging processes, we use first-order Markov chains (described below) to model them. Our first example models the debugging process assumed in previous work, in which a programmer follows a ranked list of suspicious program locations until a fault is found. Then we describe how to incorporate structural information about the program (which could influence a programmer's debugging behavior). Finally, we show how to model and compare CBSFL to a recent <em>Suspicious Behavioral Based Fault Localization</em> (SBBFL) algorithm <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span> that identifies suspicious subgraphs of dynamic control flow graphs. This third model demonstrates the flexibility of our approach and could be adapted to evaluate other SBBFL techniques <span class="citation">[<a href="#ref-Cheng2009a">8</a>], [<a href="#ref-Liu2005">39</a>]-[<a href="#ref-Yousefi2013">47</a>]</span>. It also demonstrates that our approach can be used to compare statistical and non-statistical fault localization techniques under the same assumptions about programmer debugging behavior.</p>
<p>It is important to emphasize that the quality and value of the evaluation results obtained with our approach depend primarily on the appropriateness of the model of the debugging process that is created. This model represents the evaluators' knowledge about likely programmer behavior during debugging and the factors that influence it. To avoid biasing their evaluation, evaluators must commit to an evaluation model and refrain from "tweaking" it after applying it to the data. <span class="citation">[<a href="#ref-Ioannidis2005">48</a>]</span>.</p>
<h2 id="background-on-ergodic-markov-chains">Background on Ergodic Markov Chains</h2>
<p>A finite state Markov chain consists of a set of <em>states</em> <span class="math inline">\(S = \{s_1, s_2,
..., s_n \}\)</span> and an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\({\bf P}\)</span>, called the <em>transition matrix</em> <span class="citation">[<a href="#ref-Grinstead2012">49</a>]</span>. Entry <span class="math inline">\({\bf P}_{i,j}\)</span> gives the probability for a <em>Markov process</em> in state <span class="math inline">\(s_i\)</span> to move to state <span class="math inline">\(s_j\)</span>. The probability of a Markov process moving from one state to another only depends on the state the process is currently in. This is known as the <em>Markov property</em>.</p>
<p>A Markov chain is said to be <em>ergodic</em> if, given enough steps, it can move from any state <span class="math inline">\(s_i\)</span> to any state <span class="math inline">\(s_j\)</span>, i.e. <span class="math inline">\({\textrm{Pr}\left[{s_i \xrightarrow{*} s_j}\right]} &gt; 0\)</span>. Thus, there are no states in an ergodic chain that the process can never leave.</p>
<p>Ergodic Markov chains have <em>stationary distributions</em>. Let <span class="math inline">\({\bf v}\)</span> be an arbitrary probability vector. The stationary distribution is a probability vector <span class="math inline">\({\bf w}\)</span> such that <span class="math display">\[\lim_{n \rightarrow \infty} {\bf v}{\bf P}^{n} = {\bf w}\]</span> The vector <span class="math inline">\({\bf w}\)</span> is a fixed point on <span class="math inline">\({\bf P}\)</span> implying <span class="math inline">\({\bf w}{\bf P} = {\bf
w}\)</span>. Stationary distributions give the long term behavior of a Markov chain - meaning that after many steps the chance a Markov process ends in state <span class="math inline">\(s_i\)</span> is given by <span class="math inline">\({\bf w}_i\)</span>.</p>
<p>The <em>expected hitting time</em> of a state in a Markov chain is the expected number of steps (transitions) a Markov process will make before it encounters the state for the first time. Our new evaluation metric (<span class="math inline">\(\text{HT}_{\text{Rank}}\)</span>) uses the expected hitting time of the state representing a faulty program location to score a fault localization technique's performance. Lower expected hitting times yield better localization scores.</p>
<h4 id="definition.-expected-hitting-time"><strong>Definition</strong>. Expected Hitting Time</h4>
<blockquote>
<p>Consider a Markov chain with transition matrix <span class="math inline">\({\bf P}\)</span>. Let <span class="math inline">\(T_{i,j}\)</span> be a random variable denoting the time at which a Markov process that starts at state <span class="math inline">\(s_i\)</span> reaches state <span class="math inline">\(s_j\)</span>. The expected hitting time (or just hitting time) of state <span class="math inline">\(s_j\)</span> for such a process is the expected value of <span class="math inline">\(T_{i,j}\)</span> <span class="math display">\[\begin{aligned}
    {\textrm{E}\left[{T_{i,j}}\right]} = \sum_{k=1}^{\infty} k \cdot {\textrm{Pr}\left[{T_{i,j}=k}\right]}
  \end{aligned}\]</span></p>
</blockquote>
<p>In general, a hitting time for a single state may be computed in <span class="math inline">\(\mathcal{O}(n^3)\)</span> steps <span class="citation">[<a href="#ref-Kemeny1960">50</a>]</span>. Somewhat less time is required for sparse transition matrices <span class="citation">[<a href="#ref-Davis2004">51</a>]</span>. Chapter 11 of Grinstead and Snell <span class="citation">[<a href="#ref-Grinstead2012">49</a>]</span> provides an accessible introduction to hitting time computation.</p>
<p>Some programs may have too many elements for exact hitting time computations (our case study contains one such program). To deal with large programs the expected hitting time can also be estimated by taking repeated random walks through the Markov chain to obtain a sample of hitting times. The sample can then be used to estimate the expected hitting time by computing the sample mean.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<h2 id="expected-hitting-time-rank-score-textrmht_textrmrank">Expected Hitting Time Rank Score (<span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>)</h2>

```go
    func (n *Node) Has(k int) bool {
        if n == nil {
            return false
++      } else if k == n.Key {
--      } else if k != n.Key {
            return true
        } else if (k < n.Key) {
            return n.left.Has(k)
        } else {
            return n.right.Has(k)
        }
    }
```

<div style="margin-right: 2em; margin-left: 2em;">
<strong>Listing 1.</strong>
A bug in the implementation of the "Has" method in an AVL tree.
</div>
<table>
<caption> <span style="font-variant: small-caps;">Table I: Reduced CBSFL Results for Listing 1</span> </caption>
<thead>
<tr class="header">
<th align="left">rank</th>
<th align="center">R. F1</th>
<th align="left">Function (Basic Block)</th>
<th align="left">rank</th>
<th align="center">R. F1</th>
<th align="left">Function (BB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1.5</td>
<td align="center">0.98</td>
<td align="left">Node.Has (2)</td>
<td align="left">6</td>
<td align="center">0.95</td>
<td align="left">Node.Has (3)</td>
</tr>
<tr class="even">
<td align="left">1.5</td>
<td align="center">0.98</td>
<td align="left">Node.String (3)</td>
<td align="left">7.5</td>
<td align="center">0.94</td>
<td align="left">main (1)</td>
</tr>
<tr class="odd">
<td align="left">1.5</td>
<td align="center">0.98</td>
<td align="left">Node.Verify (4)</td>
<td align="left">7.5</td>
<td align="center">0.94</td>
<td align="left">Scanner.Scan (24)</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="center">0.97</td>
<td align="left">Node.Has (4)</td>
<td align="left">7.5</td>
<td align="center">0.94</td>
<td align="left">Node.Verify (0)</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="center">0.97</td>
<td align="left">Node.Has (6)</td>
<td align="left"></td>
<td align="center"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p><img src="images/scam/markov-rank-list.png" alt="image" />
<div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;">
<strong>Fig. 1:</strong> A simplified version of the Markov model for evaluating the ranked list of suspicious locations for the bug in Listing 1.
</div>
</p>
<p>
<img src="images/scam/markov-rank-list-with-jumps.png" alt="image" />
<div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;">
<strong>Fig. 2:</strong> An example Markov model showing how “jump” edges can be added to represent how a programmer might examine locations which are near the location they are currently reviewing. Compare to Figure 1.
</div>
</p>
<p>This section introduces our new evaluation metric <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>. <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> produces a "rank score" similar to the score produced by the Standard Rank Score of Definition 17. In the standard score, program locations are ranked by their CBSFL suspiciousness scores. A location's position in the ordered list is that location's Rank Score (see the definition for details).</p>
<p>The new <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> score is obtained as follows:</p>
<ol>
<li><p>A Markov debugging model is supplied (as a Markov chain).</p></li>
<li><p>The expected hitting times (Def. 22) for each location in the program are computed.</p></li>
<li><p>The locations are ordered by their expected hitting times.</p></li>
<li><p>The <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> for a location is its position in the ordered list.</p></li>
</ol>
<h4 id="definition.-textrmht_textrmrank"><strong>Definition</strong>. <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span></h4>
<blockquote>
<p>Given a set of locations <span class="math inline">\(L\)</span> and a Markov chain <span class="math inline">\((S, {\bf P})\)</span> that represents the debugging process and has start state <span class="math inline">\(0\)</span>, the Hitting-Time Rank Score <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> for a location <span class="math inline">\(l \in L\cap S\)</span> is: <span class="math display">\[\begin{aligned}
    &amp; {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} &lt; {\textrm{E}\left[{T_{0,l}}\right]}
        \right\}
    }\right|}} +
    \\
    &amp; \frac{
        {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} = {\textrm{E}\left[{T_{0,l}}\right]} \right\}
        }\right|}}
    }{
      2
    }
  \end{aligned}\]</span> Note: this is almost identical to Definition 17, but it replaces the suspiciousness score with the expected hitting time. Definition 18 can also be modified in a similar way for multi-fault programs.</p>
</blockquote>
<h2 id="markov-debugging-models">Markov Debugging Models</h2>
<p><span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> is parameterized by a "debugging model" expressed as a Markov chain. As noted above, a Markov chain is made up a of set of states <span class="math inline">\(S\)</span> and transition matrix <span class="math inline">\({\bf P}\)</span>. In a debugging model, there are two types of states: 1) textual locations in the source code of a program and 2) synthetic states. Figures 6 and 7 show examples of debugging models constructed for an implementation of an AVL tree. In the figures, the square nodes are Markov states representing basic blocks in the source code. (Note that CBSFL techniques typically compute the same suspiciousness score for all statements in a basic block.) The smaller, circular nodes are Markov states which are synthetic. They are there for structural reasons but do not represent particular locations in the source code. The edges in the graphs represent possible transitions between states, and they are annotated with transition probabilities. All outgoing edges from a node should sum to 1.</p>
<p>In a Markov debugging model a programmer is represented by the Markov process. When the Markov process is simulated the debugging actions of a programmer are being simulated. This is a "first order" simulation, which means the actions of the simulated programmer (Markov process) only depend on the current location being examined. Thus, the Markov model provides a simple and easy-to-construct mathematical model of a programmer looking through the source code of a program to find the faulty statement(s). The simulated programmer begins at some starting state and moves from state to state until the faulty location is found. We require that all Markov models are <em>ergodic</em>, ensuring that every state (program location) is eventually reachable in the model.</p>
<h2 id="an-extensible-markov-model-for-cbsfl">An Extensible Markov Model for CBSFL</h2>
<p>As described in Section 12 a CBSFL report is made up of a ranked list of locations in the subject program. Our extensible Markov model includes a representation of the CBSFL ranked list. By itself, using <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> with a Markov model of a ranked list is just a mathematically complex way of restating Definition 17. However, with a Markov model of a CBSFL report in hand we can add further connections <em>between</em> program locations (represented as Markov states) to represent other actions a programmer might take besides traversing down the ranked list. For instance, in Section 25 we note that programmers use graphical debuggers to traverse the dynamic control flow in a program - allowing them to visit statements in the order they are executed. We embed the dynamic control flow graph into the transition matrix of the Markov model to reflect this observation.</p>
<h3 id="a-ranked-list-as-a-markov-chain">A Ranked List as a Markov Chain</h3>
<p>Figure 6 provides a graphical example of a Markov chain for a ranked list. Since the nodes in a graphical representation of a Markov chain represent states and the edges represent the transition matrix, the probabilities associated with outgoing edges of each node should sum to 1. In Figure 6, each circular node represents a rank in the list and the square nodes represent associated program locations, which are identified by their function names and static basic-block id number. The square nodes that are grouped together all have the same suspiciousness scores. We will provide here a brief, informal description of the structure of the transition matrix. A formal description of the chain is provided in Definition 28 in the Appendix. The exact choice of transition matrix in the formal chain was driven by a proof of equivalence between <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> with this Markov model (as defined in Definition 28) and the Standard Rank Score (Definition 17).<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>The transition matrix boils down to a couple of simple connections. The nodes representing groups form a doubly linked list (see the circular nodes in Figure 6). The ordering of the "group nodes" matches the ordering of the ranks in the ranked list. The links between the node are weighted so that a Markov process will tend to end up in a highly ranked node. More formally, the model was constructed so that if you ordered the Markov states by the probabilities in the <em>stationary distribution</em> (which characterizes the long term behavior of the Markov process) from highest to lowest that ordering would match the order of the ranked list.</p>
<p>The second type of connection is from a group node to its program location nodes. Each location node connects to exactly one group node. The transition probabilities (as shown in Figure 6) are set up such that there is an equal chance of moving to any of the locations in the group.</p>
<p>The final connection is from a location node back to its group node. This is always assigned probability <span class="math inline">\(1\)</span> (see Figure 6). Again, see Definition 28 in the Appendix for the formal description.</p>
<h3 id="adding-local-jumps-to-the-cbsfl-chain">Adding Local Jumps to the CBSFL Chain</h3>
<p>Figure 7 shows a modified version of the model in Figure 6. The modified model allows the Markov process to jump or "teleport" between program locations which are not adjacent in the CBSFL Ranked List. These "jump" connections are setup to model other ways a programmer might move through the source code of a program when debugging (see below). In the figure, these jumps are shown with added red and blue edges. For the Markov model depicted in the figure, the Markov process will with probability <span class="math inline">\(\frac{1}{2}\)</span> move back to the rank list and with probability <span class="math inline">\(\frac{1}{2}\)</span> teleport or jump to an alternative program location that is structurally adjacent (in the program's source code) to the current one.</p>
<p>Informally, the modified model is set up so that if the Markov process is in a state <span class="math inline">\(s_i\)</span> which represents program location <span class="math inline">\(l_x\)</span> it will with probability <span class="math inline">\(p_{\text{jump}}\)</span> move to a state <span class="math inline">\(s_j\)</span> which represents program location <span class="math inline">\(l_y\)</span> instead of returning to the rank list. The locations that the process can move to from <span class="math inline">\(s_i\)</span> are defined in a jump matrix <span class="math inline">\({\bf J}\)</span> which is parameter to the Markov model. The matrix <span class="math inline">\({\bf J}\)</span> encodes assumptions or observations about how programmers behave when they are debugging. A formal definition of the CBSFL chain with jumps is presented in the Appendix (see Definition 29).</p>
<p>Definition 26 defines one general-purpose jump matrix <span class="math inline">\({\bf
J}\)</span>. It encodes two assumptions about programmer behavior. First, when a programmer considers a location inside a function they will also potentially examine other locations in that function. Second, when a programmer is examining a location they may examine locations preceding or succeeding it the dynamic control flow (e.g., with the assistance of a graphical debugger). Definition 26 encodes both assumptions by setting the relevant <span class="math inline">\({\bf
J}_{i,j}\)</span> entries to <span class="math inline">\(1\)</span>.</p>
<h4 id="definition.-spacial-behavioral-jumps"><strong>Definition</strong>. Spacial + Behavioral Jumps</h4>
<blockquote>
<p><span class="math display">\[\begin{aligned}
    {\bf J}_{i,j} &amp;=
    \left\{
      \begin{array}{cll}
          1 &amp;
          \text{if}
          &amp;    \text{$s_i$ and $s_j$ represent locations} \\
          &amp; &amp;  \text{in the same function}
               \\
        \\
          1 &amp;
          \text{if}
          &amp;    \text{$s_i$ and $s_j$ are adjacent locations in the} \\
          &amp; &amp;  \text{program&#39;s dynamic control flow graph} \\
        \\
          0
          &amp; \text{otherwise}
      \end{array}
    \right.
  \end{aligned}\]</span></p>
</blockquote>
<p>In addition to <span class="math inline">\({\bf J}\)</span>, the new chain is parameterized by the probability <span class="math inline">\(p_{\text{jump}}\)</span> of a jump occurring when the process visits a state representing a program location. As <span class="math inline">\(p_{\text{jump}} \rightarrow 0\)</span> the transition matrix of new chain approaches the transition matrix for the chain in Definition 28 (see the Appendix). We suggest setting <span class="math inline">\(p_{\text{jump}}\)</span> to <span class="math inline">\(0.5\)</span> in the absence of data from a user study.</p>
<h2 id="modeling-sbbfl">Modeling SBBFL</h2>
<p>As noted earlier, Markov models can be constructed for alternative fault localization techniques. Suspicious Behavior Based Fault Localization (SBBFL) <span class="citation">[<a href="#ref-Cheng2009a">8</a>], [<a href="#ref-Henderson2018">9</a>]</span> techniques return a report containing a ranked list of subgraphs or subtrees. Each subgraph contains multiple program locations usually drawn from a dynamic control flow graph of the program. Comparing this output to CBSFL using the Standard Rank Score can be difficult as a location may appear multiple times in graphs returned by the SBBFL algorithm. However, <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> can produce an accurate and comparable score by utilizing the expected hitting times of the states representing the faulty location. Definition 30 in the Appendix provides an example Markov chain which models a ranked list of suspicious subgraphs. It can be extended (not shown due to space constraints) to add a Jump matrix in the manner of Definition 29 (see the Appendix).</p>
<h1 id="case-study">Case Study</h1>
<p>To illustrate our new approach to evaluation, we performed a case study in which it was used to evaluate several fault localization techniques of two different types: CBSFL <span class="citation">[<a href="#ref-Sun2016">3</a>], [<a href="#ref-Jones2005">4</a>]</span> and Suspicious-Behavior-Based Fault Localization (SBBFL) <span class="citation">[<a href="#ref-Cheng2009a">8</a>], [<a href="#ref-Henderson2018">9</a>]</span>. We investigated the following research questions:</p>
<ul>
<li><p><strong>RQ1</strong>: How Accurate is <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Estimation? (Table III)</p></li>
<li><p><strong>RQ2</strong>: Does it make a difference whether <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> or the Standard Rank Score is used? (Table IV, Figs. 5 and 4)</p></li>
</ul>
<p>We also considered an important general question about fault localization that a typical research study might investigate.</p>
<ul>
<li><p><strong>RQ3</strong>: Which kind of fault localization technique performs better, SBBFL or CBSFL? (Table IV)</p></li>
</ul>
<p>In order to use an SBBFL technique, more complex profiling data (dynamic control flow graphs or DCFGs) are needed than for CBSFL (which requires only coverage data). Therefore, a more specialized profiler was required for this study. We used Dynagrok (https://github.com/timtadh/dynagrok) <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span> - a profiling tool for the Go Programming Language. We also reused subject programs used in a previous study <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span> (see Table II). They are all real-world Go programs of various sizes that were injected with mutation faults. The test cases for the programs are all either real-world inputs or test cases from the system regression testing suites that are distributed with the programs. Six representative suspiciousness metrics were considered: Ochiai, F1, Jaccard, Relative Ochiai, Relative F1, and Relative Jaccard <span class="citation">[<a href="#ref-Sun2016">3</a>]</span>. These measures were all previously adapted to the SBBFL context <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span>.</p>

<table>
<caption> <span style="font-variant: small-caps;">Table II: Datasets used in the evaluation</span> </caption>
<thead>
<tr class="header">
<th align="left">Program</th>
<th align="right">L.O.C.</th>
<th align="right">Mutants</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">AVL (github.com/timtadh/dynagrok)</td>
<td align="right">483</td>
<td align="right">19</td>
<td align="left">An AVL tree</td>
</tr>
<tr class="even">
<td align="left">Blackfriday (github.com/russross/blackfriday)</td>
<td align="right">8,887</td>
<td align="right">19</td>
<td align="left">Markdown processor</td>
</tr>
<tr class="odd">
<td align="left">HTML (golang.org/x/net/html)</td>
<td align="right">9,540</td>
<td align="right">20</td>
<td align="left">An HTML parser</td>
</tr>
<tr class="even">
<td align="left">Otto (github.com/robertkrimen/otto)</td>
<td align="right">39,426</td>
<td align="right">20</td>
<td align="left">Javascript interpreter</td>
</tr>
<tr class="odd">
<td align="left"><span><code>gc</code></span> (go.googlesource.com/go)</td>
<td align="right">51,873</td>
<td align="right">16</td>
<td align="left">The Go compiler</td>
</tr>
</tbody>
</table>
<div style="margin-right: 2em; margin-left: 2em;">
<p>Note: The AVL tree is in the examples directory.</p>
</div>

<p>All applications of techniques were replicated to address random variation in the results as the SBBFL algorithm that we used employs sampling <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span>. The results from the replications were averaged and, unless otherwise noted, the average Rank Scores are presented. Finally, the exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> score was computed for 4 of the 5 programs. For the fifth program, the Go compiler, although <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> can be computed exactly, this requires so much time for each run to complete (<span class="math inline">\(\approx\)</span> 4 hours) that it precluded computing exact results for each program version (including all models for both SBBFL and CBSFL). Therefore, expected hitting times for the Go compiler were estimated using the method outlined in Section 21. Specifically, we collected 500 samples of hitting times of all states in the Markov debugging model by taking random walks (with a maximum walk length of 1,000,000 steps). The expected hitting times were estimated by taking the sample mean.</p>
<h2 id="the-chosen-textrmht_textrmrank-model">The Chosen <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Model</h2>
<p>To avoid bias, it was important to choose the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> model before the case study was conducted, which we did. We used <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> with jumps (Definition 29) together with the jump matrix specified in Definition 26. We chose this matrix because we believe the order in which programmers examine program elements during debugging is driven in part by the structure of the program, even when they are debugging with the assistance of a fault localization report. The <span class="math inline">\(p_{\text{jump}}\)</span> probability was set to <span class="math inline">\(0.5\)</span> to indicate an equal chance of the programmer using or not using the fault localization report. A future large scale user study could empirically characterize the behavior of programmers while performing debugging to inform the choice of the <span class="math inline">\(p_{\text{jump}}\)</span> parameter. However, without such a study it is reasonable to use <span class="math inline">\(0.5\)</span>, which indicates "no information."</p>
<h2 id="rq1-how-accurate-is-textrmht_textrmrank-estimation">RQ1: How Accurate is <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Estimation?</h2>
<table>
<caption> <span style="font-variant: small-caps;"><span><span class="math inline">Table III: \(\textrm{HT}_{\textrm{Rank}}\)</span></span> Estimation Error</span> </caption>
<thead>
<tr class="header">
<th align="left">Subject Program</th>
<th align="center">avl</th>
<th align="center">blackfriday</th>
<th align="center">html</th>
<th align="center">otto</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mean   median   stdev   p-value</td>
<td align="center">0.781</td>
<td align="center">0.469</td>
<td align="center">0.473</td>
<td align="center">0.476</td>
</tr>
</tbody>
</table>
<div style="margin-right: 2em; margin-left: 2em;">
<p>Percentage error of the estimated <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> versus the exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> (see note under Def 22). Letting <span class="math inline">\(y\)</span> be the exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> and <span class="math inline">\(\hat{y}\)</span> be the estimated <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>, the percentage error is <span class="math inline">\(\frac{|\hat{y} - y|}{y} * 100\)</span>. <span>-2.0em</span></p>
</div>


<p>For large programs the cost of computing the expected hitting times for <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> may be too high. Therefore, we have suggested estimating the expected hitting times (see note under Definition 22). To assess the accuracy of estimating the expected hitting time instead of computing it exactly, we did both for four subject programs, using the Relative F1 measure. For each program version, SFL technique, and Markov chain type the estimation error was computed as a percentage of the true value. Table III presents descriptive statistics characterizing these errors for each subject program. The last row of the table gives p-values from a independent two-sample T-test comparing the estimated <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> values and the exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> values. The null hypothesis is that the expected estimate <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> is the same as the expected exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>. All p-values are well above the <span class="math inline">\(0.05\)</span> significance level that would suggest rejecting the null hypothesis. Therefore, we accept the null hypotheses that the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> estimates are not significantly different from the exact <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> values. As indicated in the table, the maximum estimation error was around 3.7%. Therefore, if estimation is used we recommend considering <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> scores that are within 5% of each other to be equivalent.</p>
<h2 id="rq2-does-it-make-a-difference-whether-textrmht_textrmrank-or-the-standard-rank-score-is-used">RQ2: Does it make a difference whether <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> or the Standard Rank Score is used? </h2>
<table>
<caption> <span style="font-variant: small-caps;">Table IV: Fault Localization Performance</span> </caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"></th>
<th align="center" colspan=3>Standard Rank Score</th>
<th align="center" colspan=3>HTRank Score</th>
</tr>
</thead>
<tbody>
<th align="left">score</td>
<th align="left">subject</td>
<th align="center">CBSFL</td>
<th align="center">SBBFL</td>
<th align="center"> %<span class="math inline">\(\Delta\)</span> </td>
<th align="center">CBSFL</td>
<th align="center">SBBFL</td>
<th align="center"> %<span class="math inline">\(\Delta\)</span> </td>
</tr>
<tr class="even">
<td align="left">RelativeF1</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">9.1</td>
<td align="center">4.9</td>
<td align="center">-47 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeF1</td>
<td align="left">blackfriday</td>
<td align="center">8.8</td>
<td align="center">4.4</td>
<td align="center">-50 %</td>
<td align="center">42.6</td>
<td align="center">11.0</td>
<td align="center">-74 %</td>
</tr>
<tr class="even">
<td align="left">RelativeF1</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">40.7</td>
<td align="center">25.3</td>
<td align="center">-38 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeF1</td>
<td align="left">otto</td>
<td align="center">8.2</td>
<td align="center">6.2</td>
<td align="center">-24 %</td>
<td align="center">102.2</td>
<td align="center">21.5</td>
<td align="center">-79 %</td>
</tr>
<tr class="even">
<td align="left">RelativeF1</td>
<td align="left">compiler</td>
<td align="center">264.8</td>
<td align="center">98.9</td>
<td align="center">-63 %</td>
<td align="center">1148.9</td>
<td align="center">1475.1</td>
<td align="center">28 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeOchiai</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">7.6</td>
<td align="center">6.3</td>
<td align="center">-16 %</td>
</tr>
<tr class="even">
<td align="left">RelativeOchiai</td>
<td align="left">blackfriday</td>
<td align="center">8.8</td>
<td align="center">4.4</td>
<td align="center">-50 %</td>
<td align="center">43.6</td>
<td align="center">9.8</td>
<td align="center">-78 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeOchiai</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">38.2</td>
<td align="center">22.4</td>
<td align="center">-41 %</td>
</tr>
<tr class="even">
<td align="left">RelativeOchiai</td>
<td align="left">otto</td>
<td align="center">8.7</td>
<td align="center">6.8</td>
<td align="center">-22 %</td>
<td align="center">99.2</td>
<td align="center">102.6</td>
<td align="center">3 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeOchiai</td>
<td align="left">compiler</td>
<td align="center">262.2</td>
<td align="center">101.6</td>
<td align="center">-61 %</td>
<td align="center">2888.8</td>
<td align="center">2984.1</td>
<td align="center">3 %</td>
</tr>
<tr class="even">
<td align="left">RelativeJaccard</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">10.1</td>
<td align="center">5.1</td>
<td align="center">-50 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeJaccard</td>
<td align="left">blackfriday</td>
<td align="center">16.0</td>
<td align="center">12.1</td>
<td align="center">-24 %</td>
<td align="center">80.0</td>
<td align="center">176.2</td>
<td align="center">120 %</td>
</tr>
<tr class="even">
<td align="left">RelativeJaccard</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">33.3</td>
<td align="center">24.4</td>
<td align="center">-27 %</td>
</tr>
<tr class="odd">
<td align="left">RelativeJaccard</td>
<td align="left">otto</td>
<td align="center">8.2</td>
<td align="center">6.3</td>
<td align="center">-23 %</td>
<td align="center">75.2</td>
<td align="center">19.9</td>
<td align="center">-74 %</td>
</tr>
<tr class="even">
<td align="left">RelativeJaccard</td>
<td align="left">compiler</td>
<td align="center">747.9</td>
<td align="center">482.0</td>
<td align="center">-36 %</td>
<td align="center">1074.1</td>
<td align="center">1540.5</td>
<td align="center">43 %</td>
</tr>
<tr class="odd">
<td align="left">F1</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">10.4</td>
<td align="center">5.1</td>
<td align="center">-51 %</td>
</tr>
<tr class="even">
<td align="left">F1</td>
<td align="left">blackfriday</td>
<td align="center">16.0</td>
<td align="center">12.1</td>
<td align="center">-24 %</td>
<td align="center">81.9</td>
<td align="center">218.8</td>
<td align="center">167 %</td>
</tr>
<tr class="odd">
<td align="left">F1</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">33.9</td>
<td align="center">18.6</td>
<td align="center">-45 %</td>
</tr>
<tr class="even">
<td align="left">F1</td>
<td align="left">otto</td>
<td align="center">8.2</td>
<td align="center">6.2</td>
<td align="center">-24 %</td>
<td align="center">81.0</td>
<td align="center">28.5</td>
<td align="center">-65 %</td>
</tr>
<tr class="odd">
<td align="left">F1</td>
<td align="left">compiler</td>
<td align="center">746.8</td>
<td align="center">470.4</td>
<td align="center">-37 %</td>
<td align="center">1047.0</td>
<td align="center">1537.9</td>
<td align="center">47 %</td>
</tr>
<tr class="even">
<td align="left">Ochiai</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">5.9</td>
<td align="center">6.3</td>
<td align="center">7 %</td>
</tr>
<tr class="odd">
<td align="left">Ochiai</td>
<td align="left">blackfriday</td>
<td align="center">14.8</td>
<td align="center">10.3</td>
<td align="center">-30 %</td>
<td align="center">51.2</td>
<td align="center">108.7</td>
<td align="center">112 %</td>
</tr>
<tr class="even">
<td align="left">Ochiai</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">30.7</td>
<td align="center">25.3</td>
<td align="center">-18 %</td>
</tr>
<tr class="odd">
<td align="left">Ochiai</td>
<td align="left">otto</td>
<td align="center">10.3</td>
<td align="center">8.4</td>
<td align="center">-19 %</td>
<td align="center">312.8</td>
<td align="center">607.7</td>
<td align="center">94 %</td>
</tr>
<tr class="even">
<td align="left">Ochiai</td>
<td align="left">compiler</td>
<td align="center">1091.6</td>
<td align="center">773.5</td>
<td align="center">-29 %</td>
<td align="center">3137.2</td>
<td align="center">2988.8</td>
<td align="center">-5 %</td>
</tr>
<tr class="odd">
<td align="left">Jaccard</td>
<td align="left">avl</td>
<td align="center">4.6</td>
<td align="center">2.0</td>
<td align="center">-56 %</td>
<td align="center">10.1</td>
<td align="center">5.1</td>
<td align="center">-50 %</td>
</tr>
<tr class="even">
<td align="left">Jaccard</td>
<td align="left">blackfriday</td>
<td align="center">16.0</td>
<td align="center">12.1</td>
<td align="center">-24 %</td>
<td align="center">77.8</td>
<td align="center">187.7</td>
<td align="center">141 %</td>
</tr>
<tr class="odd">
<td align="left">Jaccard</td>
<td align="left">html</td>
<td align="center">12.8</td>
<td align="center">5.0</td>
<td align="center">-61 %</td>
<td align="center">33.3</td>
<td align="center">24.6</td>
<td align="center">-26 %</td>
</tr>
<tr class="even">
<td align="left">Jaccard</td>
<td align="left">otto</td>
<td align="center">8.2</td>
<td align="center">6.2</td>
<td align="center">-24 %</td>
<td align="center">75.3</td>
<td align="center">24.7</td>
<td align="center">-67 %</td>
</tr>
<tr class="odd">
<td align="left">Jaccard</td>
<td align="left">compiler</td>
<td align="center">747.9</td>
<td align="center">485.4</td>
<td align="center">-35 %</td>
<td align="center">1078.8</td>
<td align="center">1435.6</td>
<td align="center">33 %</td>
</tr>
</tbody>
</table>
<div style="margin-right: 2em; margin-left: 2em;">
<p>Summarizes the fault localization performance for CBSFL and SBBFL. Each fault localization technique is evaluated using both the Standard Rank Score and the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Score (Defs. 28 and 25). The mean rank scores are shown as well as the percentage changes from CBSFL to SBBFL. Lower ranks scores indicate better fault localization performance. A negative percentage difference (%<span class="math inline">\(\Delta\)</span>) indicates SBBFL <strong>improved</strong> on CBSFL. A positive %<span class="math inline">\(\Delta\)</span> indicates CBSFL <strong>outperformed</strong> SBBFL.</p>
</div>

<p><img src="images/scam/model-cmp-otto.png" title="Fig: 3" alt="Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the HT_Rank Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1." />
<div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;">
<strong>Fig 3:</strong>
Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1. 
</div>
</p>

<p>
<img src="images/scam/cbsfl-vs-sbbfl-by-rank-score.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " />
<img style="margin-top: -2em;" src="images/scam/cbsfl-vs-sbbfl-by-htrank.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " />
<div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;">
<strong>Fig 4:</strong>
Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1.
</div>
</p>

<p>Table IV details the fault localization performance we observed for CBSFL and SBBFL under the Standard Rank Score and <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> using six different suspiciousness metrics. The results obtained with <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> were substantially different from those obtained with the Standard Rank Score, in terms of absolute ranks and percentage differences (%<span class="math inline">\(\Delta\)</span>). The mean Standard Rank Score for SBBFL is lower than that for CBSFL for <em>every</em> suspiciousness metric and subject program. By contrast, for some programs and metrics, the mean <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> scores for CBSFL are lower than those for SBBFL. The mean <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> scores are also higher than the mean Standard Rank Scores overall.</p>
<p>Another way to look at the same phenomenon is shown in Figure 5, which displays empirical probability density plots for the program Otto. Each plot compares the performance of CBSFL to SBBFL using a different evaluation method. The top plot uses the Standard Rank Score while the other plot uses <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>. As shown in the top plot, the Standard Rank Scores for both CBSFL and SBBFL are concentrated mainly between 0 and 15, with no values over 60. In the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> plot, by contrast, the scores for both CBSFL and SBBFL are much more widely dispersed. Figure 4 compares CBSFL to SBBFL with respect to average ranks (on a log scale), under the Standard Rank Score (top) and <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> (bottom), for each version of the program Otto. The suspiciousness metric is RelativeF1. The results for <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> are quite distict from those for the Standard Rank Score, with CBSFL showing much more variability under <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>.</p>
<p>These results provide confirmation for the theoretical motivation for <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>. Recall that one of the problems with the Standard Rank Score is that it does not account for either the differing structures of fault localization reports or differences in report granularity. SBBFL and CBSFL differ in both structure (ranked CFG fragments vs. ranked basic blocks) and granularity (multiple basic blocks vs. lone basic blocks). We expected the Standard Rank Score to unfairly favor SBBFL because it does not account for these differences and that is exactly what we see in Table IV. Under the Standard Rank Score SBBFL outperforms CBSFL on every program using every suspiciousness score.</p>
<p>To be explicit, SBBFL reports ranked CFG fragments, each of which contains multiple basic blocks. Under the Standard Rank Score those fragments are ranked and the score is the rank of the first fragment in which the bug appears. CBSFL will, by contrast, rank each basic block independently. Now, consider the case where SBBFL reports a CFG fragment at rank 1 that contains 10 basic blocks, one of which contains the bug. Suppose CBSFL also reports each of those basic blocks as maximally suspicious. The Standard Rank Score for CBSFL will be 5 while for SBBFL it will be 1. Thus, the Standard Rank Score is unfairly biased toward SBBFL and against CBSFL. This is once again reflected in Table IV. The new metric <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> does not suffer from this problem. As shown in the table and discussed below, SBBFL often but not always outperforms CBSFL under <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>, suggesting that the theoretical correction we expect is indeed occurring. We therefore conclude that <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> provides a better metric for comparison when reports differ in structure and granularity.</p>
<h2 id="rq3-which-kind-of-fault-localization-technique-performs-better-sbbfl-or-cbsfl">RQ3: Which kind of fault localization technique performs better, SBBFL or CBSFL?</h2>
<p>Referring once again to Table IV, the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> results indicate that SBBFL often but not always outperformed CBSFL. In particular, when the measure used was Relative F1 (which was found to be the best-performing measure for SBBFL in <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span>), SBBFL performed better for all programs but the compiler. However, when Ochiai was used CBSFL outperformed SBBFL, although CBSFL with Relative F1 outperforms CBSFL with Ochiai. This indicates that SBBFL and Relative F1 may be the best combination tested with one caveat. For the compiler, SBBFL never outperforms CBSFL. However, CBSFL also performs very badly on this large program. This indicates that while CBSFL beats SBBFL on this program neither technique is effective at localizing faults in it.</p>
<h2 id="study-limitations">Study Limitations</h2>
<p>The purpose of our case study is to illustrate the application of <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span>. It was not designed to settle controversies about the suspiciousness metrics we employed. Second, this study used real programs and tests but used synthetic bugs. In the future, we intend to provide an automated analysis system that builds upon the Defects4J dataset <span class="citation">[<a href="#ref-Just2014">52</a>]</span>. Third, our study included a representative but not exhaustive set of suspiciousness metrics <span class="citation">[<a href="#ref-Lucia2014">2</a>]</span> and may not generalize to other metrics.</p>
<p>Finally, no user-study was performed to validate the chosen debugging model against actions an actual programmer would take. Although we would have liked to perform such a study at this time, our resources do not permit us to do so. To be conclusive, such a study would need to be large-scale and to utilize professional programmers who are skilled in traditional debugging and are able to spend significant time learning to use SFL techniques with comparable skill.</p>
<h1 id="discussion">Discussion</h1>
<p>A new, flexible approach to evaluating automatic fault localization techniques was presented. The new <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Score provides a principled and flexible way of comparing different fault localization techniques. It is robust to differences in granularity and allows complex fault localization reports (such as those produced by SBBFL) to be incorporated. Unlike previous attempts at cross-technique comparison (see Section 12) the final scores are based on the expected number of steps through a Markov model. The model can incorporate both information from the chosen fault localization technique as well as other information available to the programmer (such as the structure of the program). The <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Score is sensitive to the model used (see Fig 5) and hence the choice of model is important. The choice should be made based on 1) the fault localization technique(s) being evaluated and 2) observations of programmer behavior. Choosing a model after viewing the results (and picking the model that gives the "best" results) leads to a biased outcome.</p>
<h2 id="recommendations-for-researchers" class="unnumbered">Recommendations for Researchers</h2>
<ol>
<li><p>Report both Standard Rank Score and <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Score.</p></li>
<li><p>If evaluating multi-line faults the Steinmann Rank Score (Def. 18) should be used as the basis for defining the <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> Score.</p></li>
<li><p>Report which model is being used and why it was chosen, and include a description of the model.</p></li>
<li><p>In the absence of user study data, set <span class="math inline">\(p_{\text{jump}} = .5\)</span> and set the weights in <span class="math inline">\({\bf J}\)</span> uniformly.</p></li>
</ol>
<p>By evaluating fault localization methods with <span><span class="math inline">\(\textrm{HT}_{\textrm{Rank}}\)</span></span> in addition to the Standard Rank Score researchers will be able to make valid cross-technique comparisons. This will enable the community to better understand the relationships between techniques and report-granularity while taking into account potential programmer behavior during debugging.</p>
<h1 id="acknowledgement" class="unnumbered">Acknowledgement</h1>
<p>This work was partially supported by NSF award CCF-1525178 to Case Western Reserve University.</p>
<h1 id="markov-chain-definitions">Markov Chain Definitions</h1>
<h4 id="definition.-cbsfl-ranked-list-markov-chain"><strong>Definition</strong>. CBSFL Ranked List Markov Chain</h4>
<blockquote>
<p>To construct a Markov chain representing a list of program locations ranked in descending order by their suspiciousness scores:</p>
<ol>
<li><p>Let <span class="math inline">\(L\)</span> be the set of locations in the program.</p></li>
<li><p>For a location <span class="math inline">\(l \in L\)</span> let <span class="math inline">\(s(l)\)</span> be its CBSFL suspiciousness score.</p></li>
<li><p>Partition the locations in <span class="math inline">\(L\)</span> into a list of groups <span class="math inline">\(G = \left\{ g_1 \subseteq L, g_2 \subseteq L, ..., g_n \subseteq L \right\}\)</span> such that for each group <span class="math inline">\(g_i\)</span> all of the locations it contains have the same score: <span class="math inline">\( \forall ~ {g_i \in G}, ~ \forall ~ {l, l&#39; \in g_i} \left[
        s(l) = s(l&#39;) \right]\)</span></p></li>
<li><p>The score of a group <span class="math inline">\(s(g_i)\)</span> is defined to be the common score of its members: <span class="math inline">\( \forall ~ {l \in g_i} \left[ s(g_i) = s(l) \right]\)</span></p></li>
<li><p>Order <span class="math inline">\(G\)</span> by the scores of its groups, such that <span class="math inline">\(g_0\)</span> has the highest score and <span class="math inline">\(g_n\)</span> has the lowest: <span class="math inline">\(s(g_0) &gt; s(g_1) &gt; ... &gt; s(g_n)\)</span></p></li>
<li><p>Now construct the set of states. There is one state for each group <span class="math inline">\(g
      \in G\)</span> and for each location <span class="math inline">\(l \in L\)</span>. <span class="math display">\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ l : l \in L \right\}\]</span></p></li>
<li><p>Finally construct the transition matrix <span class="math inline">\({\bf P}\)</span> for the states <span class="math inline">\(S\)</span>. <span class="math display">\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{lll}
           1 &amp;
            \text{if}
            &amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
          \\
             \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
             \frac{1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp; \text{otherwise}
        \end{array}
      \right.\]</span></p></li>
</ol>
</blockquote>
<h4 id="definition.-cbsfl-with-jumps-markov-chain"><strong>Definition</strong>. CBSFL with Jumps Markov Chain</h4>
<blockquote>
<p>This definition augments Definition 28.</p>
<ol>
<li><p>Let <span class="math inline">\({\bf J}\)</span> be a "jump" matrix representing ways a programmer might move through the program during debugging.</p></li>
<li><p>If <span class="math inline">\({\bf J}_{x,y} &gt; 0\)</span> then locations <span class="math inline">\(x,y \in L\)</span> are "connected" by <span class="math inline">\({\bf J}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(p_{\text{jump}}\)</span> be the probability that when visiting a location <span class="math inline">\(l \in L\)</span> the Markov process "jumps" to another location. Let <span class="math inline">\(1 -
      p_{\text{jump}}\)</span> be the probability that the process returns to the state which represents its group instead of jumping. As <span class="math inline">\(p_\text{{jump}}
      \rightarrow 0\)</span> the behavior of the chain approaches the behavior of the chain in Definition 28.</p></li>
<li><p>The new transition matrix <span class="math inline">\({\bf P}\)</span> for the states <span class="math inline">\(S\)</span> is</p></li>
</ol>
<p><span class="math display">\[\begin{aligned}
      {\bf P}_{i,j} &amp;=
      \left\{
        \begin{array}{cll}
            1 - p_{\text{jump}} &amp;
            \text{if}
            &amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
            &amp; &amp;  \wedge \left( \sum_{k}{{\bf J}_{i,k}} \right) &gt; 0 \\
          \\
              p_{\text{jump}}
              \left( \frac{{\bf J}_{i,j}}{\sum_{k}{{\bf J}_{i,k}}} \right)
            &amp;
            \text{if}
            &amp;
                 s_i \in L \wedge s_j \in L \wedge {\bf J}_{i,j} &gt; 0 \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp; \text{otherwise}
        \end{array}
      \right.
    \end{aligned}\]</span></p>
</blockquote>
<h4 id="definition.-suspicious-behavior-markov-chain"><strong>Definition</strong>. Suspicious Behavior Markov Chain</h4>
<blockquote>
<p>A chain that models a ranked list of suspicious subgraphs:</p>
<ol>
<li><p>Let <span class="math inline">\(H\)</span> be a set of suspicious subgraphs (behaviors).</p></li>
<li><p>For a subgraph <span class="math inline">\(h \in H\)</span> let <span class="math inline">\(\varsigma(h)\)</span> be its suspiciousness score <span class="citation">[<a href="#ref-Henderson2018">9</a>]</span>.</p></li>
<li><p>Let <span class="math inline">\(L\)</span> be the set of locations in the program.</p></li>
<li><p>Partition the subgraphs in <span class="math inline">\(H\)</span> into a list of groups <span class="math inline">\(G = \left\{ g_1 \subseteq H, g_2 \subseteq H, ..., g_n \subseteq H \right\}\)</span> such that for each group <span class="math inline">\(g_i\)</span> all of the locations in <span class="math inline">\(g_i\)</span> have the same score: <span class="math inline">\(\forall ~ {g_i \in G} ~ \forall ~ {a, b \in g_i} \left[
        \varsigma(a) = \varsigma(b) \right]\)</span></p></li>
<li><p>Let the score of a group <span class="math inline">\(\varsigma(g_i)\)</span> be the same as the scores of its members: <span class="math inline">\( \forall ~ {g_i \in G} ~ \forall ~ {h \in g_i} \left[
        \varsigma(g_i) = \varsigma(h) \right]\)</span></p></li>
<li><p>Order <span class="math inline">\(G\)</span> by the scores of its groups, such that <span class="math inline">\(g_0\)</span> has the highest score and <span class="math inline">\(g_n\)</span> has the lowest: <span class="math inline">\( \varsigma(g_0) &gt; \varsigma(g_1) &gt; ... &gt;
      \varsigma(g_n)\)</span></p></li>
<li><p>Now construct the set of states. One state for each group <span class="math inline">\(g \in G\)</span>, one state for each subgraph <span class="math inline">\(h \in H\)</span>, and one state for each location <span class="math inline">\(l
      \in V_{h}\)</span> for all <span class="math inline">\(h \in H\)</span>. <span class="math display">\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ h : h \in H \right\}
          \cup
          \left\{ l : l \in V_h,~ \forall~ h \in H \right\}\]</span></p></li>
<li><p>Let <span class="math inline">\(c: L \rightarrow \mathbb{N}^{+}\)</span> be a function that gives the number of subgraphs <span class="math inline">\(h \in H\)</span> which a location <span class="math inline">\(l\)</span> appears in.</p></li>
<li><p>Finally construct the transition matrix <span class="math inline">\({\bf P}\)</span> for the states <span class="math inline">\(S\)</span>. <span class="math display">\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{cll}
            \frac{1}{c(s_i)} &amp;
            \text{if}
            &amp;
                 s_i \in L \wedge s_j \in H \wedge s_i \in V_{s_j} \\
         \\
            \frac{1}{2}\frac{1}{{{\left|{V_{s_i}}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in H \wedge s_j \in L \wedge s_j \in V_{s_i} \\
          \\
            \frac{1}{2} &amp;
            \text{if}
            &amp;
                 s_i \in H \wedge s_j \in G \wedge s_i \in s_j \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;
            \text{if}
            &amp;
                 s_i \in G \wedge s_j \in H \wedge s_j \in s_i \\
          \\
            0
            &amp; \text{otherwise}
        \end{array}
      \right.\]</span></p></li>
</ol>
</blockquote>
<div id="refs" class="references">
<h1>References</h1>
<div id="ref-Jones2002">
<p>[1] J. Jones, M. Harrold, and J. Stasko, "Visualization of test information to assist fault localization," <em>Proceedings of the 24th International Conference on Software Engineering. ICSE 2002</em>, 2002, doi:<a href="https://doi.org/10.1145/581339.581397">10.1145/581339.581397</a>.</p>
</div>
<div id="ref-Lucia2014">
<p>[2] Lucia, D. Lo, L. Jiang, F. Thung, and A. Budi, "Extended comprehensive study of association measures for fault localization," <em>Journal of Software: Evolution and Process</em>, vol. 26, no. 2, pp. 172-219, Feb. 2014, doi:<a href="https://doi.org/10.1002/smr.1616">10.1002/smr.1616</a>.</p>
</div>
<div id="ref-Sun2016">
<p>[3] S.-F. Sun and A. Podgurski, "Properties of Effective Metrics for Coverage-Based Statistical Fault Localization," in <em>2016 ieee international conference on software testing, verification and validation (icst)</em>, 2016, pp. 124-134, doi:<a href="https://doi.org/10.1109/ICST.2016.31">10.1109/ICST.2016.31</a>.</p>
</div>
<div id="ref-Jones2005">
<p>[4] J. A. Jones and M. J. Harrold, "Empirical Evaluation of the Tarantula Automatic Fault-localization Technique," in <em>Proceedings of the 20th ieee/acm international conference on automated software engineering</em>, 2005, pp. 273-282, doi:<a href="https://doi.org/10.1145/1101908.1101949">10.1145/1101908.1101949</a>.</p>
</div>
<div id="ref-Pearson2017">
<p>[5] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst, D. Pang, and B. Keller, "Evaluating and Improving Fault Localization," in <em>Proceedings of the 39th international conference on software engineering</em>, 2017, pp. 609-620, doi:<a href="https://doi.org/10.1109/ICSE.2017.62">10.1109/ICSE.2017.62</a>.</p>
</div>
<div id="ref-Parnin2011">
<p>[6] C. Parnin and A. Orso, "Are Automated Debugging Techniques Actually Helping Programmers?" in <em>ISSTA</em>, 2011, pp. 199-209.</p>
</div>
<div id="ref-Renieres2003">
<p>[7] M. Renieres and S. Reiss, "Fault localization with nearest neighbor queries," in <em>18th ieee international conference on automated software engineering, 2003. proceedings.</em>, 2003, pp. 30-39, doi:<a href="https://doi.org/10.1109/ASE.2003.1240292">10.1109/ASE.2003.1240292</a>.</p>
</div>
<div id="ref-Cheng2009a">
<p>[8] H. Cheng, D. Lo, Y. Zhou, X. Wang, and X. Yan, "Identifying Bug Signatures Using Discriminative Graph Mining," in <em>Proceedings of the eighteenth international symposium on software testing and analysis</em>, 2009, pp. 141-152, doi:<a href="https://doi.org/10.1145/1572272.1572290">10.1145/1572272.1572290</a>.</p>
</div>
<div id="ref-Henderson2018">
<p>[9] T. A. D. Henderson and A. Podgurski, "Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs," in <em>IEEE conference on software testing, validation and verification</em>, 2018.</p>
</div>
<div id="ref-Baah2010">
<p>[10] G. G. K. Baah, A. Podgurski, and M. J. M. Harrold, "Causal inference for statistical fault localization," in <em>Proceedings of the 19th international symposium on software testing and analysis</em>, 2010, pp. 73-84, doi:<a href="https://doi.org/10.1145/1831708.1831717">10.1145/1831708.1831717</a>.</p>
</div>
<div id="ref-Aggarwal2014">
<p>[11] C. C. Aggarwal and J. Han, Eds., <em>Frequent Pattern Mining</em>. Cham: Springer International Publishing, 2014.</p>
</div>
<div id="ref-Agrawal1993">
<p>[12] R. Agrawal, T. Imieliński, and A. Swami, "Mining association rules between sets of items in large databases," <em>ACM SIGMOD Record</em>, vol. 22, no. 2, pp. 207-216, Jun. 1993, doi:<a href="https://doi.org/10.1145/170036.170072">10.1145/170036.170072</a>.</p>
</div>
<div id="ref-Yan2002">
<p>[13] X. Yan and J. Han, "gSpan: graph-based substructure pattern mining," in <em>2002 ieee international conference on data mining, 2002. proceedings.</em>, 2002, pp. 721-724, doi:<a href="https://doi.org/10.1109/ICDM.2002.1184038">10.1109/ICDM.2002.1184038</a>.</p>
</div>
<div id="ref-Aggarwal2014a">
<p>[14] C. C. Aggarwal, M. A. Bhuiyan, and M. A. Hasan, "Frequent Pattern Mining Algorithms: A Survey," in <em>Frequent pattern mining</em>, Cham: Springer International Publishing, 2014, pp. 19-64.</p>
</div>
<div id="ref-Yan2008">
<p>[15] X. Yan, H. Cheng, J. Han, and P. S. Yu, "Mining Significant Graph Patterns by Leap Search," in <em>Proceedings of the 2008 acm sigmod international conference on management of data</em>, 2008, pp. 433-444, doi:<a href="https://doi.org/10.1145/1376616.1376662">10.1145/1376616.1376662</a>.</p>
</div>
<div id="ref-Abreu2006">
<p>[16] R. Abreu, P. Zoeteweij, and A. Van Gemund, "An Evaluation of Similarity Coefficients for Software Fault Localization," in <em>2006 12th pacific rim international symposium on dependable computing (prdc'06)</em>, 2006, pp. 39-46, doi:<a href="https://doi.org/10.1109/PRDC.2006.18">10.1109/PRDC.2006.18</a>.</p>
</div>
<div id="ref-Abreu2009">
<p>[17] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. C. van Gemund, "A practical evaluation of spectrum-based fault localization," <em>Journal of Systems and Software</em>, vol. 82, no. 11, pp. 1780-1792, 2009, doi:<a href="https://doi.org/10.1016/j.jss.2009.06.035">10.1016/j.jss.2009.06.035</a>.</p>
</div>
<div id="ref-Agarwal2014">
<p>[18] P. Agarwal and A. P. Agrawal, "Fault-localization Techniques for Software Systems: A Literature Review," <em>SIGSOFT Softw. Eng. Notes</em>, vol. 39, no. 5, pp. 1-8, Sep. 2014, doi:<a href="https://doi.org/10.1145/2659118.2659125">10.1145/2659118.2659125</a>.</p>
</div>
<div id="ref-Wong2016">
<p>[19] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, "A Survey on Software Fault Localization," <em>IEEE Transactions on Software Engineering</em>, vol. 42, no. 8, pp. 707-740, Aug. 2016, doi:<a href="https://doi.org/10.1109/TSE.2016.2521368">10.1109/TSE.2016.2521368</a>.</p>
</div>
<div id="ref-Zeller1999">
<p>[20] A. Zeller, "Yesterday, My Program Worked. Today, It Does Not. Why?" <em>SIGSOFT Softw. Eng. Notes</em>, vol. 24, no. 6, pp. 253-267, Oct. 1999, doi:<a href="https://doi.org/10.1145/318774.318946">10.1145/318774.318946</a>.</p>
</div>
<div id="ref-Tip1995">
<p>[21] F. Tip, "A survey of program slicing techniques," <em>Journal of programming languages</em>, vol. 3, no. 3, pp. 121-189, 1995.</p>
</div>
<div id="ref-Mao2014">
<p>[22] X. Mao, Y. Lei, Z. Dai, Y. Qi, and C. Wang, "Slice-based statistical fault localization," <em>Journal of Systems and Software</em>, vol. 89, no. 1, pp. 51-62, 2014, doi:<a href="https://doi.org/10.1016/j.jss.2013.08.031">10.1016/j.jss.2013.08.031</a>.</p>
</div>
<div id="ref-Marcus2004">
<p>[23] A. Marcus, A. Sergeyev, V. Rajlieh, and J. I. Maletic, "An information retrieval approach to concept location in source code," <em>Proceedings - Working Conference on Reverse Engineering, WCRE</em>, pp. 214-223, 2004, doi:<a href="https://doi.org/10.1109/WCRE.2004.10">10.1109/WCRE.2004.10</a>.</p>
</div>
<div id="ref-Zhou2012">
<p>[24] J. Zhou, H. Zhang, and D. Lo, "Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports," <em>Proceedings - International Conference on Software Engineering</em>, pp. 14-24, 2012, doi:<a href="https://doi.org/10.1109/ICSE.2012.6227210">10.1109/ICSE.2012.6227210</a>.</p>
</div>
<div id="ref-Le2015">
<p>[25] T.-D. B. Le, R. J. Oentaryo, and D. Lo, "Information retrieval and spectrum based bug localization: better together," in <em>Proceedings of the 2015 10th joint meeting on foundations of software engineering - esec/fse 2015</em>, 2015, pp. 579-590, doi:<a href="https://doi.org/10.1145/2786805.2786880">10.1145/2786805.2786880</a>.</p>
</div>
<div id="ref-Artzi2010">
<p>[26] S. Artzi, J. Dolby, F. Tip, and M. Pistoia, "Directed Test Generation for Effective Fault Localization," in <em>Proceedings of the 19th international symposium on software testing and analysis</em>, 2010, pp. 49-60, doi:<a href="https://doi.org/10.1145/1831708.1831715">10.1145/1831708.1831715</a>.</p>
</div>
<div id="ref-Sahoo2013">
<p>[27] S. K. Sahoo, J. Criswell, C. Geigle, and V. Adve, "Using likely invariants for automated software fault localization," in <em>Proceedings of the eighteenth international conference on architectural support for programming languages and operating systems</em>, 2013, vol. 41, p. 139, doi:<a href="https://doi.org/10.1145/2451116.2451131">10.1145/2451116.2451131</a>.</p>
</div>
<div id="ref-Perez2014">
<p>[28] A. Perez, R. Abreu, and A. Riboira, "A Dynamic Code Coverage Approach to Maximize Fault Localization Efficiency," <em>J. Syst. Softw.</em>, vol. 90, pp. 18-28, Apr. 2014, doi:<a href="https://doi.org/10.1016/j.jss.2013.12.036">10.1016/j.jss.2013.12.036</a>.</p>
</div>
<div id="ref-Agrawal1995">
<p>[29] H. Agrawal, J. Horgan, S. London, and W. Wong, "Fault localization using execution slices and dataflow tests," in <em>Proceedings of sixth international symposium on software reliability engineering. issre'95</em>, 1995, pp. 143-151, doi:<a href="https://doi.org/10.1109/ISSRE.1995.497652">10.1109/ISSRE.1995.497652</a>.</p>
</div>
<div id="ref-Cleve2005">
<p>[30] H. Cleve and A. Zeller, "Locating causes of program failures," <em>Proceedings of the 27th international conference on Software engineering - ICSE '05</em>, p. 342, 2005, doi:<a href="https://doi.org/10.1145/1062455.1062522">10.1145/1062455.1062522</a>.</p>
</div>
<div id="ref-Horwitz1990">
<p>[31] S. Horwitz, "Identifying the Semantic and Textual Differences Between Two Versions of a Program," <em>SIGPLAN Not.</em>, vol. 25, no. 6, pp. 234-245, Jun. 1990, doi:<a href="https://doi.org/10.1145/93548.93574">10.1145/93548.93574</a>.</p>
</div>
<div id="ref-Wong2008">
<p>[32] E. Wong, T. Wei, Y. Qi, and L. Zhao, "A Crosstab-based Statistical Method for Effective Fault Localization," in <em>2008 international conference on software testing, verification, and validation</em>, 2008, pp. 42-51, doi:<a href="https://doi.org/10.1109/ICST.2008.65">10.1109/ICST.2008.65</a>.</p>
</div>
<div id="ref-Landsberg2015">
<p>[33] D. Landsberg, H. Chockler, D. Kroening, and M. Lewis, "Evaluation of Measures for Statistical Fault Localisation and an Optimising Scheme," in <em>International conference on fundamental approaches to software engineering</em>, 2015, vol. 9033, pp. 115-129, doi:<a href="https://doi.org/10.1007/978-3-662-46675-9">10.1007/978-3-662-46675-9</a>.</p>
</div>
<div id="ref-Zheng2018">
<p>[34] Y. Zheng, Z. Wang, X. Fan, X. Chen, and Z. Yang, "Localizing multiple software faults based on evolution algorithm," <em>Journal of Systems and Software</em>, vol. 139, pp. 107-123, 2018, doi:<a href="https://doi.org/10.1016/j.jss.2018.02.001">10.1016/j.jss.2018.02.001</a>.</p>
</div>
<div id="ref-ChaoLiu2006">
<p>[35] C. Liu, L. Fei, X. Yan, J. Han, and S. P. Midkiff, "Statistical debugging: A hypothesis testing-based approach," <em>IEEE Transactions on Software Engineering</em>, vol. 32, no. 10, pp. 831-847, Oct. 2006, doi:<a href="https://doi.org/10.1109/TSE.2006.105">10.1109/TSE.2006.105</a>.</p>
</div>
<div id="ref-Ferrante1987">
<p>[36] J. Ferrante, K. J. Ottenstein, and J. D. Warren, "The program dependence graph and its use in optimization," vol. 9. pp. 319-349, Jul-1987.</p>
</div>
<div id="ref-Ali2009">
<p>[37] S. Ali, J. H. Andrews, T. Dhandapani, and W. Wang, "Evaluating the Accuracy of Fault Localization Techniques," <em>2009 IEEE/ACM International Conference on Automated Software Engineering</em>, pp. 76-87, 2009, doi:<a href="https://doi.org/10.1109/ASE.2009.89">10.1109/ASE.2009.89</a>.</p>
</div>
<div id="ref-Moon2014">
<p>[38] S. Moon, Y. Kim, M. Kim, and S. Yoo, "Ask the Mutants: Mutating faulty programs for fault localization," <em>Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014</em>, pp. 153-162, 2014, doi:<a href="https://doi.org/10.1109/ICST.2014.28">10.1109/ICST.2014.28</a>.</p>
</div>
<div id="ref-Liu2005">
<p>[39] C. Liu, H. Yu, P. S. Yu, X. Yan, H. Yu, J. Han, and P. S. Yu, "Mining Behavior Graphs for ‘Backtrace' of Noncrashing Bugs," in <em>Proceedings of the 2005 siam international conference on data mining</em>, 2005, pp. 286-297, doi:<a href="https://doi.org/10.1137/1.9781611972757.26">10.1137/1.9781611972757.26</a>.</p>
</div>
<div id="ref-DiFatta2006">
<p>[40] G. Di Fatta, S. Leue, and E. Stegantova, "Discriminative Pattern Mining in Software Fault Detection," in <em>Proceedings of the 3rd international workshop on software quality assurance</em>, 2006, pp. 62-69, doi:<a href="https://doi.org/10.1145/1188895.1188910">10.1145/1188895.1188910</a>.</p>
</div>
<div id="ref-Eichinger2008">
<p>[41] F. Eichinger, K. Böhm, and M. Huber, "Mining Edge-Weighted Call Graphs to Localise Software Bugs," in <em>European conference machine learning and knowledge discovery in databases</em>, 2008, pp. 333-348, doi:<a href="https://doi.org/10.1007/978-3-540-87479-9_40">10.1007/978-3-540-87479-9_40</a>.</p>
</div>
<div id="ref-Eichinger2010">
<p>[42] F. Eichinger, K. Krogmann, R. Klug, and K. Böhm, "Software-defect Localisation by Mining Dataflow-enabled Call Graphs," in <em>Proceedings of the 2010 european conference on machine learning and knowledge discovery in databases: Part i</em>, 2010, pp. 425-441.</p>
</div>
<div id="ref-Mousavian2011">
<p>[43] Z. Mousavian, M. Vahidi-Asl, and S. Parsa, "Scalable Graph Analyzing Approach for Software Fault-localization," in <em>Proceedings of the 6th international workshop on automation of software test</em>, 2011, pp. 15-21, doi:<a href="https://doi.org/10.1145/1982595.1982599">10.1145/1982595.1982599</a>.</p>
</div>
<div id="ref-Eichinger2011">
<p>[44] F. Eichinger, C. Oßner, and K. Böhm, "Scalable software-defect localisation by hierarchical mining of dynamic call graphs," <em>Proceedings of the 11th SIAM International Conference on Data Mining, SDM 2011</em>, no. c, pp. 723-734, 2011.</p>
</div>
<div id="ref-Parsa2011">
<p>[45] S. Parsa, S. A. Naree, and N. E. Koopaei, "Software Fault Localization via Mining Execution Graphs," in <em>Proceedings of the 2011 international conference on computational science and its applications - volume part ii</em>, 2011, pp. 610-623.</p>
</div>
<div id="ref-Mariani2011">
<p>[46] L. Mariani, F. Pastore, and M. Pezze, "Dynamic Analysis for Diagnosing Integration Faults," <em>IEEE Trans. Softw. Eng.</em>, vol. 37, no. 4, pp. 486-508, Jul. 2011, doi:<a href="https://doi.org/10.1109/TSE.2010.93">10.1109/TSE.2010.93</a>.</p>
</div>
<div id="ref-Yousefi2013">
<p>[47] A. Yousefi and A. Wassyng, "A Call Graph Mining and Matching Based Defect Localization Technique," in <em>2013 ieee sixth international conference on software testing, verification and validation workshops</em>, 2013, pp. 86-95, doi:<a href="https://doi.org/10.1109/ICSTW.2013.17">10.1109/ICSTW.2013.17</a>.</p>
</div>
<div id="ref-Ioannidis2005">
<p>[48] J. P. A. Ioannidis, "Why most published research findings are false," <em>PLoS Medicine</em>, vol. 2, no. 8, pp. 0696-0701, 2005, doi:<a href="https://doi.org/10.1371/journal.pmed.0020124">10.1371/journal.pmed.0020124</a>.</p>
</div>
<div id="ref-Grinstead2012">
<p>[49] C. M. Grinstead and J. L. Snell, <em>Introduction to Probability</em>, 2nd ed. Providence, RI: American Mathematical Society, 1997.</p>
</div>
<div id="ref-Kemeny1960">
<p>[50] J. G. Kemeny and J. L. Snell, <em>Finite Markov Chains</em>, First. Princeton, NJ: Van Nostrand, 1960.</p>
</div>
<div id="ref-Davis2004">
<p>[51] T. A. Davis, "Algorithm 832: UMFPACK V4.3—an Unsymmetric-pattern Multifrontal Method," <em>ACM Trans. Math. Softw.</em>, vol. 30, no. 2, pp. 196-199, Jun. 2004, doi:<a href="https://doi.org/10.1145/992200.992206">10.1145/992200.992206</a>.</p>
</div>
<div id="ref-Just2014">
<p>[52] R. Just, D. Jalali, and M. D. Ernst, "Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs," in <em>Proceedings of the 2014 international symposium on software testing and analysis</em>, 2014, pp. 437-440, doi:<a href="https://doi.org/10.1145/2610384.2628055">10.1145/2610384.2628055</a>.</p>
</div>
<div id="ref-Steimann2013">
<p>[53] F. Steimann, M. Frenkel, and R. Abreu, "Threats to the validity and value of empirical assessments of the accuracy of coverage-based fault locators," <em>Proceedings of the 2013 International Symposium on Software Testing and Analysis - ISSTA 2013</em>, p. 314, 2013, doi:<a href="https://doi.org/10.1145/2483760.2483767">10.1145/2483760.2483767</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Jones <em>et al.</em> did not use the term "suspiciousness score" or "suspiciousness metric" in their 2002 paper <span class="citation">[<a href="#ref-Jones2002">1</a>]</span>. They introduced the term "suspiciousness score" in their 2005 paper <span class="citation">[<a href="#ref-Jones2005">4</a>]</span>, in the context of ranking statements. Both terms are now in common use.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>A dependence sphere is computed from the Program Dependence Graph (PDG) <span class="citation">[<a href="#ref-Horwitz1990">31</a>], [<a href="#ref-Ferrante1987">36</a>]</span>. In a PDG, program elements are nodes and their <em>control</em> and <em>data</em> dependencies are represented as edges. Given two nodes <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> a graph with a shortest path (ignoring edge directionality) <span class="math inline">\(\pi\)</span> between them, the sphere is all those nodes in the graph have have a path from <span class="math inline">\(\alpha\)</span> as short (or shorter) than <span class="math inline">\(\pi\)</span> (once again ignoring edge directionality).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Implementations of these two computations maybe found in <span><code>https://github.com/timtadh/expected-hitting-times</code></span>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>See https://hackthology.com/pdfs/scam-2019-supplement.pdf<a href="#fnref4">↩</a></p></li>
</ol>
</div>
