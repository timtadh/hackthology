Title: SafeRevert: When Can Breaking Changes be Automatically Reverted?
Author: <a href="http://hackthology.com">Tim Henderson</a>,  Avi Kondareddy, Sushmita Azad, Eric Nickell
Citation: <strong>Tim A. D. Henderson</strong>, Avi Kondareddy, Sushmita Azad, and Eric Nickell. <i>SafeRevert: When Can Breaking Changes be Automatically Reverted?</i>. ICST Industry Track 2024.
CiteAuthor: <strong>Tim A. D. Henderson</strong>, Avi Kondareddy, Sushmita Azad, Eric Nickell
Publication: ICST Industry Track 2024
Date: 2024-05-27
Category: Paper
MainPage: show

**Tim A. D. Henderson**, Avi Kondareddy, Sushmita Azad, and Eric Nickell.
*SafeRevert: When Can Breaking Changes be Automatically Reverted?*.  [ICST Industry Track 2024](https://conf.researchr.org/details/icst-2024/icst-2024-industry/7/SafeRevert-When-Can-Breaking-Changes-be-Automatically-Reverted-).
<br/>
[DOI](http://tba).
[PDF]({static}/pdfs/icst-2024.pdf).
[WEB]({filename}/papers/2024-icst.md).
[Google Research Preprint](http://tba).

<h4>Note</h4>
> This is a conversion from a latex paper I wrote. If you want all formatting
> correct you should read the
> [pdf version]({static}/pdfs/icst-2024.pdf).

#### Abstract

When bugs or defects are introduced into a large scale software repository,
they reduce productivity. Programmers working on related areas of the code
will encounter test failures, compile breakages, or other anomalous behavior.
On encountering these issues, they will need to troubleshoot and determine
that their changes were not the cause of the error and that another change is
at fault. They must then find that change and *revert it* to return the
repository to a healthy state. In the past, our group has identified ways to
identify the root cause (or culprit) change that introduced a test failure
even when the test is flaky. This paper focuses on a related issue: at what
point does the Continuous Integration system have enough evidence to support
*automatically reverting a change*? We will motivate the problem, provide
several methods to address it, and empirically evaluate our solution on a
large set (25,137) of real-world breaking changes that occurred at
Google. SafeRevert improved recall (number of changes recommend for reversion)
by 2x over the baseline method while meeting our safety criterion.

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h1 id="sec:introduction">Introduction</h1>
<p>Large scale software development is enabled by automatically
executing tests in a continuous integration environment. Continuous
integration (CI) <span class="citation" data-cites="Fowler2006">(<a
href="#ref-Fowler2006" role="doc-biblioref">Fowler 2006</a>)</span> is
the industrial practice of using automated systems to automatically
integrate changes into the source of truth for the software system or
repository. This improves collaboration by helping software developers
avoid breaking compilation, tests, or structure of the system that
others are relying on.</p>
<p>Corporations and teams may engage in CI using adhoc tools across many
independent software repositories. For instance Github (github.com)
supports CI “Actions" which can be integrated into any repository.
However, many organizations are recognizing the value of using a
“mono-repository" (monorepo) development model where many or all teams
in the organization use a single shared software repository <span
class="citation" data-cites="Potvin2016">(<a href="#ref-Potvin2016"
role="doc-biblioref">Potvin and Levenberg 2016</a>)</span>. At the
largest organizations such as Google <span class="citation"
data-cites="Potvin2016 Memon2017 Henderson2023">(<a
href="#ref-Potvin2016" role="doc-biblioref">Potvin and Levenberg
2016</a>; <a href="#ref-Memon2017" role="doc-biblioref">Memon et al.
2017</a>; <a href="#ref-Henderson2023" role="doc-biblioref">Henderson et
al. 2023</a>)</span>, Microsoft <span class="citation"
data-cites="Herzig2015a">(<a href="#ref-Herzig2015a"
role="doc-biblioref">Herzig and Nagappan 2015</a>)</span>, and Facebook
<span class="citation" data-cites="Machalica2019">(<a
href="#ref-Machalica2019" role="doc-biblioref">Machalica et al.
2019</a>)</span> large repositories are complimented by advanced
centralized CI systems.</p>
<p>In these large, modern CI systems, the integration goes beyond just
ensuring the code textually merges. Compilations are invoked, tests are
executed, and additional static and dynamic verification steps are
performed. The demand for machine resources can exceed capacity for
build, test, and verification tasks desired by a large-scale CI system.
To combat this problem, test case selection or prioritization is used
<span class="citation"
data-cites="Gupta2011 Micco2013 Machalica2019">(<a href="#ref-Gupta2011"
role="doc-biblioref">Gupta, Ivey, and Penix 2011</a>; <a
href="#ref-Micco2013" role="doc-biblioref">Micco 2013</a>; <a
href="#ref-Machalica2019" role="doc-biblioref">Machalica et al.
2019</a>)</span> to select fewer tests to run.</p>
<p>Additionally, CI steps are often invoked at multiple points in the
Software Development Life Cycle. In the past, it may have been assumed
that the tests were executed by the CI system once, at the time a commit
was created in the version control. Today, CI may execute tests multiple
times during development: when a change is sent for code review,
immediately prior to submission or integration into the main development
branch, after one or more changes has been integrated into the
development branch, and when a new release is created. <em>This paper is
primary concerned with testing that occurs after the code has been
integrated into the main development branch.</em></p>
<h2 id="why-is-testing-necessary-after-code-integration">Why is Testing
Necessary After Code Integration?</h2>
<p>When deploying CI for the first time, many organizations primarily
focus on conducting testing at the time a change is merged into the main
branch. For instance, they may test when a Pull Request (PR) on Github
is going to be merged into the main branch. The PR is merged if the
tests pass and no changes have been made to the main branch since the
testing started. This strategy works well until the rate of PR
submission exceeds the average amount of time it takes to run all the
tests.</p>
<p>At this point, organizations may do a stop gap fix such as adding
more machines to run tests in parallel or reducing the size of the test
suite. However, at some point, these measures will prove ineffective and
the rate that testing can be conducted will become a impediment to an
organization’s engineering velocity. To address this, one common
solution is to introduce a “submission queue" which batches changes
together for testing and merges them all if the tests pass <span
class="citation" data-cites="Ananthanarayanan2019">(<a
href="#ref-Ananthanarayanan2019" role="doc-biblioref">Ananthanarayanan
et al. 2019</a>)</span>. If the tests fail, the offending change must be
identified and the remaining changes in the batch must be retested (at
least doubling the total testing time) <span class="citation"
data-cites="Najafi2019">(<a href="#ref-Najafi2019"
role="doc-biblioref">Najafi, Shang, and Rigby 2019</a>)</span>.</p>
<p>As the submission rate continues to increase, the organization may
add conservative test selection based on file-level dependence analysis
<span class="citation" data-cites="Ananthanarayanan2019">(<a
href="#ref-Ananthanarayanan2019" role="doc-biblioref">Ananthanarayanan
et al. 2019</a>)</span>. But at this point, the organization will be
reaching the limit of what can be done to completely prevent any
breaking changes from being integrated into the main development branch
beyond just buying more and more machines to further parallelize the
testing.</p>
<p>And what about just buying more machines? Won’t this effectively
solve the problem? It would until the submission rate exceeds the time
it takes to run the slowest test. At this point, the number of machines
purchased can only partially control impact to developer productivity.
Even with a very large budget for testing, the number of changes per
batch will continue to grow as increases in the number of machines will
not reduce the batch size but only keep the testing time per batch
relatively constant. The larger the batch size, the more likely that a
developer will experience conflicts with another change when attempting
to submit. These conflicts (either at the syntactical or semantic level)
will make it difficult for developers to reliably submit their changes
and require them to constantly monitor the submission process. This
monitoring will hurt overall developer productivity.</p>
<p>At the highest submission rates seen in industry today, using
submission queues that guarantee zero breaking changes becomes
infeasible both from a machine cost perspective and from a developer
productivity perspective. Therefore, many organizations relax the
requirement that an integrated change will never break a test. Even
while using submit queues to gate contributions for a single team,
Google has relied on postsubmit testing for mission-critical software
since the early 2000s as documented by Mike Bland <span class="citation"
data-cites="Bland2012">(<a href="#ref-Bland2012"
role="doc-biblioref">Bland 2012</a>)</span>.</p>
<h2 id="post-submission-testing">Post Submission Testing</h2>
<p>The purpose of testing after a change has been integrated (called
<em>Post-Submission</em> Testing or <em>Postsubmit</em> Testing
hereafter) is to identify defects that slipped through testing that
occurred prior to integration. Typically, organizations are unable to
run all tests at every integrated change. Instead, testing is conducted
periodically. In this paper we will refer to this periodic testing as
<em>Testing Cycles</em>.</p>
<p>Figure <a href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a> is a simplified
view of the postsubmit testing strategy used at Google. The test
scheduler waits until our Build System <span class="citation"
data-cites="Wang2020">(<a href="#ref-Wang2020" role="doc-biblioref">Wang
et al. 2020</a>)</span> has capacity to start a test cycle. It then
schedules tests and then waits until the system has capacity again. When
scheduling tests, certain tests may be temporarily skipped or throttled
to conserve resources. As shown in Figure <a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a>, this leads to
the system usually detecting new failures some time after the version
that introduced them had been integrated. This leads to the problem of
“Culprit Finding."</p>
<h2 id="culprit-finding">Culprit Finding</h2>
<p>Culprit finding is conceptually simple: given a list of versions that
may have introduced a fault, locate the offending change. One may solve
this problem with a variety of techniques: Zeller’s Delta Debugging
<span class="citation" data-cites="Zeller1999">(<a
href="#ref-Zeller1999" role="doc-biblioref">Zeller 1999</a>)</span>,
Git’s bisection algorithm <span class="citation"
data-cites="Couder2008">(<a href="#ref-Couder2008"
role="doc-biblioref">Couder 2008</a>)</span>, or Google’s Flake Aware
Culprit Finding (FACF) algorithm <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span>. Every technique
used to identify the offending change will have some error rate in
industrial practice.</p>
<p>But wait! How can a binary search or delta debugging have an “error
rate"? The answer is that old nemesis of industrial practice: flaky or
non-deterministic test behavior <span class="citation"
data-cites="parry2022">(<a href="#ref-parry2022"
role="doc-biblioref">Parry et al. 2022</a>)</span>. Flaky tests can be
caused both by problems in the production code (ex: race conditions
causing rare errors), in the test code (ex: use of “sleep") and by
infrastructure problems (ex: unreliable machine with bad ram or network
congestion). All of these problems behave in <em>version
non-hermetic</em> ways, where failures may not be strictly linked to the
version of code executed. Furthermore, even when the test is fully
<em>version hermetic</em> the flakiness may not obey a Bernoulli
distribution as subsequent executions may not be fully independent of
the prior ones.</p>
<p>The above issues mean that even culprit finding algorithms such as
FACF <span class="citation" data-cites="Henderson2023">(<a
href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
2023</a>)</span> that have been purpose-built to mitigate flakiness will
have some error rate. While that error rate will be much less than a
naive algorithm, it may still be high enough to cause problems when
deployed in certain environments.</p>
<h2 id="intro:auto-revert">Automatically Reverting Changes</h2>
<p>Once an organization has a reliable and high performance culprit
finding system, it is natural to use it to automatically revert (undo,
roll back) changes that introduce defects into the main development
branch. By automatically undoing these changes, the system decreases the
amount of time developers working with impacted tests will experience
breakages that are unrelated to the changes they are working on. It will
also increase the number of versions that are viable to be used to make
software releases. Reducing development friction and increasing the
number of release candidates improves developer productivity by reducing
the amount of time developers spend troubleshooting tests that were
broken by someone else.</p>
<p>Unfortunately, naively integrating FACF directly into a system that
immediately reverts all changes it identifies will lead to unhappy
developers. This is because even though Google’s FACF has a measured
<em>per-test accuracy</em> of 99.54%, when aggregated (grouped) by
blamed change, the accuracy <em>per-change</em> is only 77.37% in the
last 28 day window as of this writing.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
This would translate into <em>incorrectly reverting</em> approximately
20-130 changes per day out of an approximately of 300-500 total changes
per day reverted (see Figure <a href="#fig:baseline"
data-reference-type="ref"
data-reference="fig:baseline">[fig:baseline]</a>). At Google, we find
the cost of incorrectly reverting a change is extremely high in terms of
developer toil and frustration. Therefore, we need to reduce the rate of
bad reversions as much as possible. In this work, we aim to incorrectly
revert fewer than one change per day on average.</p>
<h2 id="our-contributions-and-findings">Our Contributions and
Findings</h2>
<p>In this paper we propose a shallow machine-learning based method for
using the output of a culprit finding algorithm to automatically revert
(undo, rollback) a change that controls for potential errors from the
culprit finding algorithm. Our method is generic and can be used with
any culprit finding algorithm.</p>
<ol>
<li><p>SafeRevert: a method for using the output of any culprit finding
algorithm to automatically revert (undo) a change. SafeRevert controls
for the error rate while increasing total number of reversions over
baseline methods.</p></li>
<li><p>An ablation study on the models, designs and features used by the
machine learning system used in SafeRevert to identify the most
impactful features and best models.</p></li>
<li><p>A large scale study on SafeRevert’s efficacy using <span
class="math inline">\(\thicksim\)</span>25,137 potential culprit changes
identified by Google’s production culprit finder over a <span
class="math inline">\(\sim\)</span>3 month window yields a recall of
55.7%, <span class="math inline">\(\sim2.1 \times\)</span> higher than
the baseline method.</p></li>
</ol>
<h1 id="background">Background</h1>


<span id="fig:ci-timeline" label="fig:ci-timeline">
[<em><strong>Figure 1</strong></em>
![Figure 1](images/icst-2024/fig1.png)](images/icst-2024/fig1.png)
</span>

<p>The Test Automation Platform (TAP) is a proprietary Continuous
Integration (CI) system at Google. It runs tests that execute on a
single machine without the use of the network or external resources.<a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> All tests on TAP are required to run
in under 15 minutes and exceeding that limit is considered a test
failure. TAP executes tests both before a user submits their changes and
after the user has submitted their change. This paper is only concerned
with the testing that has occurred after a user has submitted a change.
At Google, this is referred to as “Postsubmit Testing". The part of TAP
that does Postsubmit Testing is called TAP Postsubmit.</p>
<p>TAP has appeared in the literature several times <span
class="citation"
data-cites="Gupta2011 Micco2012 Micco2013 Memon2017 Leong2019 Wang2020 Henderson2023">(<a
href="#ref-Gupta2011" role="doc-biblioref">Gupta, Ivey, and Penix
2011</a>; <a href="#ref-Micco2012" role="doc-biblioref">Micco 2012</a>,
<a href="#ref-Micco2013" role="doc-biblioref">2013</a>; <a
href="#ref-Memon2017" role="doc-biblioref">Memon et al. 2017</a>; <a
href="#ref-Leong2019" role="doc-biblioref">Leong et al. 2019</a>; <a
href="#ref-Wang2020" role="doc-biblioref">Wang et al. 2020</a>; <a
href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
2023</a>)</span> and there has been gradual evolution its testing
strategy over the years. However, in general TAP uses a combination of
static and dynamic Test Selection, execution throttling, and
just-in-time scheduling to control testing load. A simplified diagram of
TAP Postsubmit testing is visualized in Figure <a
href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a>. Tests are
periodically run in Testing Cycles.<a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a> During a cycle, Projects
that are eligible to run their test in the cycle are selected. Tests
from those projects are included if some file was modified since that
test’s last execution can influence their behavior via inspection of a
dependence graph at the Build Target and File level granularity <span
class="citation" data-cites="Gupta2011 Micco2012">(<a
href="#ref-Gupta2011" role="doc-biblioref">Gupta, Ivey, and Penix
2011</a>; <a href="#ref-Micco2012" role="doc-biblioref">Micco
2012</a>)</span>. When breakages inevitably occur culprit finding is
conducted using the FACF algorithm to locate the offending change <span
class="citation" data-cites="Henderson2023">(<a
href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
2023</a>)</span>.</p>
<p>FACF operates on a single test target at a time. For simplicity, we
use the term “test” both for a target which executes test code or for a
“build targets” which verifies that a binary can compile. Conceptually,
FACF performs a “Noisy Binary Search” <span class="citation"
data-cites="Rivest1978">(<a href="#ref-Rivest1978"
role="doc-biblioref">Rivest, Meyer, and Kleitman 1978</a>)</span> (also
called a Rényi-Ulam game <span class="citation"
data-cites="Pelc2002">(<a href="#ref-Pelc2002" role="doc-biblioref">Pelc
2002</a>)</span>) which FACF models under the Bayesian Search Framework
<span class="citation" data-cites="Ben-Or2008">(<a
href="#ref-Ben-Or2008" role="doc-biblioref">Ben-Or and Hassidim
2008</a>)</span>. The input to FACF includes the suspect changes
(“suspects”) that may have broken the test and an estimate collected by
an independent system of how likely it is to fail non-deterministically
without the presence of a deterministic bug, it’s “flakiness”. Much like
a normal binary search, it then divides the search space, executes
tests, and updates a probability distribution based on the outcomes.
Eventually, the system will determine that one of the suspects is above
a set probability threshold (.9999) and is the source of the test
failure, or that none of the suspects is at fault and the original
failure was spurious, due to a flake (non-deterministic failure).</p>
<h2 id="culprit-finding-accuracy">Culprit Finding Accuracy</h2>
<p>Now some caveats: The math behind FACF <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span> assumes that
individual test executions of the same test at either the same version
or different versions are independent statistical events. That is, a
prior or concurrent execution of a test cannot influence a subsequent
execution. Unfortunately, while this assumption is theoretically sound,
in practice this property does not always hold. External factors outside
a program’s code can also influence a test’s execution behavior. For
example, the time of day, day of year, load on the machine, etc., can
all influence the outcome of certain tests. Thus, while we have
configured FACF to have an accuracy of 99.99%, in practice we do not
observe this level of accuracy. As noted in the Introduction, our
observed accuracy was 99.54% in the last 28 day window as of this
writing. This level of accuracy is consistent with the empirical study
conducted in the 2023 paper <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span>.</p>
<p>How was accuracy “observed”? What was the ground truth used? By what
measurement technique? At Google, we continuously randomly sample a
subset of culprit finding execution results as they are completed. The
selected sample is then cross checked by performing extensive reruns
that demand a higher level of accuracy and assume a worse flakiness rate
for the test than used for culprit finding. Additionally, more reruns
are scheduled for a later time to control for time-based effects.
Finally, execution ordering effects are controlled for by ensuring that
executions at versions that should fail can fail both before and after
versions that should pass are executed. Full details on our methods for
verification can be found in the FACF paper <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span>. Even with
extensive reruns there remains a small probability of error. While our
method of verification is imperfect, it allows for continuously
computing a statistical estimate on the accuracy of the culprit
finder.</p>
<p>Measurements of FACF accuracy reported in the 2023 paper were
performed per test breakage. That is, culprit finding is performed on a
test <span class="math inline">\(t\)</span> when it has an observed
failure at some version <span class="math inline">\(\beta\)</span> when
it was previously passing at a prior version <span
class="math inline">\(\alpha\)</span>. The range <span
class="math inline">\((\alpha, \beta]\)</span> is referred to as the
<em>search-range</em> and the key <span class="math inline">\(\{t \times
(\alpha, \beta]\}\)</span> is the <em>search-key</em>. For a given
search-key, the culprit finder first filters the search-range to changes
on which <span class="math inline">\(t\)</span> has a transitive build
dependence, which we call the <em>suspect-set</em>. A culprit <span
class="math inline">\(\kappa\)</span> is identified somewhere between
<span class="math inline">\(\beta\)</span> and <span
class="math inline">\(\alpha\)</span> (e.g. <span
class="math inline">\(\alpha \sqsubset \kappa \sqsubseteq \beta\)</span>
where <span class="math inline">\(a
\sqsubset b\)</span> indicates version <span
class="math inline">\(a\)</span> is before version <span
class="math inline">\(b\)</span>). It is possible that there are
multiple <span class="math inline">\(\kappa_t\)</span> for different
<span class="math inline">\(t\)</span> w.r.t. the same search range,
which we can call the <em>culprit-set</em> of the search range. What is
measured is whether or not <span class="math inline">\(\kappa\)</span>
is correct (true or false) for a given <span
class="math inline">\(t\)</span> and search range <span
class="math inline">\(\alpha \sqsubset \beta\)</span>. Accuracy is the
total correct measurements over the total number of measurements
taken.</p>
<h2 id="applying-culprit-finding-results-to-change-reversion">Applying
Culprit Finding Results to Change Reversion</h2>
<p>When breaking changes get merged into the main development branch
they impede development productivity. At Google we term impediments to
productivity as “friction.” Breaking changes impact three types of
friction tracked at Google: Release Friction, Presubmit Friction and
Development Friction. Release Friction occurs when breaking changes that
merge into the main development branch impede automated releases and
require manual intervention by the primary team’s on-duty engineer.
Presubmit Friction occurs when pre-submission (“presubmit”) testing in
TAP fails due to a broken test in the main development branch.
Development Friction occurs when a developer manually triggers the
execution of a test broken on the main branch during active code
development. All three types of friction can steal time from developers
and reduce productivity.</p>
<p>To reduce friction, changes that break tests can be automatically
reverted (“rolled back”). However, auto-revert (“auto-rollback”) only
improves developer productivity if the changes reverted actually broke
tests. Let’s consider for a moment what happens when a change is
incorrectly reverted. The developer who authored the change now needs to
go on a wholly different troubleshooting journey to determine why their
change didn’t stay submitted, and they are faced with the same
damning-but-incorrect evidence used to revert their change. The
developer is frustrated and must now debug tests that their team doesn’t
own and which their changes should not affect.</p>
<p>Some changes are particularly difficult to submit with a buggy
auto-revert system: those that change core libraries. These changes have
the potential to break a large number of tests and impact a large number
of other developers. But, at the same time they are also more likely to
be incorrectly blamed (as TAP tracks what tests are affected by each
change and uses this as an input to culprit finding). Past auto-revert
systems that have been too inaccurate have caused core library authors
to opt their changes out of auto reversion as the productivity cost to
the core library teams has been too great to bear.</p>
<h2 id="accuracy-of-auto-revert-versus-culprit-finding">Accuracy of
Auto-Revert versus Culprit Finding</h2>
<p>A change <span class="math inline">\(\kappa\)</span> that breaks a
test may have broken more than one. If so, then multiple independent
executions of the culprit finding algorithm (one for each broken test)
will blame the version <span class="math inline">\(\kappa\)</span> as
the culprit. As explored above, the algorithms have an error rate. A
change <span class="math inline">\(\kappa\)</span> that breaks <span
class="math inline">\(n\)</span> tests will have <span
class="math inline">\(m \le n\)</span> culprit finding conclusions that
correctly identify <span class="math inline">\(\kappa\)</span> as the
culprit.</p>
<p>Our concern with auto-revert isn’t the accuracy of culprit finding
per search-key <span class="math inline">\(\{t \times (\alpha,
\beta]\}\)</span> but rather per culprit change <span
class="math inline">\(\kappa\)</span>. Let <span
class="math inline">\(K\)</span> be the set of all changes in the
repository blamed as culprits by the culprit finding system. Let <span
class="math inline">\(c \subseteq K\)</span> be the subset of the
culprits that are correct and <span class="math inline">\(\overline{c}
\subseteq K\)</span> be the subset of culprits that are incorrect. Then
if every culprit in <span class="math inline">\(K\)</span> was
automatically reverted, the accuracy of the auto-revert system would be
<span class="math inline">\({\left|{c}\right|} /
{\left|{K}\right|}\)</span>. In the introduction, we noted that the
accuracy <em>per change</em> was 77.37% which equals <span
class="math inline">\({\left|{c}\right|} /
{\left|{K}\right|}\)</span>.</p>
<p>Why is the auto-revert accuracy (77.37%) lower than the culprit
finding accuracy (99.54%)? Because there are many more tests than
culprits and because correctly identified culprit changes may have
broken more than one test. The chance of finding that culprit is higher
because there are more chances to find it. Additionally, all of the
culprit finding conclusions that identify one of those changes that
break many tests will be correct. The set of incorrectly identified
culprit <span class="math inline">\(\overline{c}\)</span> tends to
contain mostly changes that are blamed by a fewer number of tests than
those in set <span class="math inline">\(c\)</span>.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Identify a method for selecting as many as possible changes from the
set of culprits <span class="math inline">\(K\)</span> that can be
safely reverted. A change is safe to revert if it is a true culprit.
Safety can be defined in terms of probability as well: A change is safe
to revert if the chance it is a true culprit is <span
class="math inline">\(&gt;99\%\)</span>. The number of changes selected
should be maximized (while maintaining safety) to ensure the methods
performs reversions rather than safely doing nothing at all.</p>
<h1 id="automatically-reverting-changes">Automatically Reverting
Changes</h1>


<span id="fig:baseline" label="fig:baseline">
[<em><strong>Figure 2</strong></em>
![Figure 2](images/icst-2024/fig2.png)](images/icst-2024/fig2.png)
</span>

<p>We will describe several methods for selecting changes to revert
while maintaining the intended safety property. The first method is a
simple “baseline” method that the more complex methods will be evaluated
against. The other methods use shallow machine learning systems to take
advantage of metadata available at revert time.</p>
<p><span id="sec:baseline" label="sec:baseline"></span></p>
<p>Our baseline method is based on a single observation: a change <span
class="math inline">\(\kappa_0\)</span> that is identified as the
culprit by culprit finding on many tests is more likely to be a culprit
than a change <span class="math inline">\(\kappa_1\)</span> that is only
blamed by a few tests. Therefore, the simplest heuristic approach to
selecting changes to revert is to threshold on a minimum number of tests
identifying the change. BASELINE(N) will refer to this method with
selecting changes with at least N blaming targets. Figure <a
href="#fig:baseline" data-reference-type="ref"
data-reference="fig:baseline">[fig:baseline]</a> shows the performance
of this method at various minimum number of targets. The most
conservative, BASELINE(50) can avoid most false positives while
reverting only a <span class="math inline">\(\sim\)</span>13% of true
culprits. We choose BASELINE(10) for our final evaluation as this is the
heuristic Google has historically used. Additionally, 10 is the lowest
configuration of BASELINE that generally (although not always) meets the
desired safety criteria.</p>
<h2 id="subsec:ml-models">Predicting Auto-Revert Correctness using
Machine Learning</h2>
<p>Instead of choosing just one suggestive feature – the number of
blaming tests – to decide on a reversion, our proposed method uses
multiple features and shallow machine learning models to improve
performance relative to the baseline method. A selection of easily
obtainable coarse-grained metadata on the changes, tests, and culprit
finding process are used as features to the models. As these features
are mainly simple numerical, categorical, and textual features about the
code, we use simple model architectures to control for over-fitting.
Architectures examined are: Random Forests (RF) and AdaBoost (ADA) (both
provided by scikit-learn<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a> <span class="citation"
data-cites="scikit-learn">(<a href="#ref-scikit-learn"
role="doc-biblioref">Pedregosa et al. 2011</a>)</span>), and XGBoost
(XGB)<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a> <span class="citation"
data-cites="Chen2016a">(<a href="#ref-Chen2016a"
role="doc-biblioref">Chen and Guestrin 2016</a>)</span>. These
tree-based model architectures are easy to use, have low computational
costs, and are robust to the feature representation versus alternatives
that require more preprocessing.</p>
<h2 id="features-used">Features Used</h2>
<p>A suspect change is one that has been identified by at least one
culprit-finder in the process of culprit finding a test. We can consider
this the test <em>blaming</em> the change. For any suspect, we have one
or more blaming tests. Then, we have a few sources of information that
may be valuable for predicting a true breakage: characteristics of the
change itself and characteristics of the blaming tests or culprit
finding results. Table <a href="#table:features-table"
data-reference-type="ref" data-reference="table:features-table">1</a>
contains the grouping of the features against their logical feature
category, arrived at using the Pearson correlation between numerical
features and lift analysis for non-numeric features.</p>
<h2 id="feature-representation">Feature Representation</h2>
<h3 id="categorical-features">Categorical Features</h3>
<p>Given a set of categories, we can create a fixed length
representation by encoding a choice of category as a one-hot vector.
When we have a variable number of categories per instance, such as
language per test, this trivially becomes a multi-hot representation by
summing the vectors.</p>
<h3 id="numerical-features">Numerical Features</h3>
<p>Singular numerical fields are integrated without preprocessing or
normalization, as we expect decision trees to be robust to
value-scaling. Variable-length numerical fields are turned into
variable-length categorical fields by generating bins representing
quantiles of the training set distribution of that value. Then,
bin-mapped values can be condensed into a fixed-length multi-hot
encoding as above.</p>
<h3 id="token-set-features">Token Set Features</h3>
<p>Token Sets correspond to variable-length categorical features where
the list of categories is not known in advance. A good example of such a
feature is compiler flags which should be dealt with adaptively by the
model. We handle this issue by building a vocabulary per token-set
feature on the training set parameterized by a minimum and maximum
across-instance frequency. The change description, while potentially
better dealt with through doc2vec or a more sophisticated NLP embedding
approach, is treated like a token set and the resulting multi-hot
encoding, having each field normalized by document frequency
(TF-IDF).</p>
<p> <span id="table:features-table"
label="table:features-table"></span></p>
<div class="center">
<div id="table:features-table">
<table>
<caption> <span class="smallcaps">Features for Machine Learning
Model.</span> </caption>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BASE</td>
<td style="text-align: left;"># of Blaming Tests</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">CHANGE_CONTENT</td>
<td style="text-align: left;">LOC (changed, added, deleted, total)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"># of Files Changed</td>
</tr>
<tr class="even">
<td style="text-align: left;">File Extensions</td>
</tr>
<tr class="odd">
<td rowspan="3" style="text-align: left;">CHANGE_METADATA</td>
<td style="text-align: left;">Reviewer/Approver Count</td>
</tr>
<tr class="even">
<td style="text-align: left;">Issue/Bug Count</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Robot Author</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">CHANGE_TOKENS</td>
<td style="text-align: left;">Directory Tokens</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Description</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">CULPRIT_FINDER</td>
<td style="text-align: left;"><span
class="math inline">\(|\)</span>Suspect-Set<span
class="math inline">\(|\)</span> of Search-Key</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(|\)</span>Culprit-Set<span
class="math inline">\(|\)</span> of Search-Range</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">FLAKINESS</td>
<td style="text-align: left;">Historical Flaky Identification Count
(Build/Test)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Historical Execution Flake Rate</td>
</tr>
<tr class="even">
<td rowspan="6" style="text-align: left;">TEST_ATTRIBUTES</td>
<td style="text-align: left;">Type of Test (Test vs Build)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Bazel Rule (Ex: “java_test”)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Machine Types (Ex: gpu)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Test Language (Ex: java, c++, etc)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Compiler Flags (ex: sanitizers)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average Machine Cost</td>
</tr>
</tbody>
</table>
</div>
</div>
<p></p>
<h2 id="feature-grouping">Feature Grouping</h2>
<h3 id="logical-sub-grouping-for-feature-lift-analysis">Logical
Sub-Grouping for Feature Lift Analysis</h3>
<p>In order to avoid crowding out the evaluation and analysis with too
many combinations of features, we’ve grouped them in Table <a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table">1</a> into logical groups of
features based off of conceptual categories and to reflect cross-feature
correlations for numerical features, computed via pearsons correlation
coefficient. As our “Token Set" features represent a distinct class of
features both in terms of encoding method and granularity (highly
specific to individual changes), we separate them from the coarse-grain
change features. These logical groups will allow us to clearly convey
the individual signal being provided by each class of feature during our
feature analysis.</p>
<h3 id="subsec:avail-group-features">Availability Sub-Grouping for Model
Comparison Analysis</h3>
<p>Separately, we define the feature sets F = {<em>BASE</em>,
<em>AHEAD-OF-TIME</em>, <em>REAL-TIME</em>, <em>ALL</em>} to distinguish
between features available at any potential inference time</p>
<ol>
<li><p><em>BASE</em> is just the number of blaming tests. This
corresponds to the BASELINE heuristic method.</p></li>
<li><p><em>AHEAD-OF-TIME</em> features are properties of the change
under suspicion, and are therefore available immediately. Features under
CHANGE_CONTENT, CHANGE_METADATA, and CHANGE_TOKENS from Table <a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table">1</a> belong here.</p></li>
<li><p><em>REAL-TIME</em> features are available only as a result of
culprit finding processes at play continuously during the decision
window, and can become available at different times based on different
sources of generation, often after a change has already been suspected
by a culprit finder – all other features from Table <a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table">1</a> fall in this set.</p></li>
<li><p><em>ALL</em> features is the union of <em>AHEAD-OF-TIME</em> and
<em>REAL-TIME</em>.</p></li>
</ol>
<p>This distinction is important: the usefulness of automatically
reverting changes is critically dependent on its latency from the point
of discovering that a breakage exists. The longer we take to
automatically revert the change, the more friction experienced by
developers, and the more likely a developer would have had to manually
intervene.</p>
<h3 id="subsubsec:feature-min">Feature Minimization</h3>
<p>Given the demonstrated feature lift, we’re also interested in
minimizing the features needed to achieve similar performance levels
without over-fitting on redundant features. We’ll elaborate on the exact
minimized feature sets in the evaluation section for feature analysis,
where we use representative features from each logical sub-groups that
provide significant lift individually. Thus we have 3 further feature
sets, MIN(f) for <span class="math inline">\(f \in F\)</span>.
Representative features are determined over the test set based on
individual feature comparison, left out for brevity in the analysis,
rather than the logical category evaluation presented here. We evaluate
RF(f), ADA(f), XGB(f), RF(MIN(f)), ADA(MIN(f)), and XGB(MIN(f)) for each
feature set <span class="math inline">\(f \in F\)</span>.</p>
<h2
id="thresholding-to-use-model-prediction-score-for-selection">Thresholding
to use Model Prediction Score for Selection</h2>
<p>Models produce an output probability score from 0 to 1. To discretize
these scores into actual change selections. We dynamically pick a
threshold using the TEST dataset to select the threshold that minimizes
the bad revert rate while maximizing a positive, non-zero recall.</p>
<h2 id="hyperparams-for-ml-models">Hyperparams for ML Models</h2>
<p>Using scikit-learn, we define RF as the RandomForestClassifier
parameterized with a depth of 16 and ADA as the AdaBoostClassifier with
default parameters, each of which provide discretized predictions with
the above threshold selection procedure. XGBoost is configured with:
objective=‘binary:logistic’, eta=0.05, and max_delta_step = 1.</p>
<h1 id="sec:evaluation">Empirical Evaluation</h1>
<p>We empirically evaluated SafeRevert (the ML based method) against the
BASELINE heuristic method. We evaluate: the overall performance of the
different ML models considered, the importance features used, and
compare the BASELINE method to a selected ML model.</p>
<h2 id="research-questions">Research Questions</h2>
<ol>
<li><p>What is the safety and performance of studied methods in the
context of the developer workflow?</p></li>
<li><p>What is the marginal benefit provided by each feature?</p></li>
<li><p>Did the chosen method significantly improve performance over the
baseline method while maintaining required safety levels?</p></li>
</ol>
<h2 id="sec:safety">Measuring Safety</h2>
<p>As mentioned in section <a href="#intro:auto-revert"
data-reference-type="ref" data-reference="intro:auto-revert">1.4</a>, an
incorrectly reverted change causes unacceptably high developer toil.
Safety is the likelihood that a method will produce a false positive
result by incorrectly categorizing a change as being a culprit and
reverting it. This is a referred to as a <em>Bad Revert</em> while
correctly reverting a change that introduced a bug is a <em>Good
Revert</em>. The total number of reverts is <em>Bad Reverts</em> +
<em>Good Reverts</em>. A change that should have been reverted but
wasn’t is a <em>Missed Good Revert</em> and a change that was correctly
not reverted is a <em>Avoided Bad Revert</em>.</p>
<p>In terms of classic terminology for evaluating binary
classifiers:</p>
<ol>
<li><p><em>Good Revert</em> = True Positive (TP)</p></li>
<li><p><em>Bad Revert</em> = False Positive (FP)</p></li>
<li><p><em>Avoided Bad Revert</em> = True Negative (TN)</p></li>
<li><p><em>Missed Good Revert</em> = False Negative (FN)</p></li>
</ol>
<p>The safety properties we are most interested are (a) the total number
of bad reverts, (b) total number of bad reverts per day, and (c) the bad
revert rate. The bad revert rate (BRR) is the percentage of bad reverts
out of the total number of reverts <span class="math inline">\(=
\frac{FP}{TP + FP}\)</span>. This is otherwise known as the False
Discovery Rate (FDR). Note, that <span class="math inline">\(FDR = 1 -
Precision = 1 - \frac{TP}{TP + FP}\)</span>. Thus, safety can either be
stated in terms of precision (ex. precision must be above 99%) or in
terms of bad revert rate (ex. bad revert rate must be below 1%).</p>
<p><em>In this evaluation we will consider a method safe if its bad
revert rate is below 1%, there are fewer than 14 bad reverts in the
final validation set, and there are no more than 5 bad reverts per
day.</em></p>
<p><em>Why these numbers?</em> Our end goal is a production SafeRevert.
Our small team supports a large number of services and users of TAP. We
need to minimize toil for the 2 engineers per week who are “oncall” for
them. The numbers above were selected to be manageable for us in terms
of the overhead required for communicating with our users and
investigating the root cause of bad reverts. While these numbers are
subjective and dependent on our context, they are meaningful to our
team. We expect other teams supporting central CI systems would make
similar choices.</p>
<h2 id="sec:performance">Measuring Performance</h2>
<p>If a method is deemed safe (meets above criteria) then it is an
eligible method to be used to pick changes to revert. To determine
whether one safe reversion method is better than another we look at the
how many <em>Good Reverts</em> a method is able to achieve out of the
total possible good reverts. This corresponds to the metric known as
recall <span class="math inline">\(= \frac{TP}{TP + FN}\)</span>. The
higher a safe method’s recall the better it performs. As with safety, we
prefer methods that are consistent and have low variability in their
reversion recall per day. We then define performance as the number of
breaking changes successfully reverted both in total and per-day.</p>
<h2 id="sec:dataset">Evaluation Dataset</h2>
<p>Our evaluation dataset consists of roughly 3.5 months of data split
between a training, test, and validation set and contains <span
class="math inline">\(\thicksim\)</span>25,137 unique changes identified
by the production culprit finder as culprits. Each row has a boolean
indicating whether followup verification confirmed the change was indeed
a culprit. This verification continues to be done in the manner
described in our 2023 paper <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span>.</p>
<p>For training and evaluation we produce a time-based split: TRAIN
consists of the first 2.5 months, TEST the next <span
class="math inline">\(\sim\)</span>2 weeks, and VALIDATION the final
<span class="math inline">\(\sim\)</span>2 weeks. Time based splits
avoid cross-contaminating our training with diffuse information about
types of breakages that may be based on time-dependent attributes of the
codebase. It is important that the training data is uncontaminated with
any data from the time period where the evaluation occurs. If it is, the
evaluation will not reflect the performance observed in production.</p>
<p>All comparative model and feature evaluation was performed against
the TEST set in order to determine our optimal ML model configuration,
OPTIMAL. We then evaluate OPTIMAL and BASELINE against the VALIDATION
set.</p>



<h1 id="sec:results">Results</h1>

<span id="table:feature-ablation" label="table:feature-ablation">
<span id="table:model-comparison" label="table:model-comparison">
<span id="table:featureMinimization" label="table:featureMinimization">
<span id="table:validation-evaluation" label="table:validation-evaluation">
[<em><strong>Tables 2-5</strong></em>
![Tables 2-5](images/icst-2024/tables2-5.png)](images/icst-2024/tables2-5.png)
</span>
</span>
</span>
</span>

<p><em><strong>RQ1</strong>: What is the safety and performance of
studied methods in the context of the developer workflow?</em> <span
id="result:rq1" label="result:rq1"></span></p>
<div class="tcolorbox">
<p><em><strong>Summary</strong></em>: The XGB(ALL) was best overall,
with a recall of 61.2%, and a bad revert rate of .3%.</p>
</div>
<div class="tcolorbox">
<p><em><strong>Note</strong></em>: This experiment was conducted on the
TEST data set as the VALIDATION set was reserved for RQ3.</p>
</div>
<p>Table <a href="#table:model-comparison" data-reference-type="ref"
data-reference="table:model-comparison">[table:model-comparison]</a>
summarizes the critical metrics for safety (bad revert rate) and
performance (recall) for the three model types considered: RandomForest
[RF], AdaBoost [ADA], and XGBoost [XGB]. Figure <a
href="#fig:model-comparison" data-reference-type="ref"
data-reference="fig:model-comparison">[fig:model-comparison]</a> shows
the Receiver Operator Curves (ROC) for all 3 model types and their
associated area under the curve (AUC) values. The ROC curve better
summarizes model performance over a wider range of target objectives
than the table which is focused on the safest configuration (minimizing
Bad Revert Rate). For instance if a different application had a higher
tolerance for bad reversion/false positives the ROC curves show that you
could achieve a 90% true positive rate with a 20% false positive
rate.</p>
<p><strong>Safety</strong>: In general, XGB and ADA both outperformed
the RF in terms of safety. Under none of the test configurations did the
RF have a safe configuration. XGB and ADA are safe (under our criteria,
see Section <a href="#sec:safety" data-reference-type="ref"
data-reference="sec:safety">4.2</a>) using all the features or the
REAL-TIME subset. The safest configuration was XGB(ALL) which had only 5
bad reverts and a bad revert rate of 0.003. XGB(REAL-TIME) and ADA(ALL)
has the same or smaller number of bad reverts but a worse rate due to
their lower recall.</p>
<p><strong>Performance</strong>: The highest performing configuration
was XGB(ALL) which had 1980 Good Reverts in the Test set and a recall of
63.2%. The recall rate of XGB(REAL-TIME) was 52.2% indicating that most
of the model performance is gained from the REAL-TIME features, not from
the AHEAD-OF-TIME features which performed much worse while being
unsafe. ADA(ALL) was safe but its recall rate was only 42.2% which lower
than both safe XGB configurations.</p>


<p></p>
<p> <em><strong>RQ2</strong>: What is the marginal
benefit provided by each feature group</em>? <span id="result:rq2"
label="result:rq2"></span></p>
<div class="tcolorbox">
<p><em><strong>Summary</strong></em>: Historical flakiness data was the
most valuable feature set, boosting model recall by 17%, while change
metadata was the least important.</p>
</div>
<div class="tcolorbox">
<p><em><strong>Note</strong></em>: This experiment was conducted on the
TEST data set as the VALIDATION set was reserved for RQ3.</p>
</div>
<p>To determine marginal benefit we perform an ablation study to look at
the marginal benefit provided by each feature (as grouped in Table <a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table">1</a>). The ablation study was
performed using the XGBoost model which was chosen over AdaBoost due to
its higher recall as seen in Table <a href="#table:model-comparison"
data-reference-type="ref"
data-reference="table:model-comparison">[table:model-comparison]</a>.
Three marginal benefit experiments were performed:</p>
<ol>
<li><p>Measure the performance of the model trained against
<strong>only</strong> the features in a single feature set. We visualize
the results in Figure <a href="#fig:feature-group-contrib"
data-reference-type="ref"
data-reference="fig:feature-group-contrib">[fig:feature-group-contrib]</a>
as an ROC curve. Table <a href="#table:feature-ablation"
data-reference-type="ref"
data-reference="table:feature-ablation">[table:feature-ablation]</a>
contains the critical safety and performance metrics of Bad Reverts
(safety) and Recall (performance) as well as the AUC which summarizes
overall model predictive performance – generally a model with a higher
AUC will be more performant than one with a lower AUC.</p></li>
<li><p>Measure the <strong>Positive(+) <em>Lift</em></strong> provided
by adding a single feature group to our BASE feature (the number of
blaming tests). For instance, we would add the features in
CHANGE_CONTENT to create a XGB(CHANGE_CONTENT + BASE) model. The
<strong>+Lift</strong> for Recall is computed as
Recall[XGB(CHANGE_CONTENT + BASE)] - Recall[XGB(BASE)] and similar for
AUC of the ROC curve. The results of this analysis are shown in Table <a
href="#table:feature-ablation" data-reference-type="ref"
data-reference="table:feature-ablation">[table:feature-ablation]</a>.</p></li>
<li><p>Measure the <strong>Negative(–) <em>Lift</em></strong> provided
by removing a single feature group from out ALL feature set. For
instance we would remove the features in CHANGE_CONTENT to create a
XGB(ALL - CHANGE_CONTENT) model. The <strong>–Lift</strong> for Recall
is computed as Recall[XGB(ALL)] - Recall[XGB(ALL - CHANGE_CONTENT)] and
similar for AUC of the ROC curve. The results of this analysis are shown
in Table <a href="#table:feature-ablation" data-reference-type="ref"
data-reference="table:feature-ablation">[table:feature-ablation]</a>.</p></li>
</ol>
<p>As discussed in section <a href="#subsubsec:feature-min"
data-reference-type="ref"
data-reference="subsubsec:feature-min">3.4.3</a>, we attempted to
minimize the feature subsets while providing comparable performance to
the full feature sets. Table <a href="#table:featureMinimization"
data-reference-type="ref"
data-reference="table:featureMinimization">[table:featureMinimization]</a>
contains a breakdown of the performance for these minimal subsets.</p>
<p> <em><strong>RQ3</strong>:Did the chosen method
improve performance over the baseline method while maintaining required
safety levels?</em> <span id="result:rq3" label="result:rq3"></span></p>
<div class="tcolorbox">
<p><em><strong>Summary</strong></em>: XGB(ALL) improved overall recall
while meeting our safety requirements with a Bad Revert Rate under 1%
and an average of 0.6 Bad Reverts per Day. BASELINE(10) had a recall of
26.3% while XGB(ALL) had a recall of 55.7% – an improvement of <span
class="math inline">\(\sim 2.1\times\)</span>.</p>
</div>
<div class="tcolorbox">
<p><em><strong>Note</strong></em>: We used the reserved VALIDATION data
set for this research question.</p>
</div>
<p>Table <a href="#table:validation-evaluation"
data-reference-type="ref"
data-reference="table:validation-evaluation">[table:validation-evaluation]</a>
shows per day metrics comparing XGB(ALL) against the heuristic
BASELINE(10) method. XGB(ALL) was selected as our “OPTIMAL” model to
compare against the BASELINE method given its performance in the TEST
set as evaluated in RQ1. As a reminder the BASELINE method thresholds
the number of blaming tests to a fixed number. BASELINE(10)’s average
bad reverts per day is 0.86 and its total Bad Revert Rate (BRR) is 1.5%.
XGB(ALL) achieves the overall safety limit with a BRR of 0.5% and has an
average bad reverts per day of 0.6 which meets our safety threshold.</p>
<p>XGB(ALL)’s bad revert rate outperforms BASELINE(10), with a recall
 2.1 times higher than BASELINE(10). XGB(ALL) made 1,823 total good
reverts in the validation set while BASELINE(10) performed 860 good
reverts. Based on the data in Table <a
href="#table:validation-evaluation" data-reference-type="ref"
data-reference="table:validation-evaluation">[table:validation-evaluation]</a>
XGB(ALL) is both safer and more performant than BASELINE(10) on the
validation data set.</p>
<h2 id="discussion">Discussion</h2>
<p>The proposed method, SafeRevert, is generic and can be used with any
culprit finding algorithm. By grouping individual features from
different sources of data into logical feature sets (Table <a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table">1</a>), performing a detailed
feature ablation study (Fig <a href="#fig:feature-group-contrib"
data-reference-type="ref"
data-reference="fig:feature-group-contrib">[fig:feature-group-contrib]</a>),
and running the model on a minimal set of features (Table <a
href="#table:featureMinimization" data-reference-type="ref"
data-reference="table:featureMinimization">[table:featureMinimization]</a>),
we hope to provide a template via which teams in other contexts build on
when adopting the approach we outline. In particular, while some
features used may be Google specific, our feature ablation study can be
replicated on different features in other software development
organizations. While we don’t expect a team implementing SafeRevert to
achieve the exact Recall and Bad Revert Rates we report we do expect
this method to out perform the BASELINE method once an appropriate set
of features is identified.</p>
<h2 id="threats-to-validity">Threats to Validity</h2>
<p>Our dataset may misrepresent information available at inference time
in the forthcoming service as it may include information not available
to us at our decision time. This is due to using offline data in the
dataset as the production system based on this paper is currently under
construction. We adjust this threat by evaluating performance restricted
to features available independent of any culprit finding event and
restricting our real-time data to a time bound relative to change
submission time.</p>
<p>The data presented in this final manuscript differs slightly than the
reviewed manuscript. At the time of review approximated 15% of the
dataset was lacking verification results (collected using the method
described by Henderson <span class="citation"
data-cites="Henderson2023">(<a href="#ref-Henderson2023"
role="doc-biblioref">Henderson et al. 2023</a>)</span>). This was
disclosed in this section to the reviewers and we made two conservative
assumptions: 1) any culprit change lacking verification results was
considered a false positive and 2) we assumed the BASELINE method was
correct for those changes (inflating the BASELINE methods performance
versus the studied ML methods for SafeRevert). Since the peer review was
completed, a bug in the verification system was identified and fixed.
The bug caused a proportion of incorrect culprit changes to “get stuck”
in a queue waiting for an additional test execution due to a typo in a
comparison (using <code>&gt;</code> instead of <code>&gt;=</code>). Once
the bug was fixed, the research team was able to rerun the study with
the additional label data.</p>
<p>Post-rerun we observed: 1) the BASELINE method performed worse and 2)
the studied methods were robust to the change in labeling data. In the
reviewed manuscript, XGB(ALL) had the following results in RQ3: 13 Total
Bad Reverts, 1817 Good Reverts, 0.7% Bad Revert Rate, and 55.8% Recall;
BASELINE(5) was compared against and had 8 Total Bad Reverts, 1286 Good
Reverts, 0.6% Bad Revert Rate, and 39.4% Recall. Compare against Table
<a href="#table:validation-evaluation" data-reference-type="ref"
data-reference="table:validation-evaluation">[table:validation-evaluation]</a>
and Figure <a href="#fig:baseline" data-reference-type="ref"
data-reference="fig:baseline">[fig:baseline]</a>. We switched to using
BASELINE(10) for this final manuscript as it is the production method
currently used at Google.</p>
<h1 id="related-work">Related Work</h1>
<p>In this paper, we are presenting what we believe to be a novel
problem to the wider software engineering community: how to safely
choose changes to revert with incomplete but suggestive evidence. This
problem relies on identifying these problematic changes. In our case we
identify the problematic changes to revert via automated culprit finding
<span class="citation"
data-cites="Couder2008 Ziftci2013a Ziftci2017 Saha2017 Najafi2019a Beheshtian2022 keenan2019 An2021 Ocariza2022 Henderson2023">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>; <a
href="#ref-Ziftci2013a" role="doc-biblioref">Ziftci and Ramavajjala
2013</a>; <a href="#ref-Ziftci2017" role="doc-biblioref">Ziftci and
Reardon 2017</a>; <a href="#ref-Saha2017" role="doc-biblioref">Saha and
Gligoric 2017</a>; <a href="#ref-Najafi2019a"
role="doc-biblioref">Najafi, Rigby, and Shang 2019</a>; <a
href="#ref-Beheshtian2022" role="doc-biblioref">Beheshtian, Bavand, and
Rigby 2022</a>; <a href="#ref-keenan2019" role="doc-biblioref">Keenan
2019</a>; <a href="#ref-An2021" role="doc-biblioref">An and Yoo
2021</a>; <a href="#ref-Ocariza2022" role="doc-biblioref">Ocariza
2022</a>; <a href="#ref-Henderson2023" role="doc-biblioref">Henderson et
al. 2023</a>)</span>.</p>
<p>Culprit finding’s development has often occurred outside of the
academic literature. The first reference to the process of using binary
search to identify a bug was in Yourdon’s 1975 book on Program Design:
Page 286, Figure 8.3 titled “A binary search for bugs” <span
class="citation" data-cites="Yourdon1975">(<a href="#ref-Yourdon1975"
role="doc-biblioref">Yourdon 1975</a>)</span>. The process is explained
in detail in the <code>BUG-HUNTING</code> file in the Linux source tree
in 1996 by Larry McVoy <span class="citation"
data-cites="McVoy1996 Cox2023">(<a href="#ref-McVoy1996"
role="doc-biblioref">McVoy 1996</a>; <a href="#ref-Cox2023"
role="doc-biblioref">Cox, Ness, and McVoy 2023</a>)</span>. By 1997,
Brian Ness had created a binary search based system for use at Cray with
their “Unicos Source Manager” version control system <span
class="citation" data-cites="Ness1997 Cox2023">(<a href="#ref-Ness1997"
role="doc-biblioref">Ness and Ngo 1997</a>; <a href="#ref-Cox2023"
role="doc-biblioref">Cox, Ness, and McVoy 2023</a>)</span>. Previously
in <span class="citation" data-cites="Henderson2023">(<a
href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
2023</a>)</span> we had credited Linus Torvalds for the invention based
on lack of findable antecedent for his work on the
<code>git bisect</code> command <span class="citation"
data-cites="Couder2008">(<a href="#ref-Couder2008"
role="doc-biblioref">Couder 2008</a>)</span>. We regret the error and
recognize the above individuals for their important contributions.</p>
<p>Other methods for identifying buggy code or breaking changes have
been studied (some widely) in the literature. Fault Localization looks
to identify the buggy code that is causing either operational failure or
test failures <span class="citation"
data-cites="Agarwal2014 Wong2016">(<a href="#ref-Agarwal2014"
role="doc-biblioref">Agarwal and Agrawal 2014</a>; <a
href="#ref-Wong2016" role="doc-biblioref">Wong et al. 2016</a>)</span>.
Methods for fault localization include: delta debugging <span
class="citation" data-cites="Zeller1999">(<a href="#ref-Zeller1999"
role="doc-biblioref">Zeller 1999</a>)</span>, statistical coverage based
fault localization <span class="citation"
data-cites="Jones2002 Jones2005 Lucia2014 Henderson2018 Henderson2019 Kucuk2021">(<a
href="#ref-Jones2002" role="doc-biblioref">J. a. Jones, Harrold, and
Stasko 2002</a>; <a href="#ref-Jones2005" role="doc-biblioref">J. A.
Jones and Harrold 2005</a>; <a href="#ref-Lucia2014"
role="doc-biblioref">Lucia et al. 2014</a>; <a href="#ref-Henderson2018"
role="doc-biblioref">Henderson and Podgurski 2018</a>; <a
href="#ref-Henderson2019" role="doc-biblioref">Henderson, Podgurski, and
Kucuk 2019</a>; <a href="#ref-Kucuk2021" role="doc-biblioref">Kucuk,
Henderson, and Podgurski 2021</a>)</span>, information retrieval (which
may include historical information) <span class="citation"
data-cites="Zhou2012 Youm2015 Ciborowska2022">(<a href="#ref-Zhou2012"
role="doc-biblioref">J. Zhou, Zhang, and Lo 2012</a>; <a
href="#ref-Youm2015" role="doc-biblioref">Youm et al. 2015</a>; <a
href="#ref-Ciborowska2022" role="doc-biblioref">Ciborowska and Damevski
2022</a>)</span>, and program slicing <span class="citation"
data-cites="Podgurski1990 Horwitz1992 Ren2004">(<a
href="#ref-Podgurski1990" role="doc-biblioref">Podgurski and Clarke
1990</a>; <a href="#ref-Horwitz1992" role="doc-biblioref">Horwitz and
Reps 1992</a>; <a href="#ref-Ren2004" role="doc-biblioref">Ren et al.
2004</a>)</span>.</p>
<p>Bug Prediction attempts to predict if a change, method, class, or
file is likely to contain a bug <span class="citation"
data-cites="Lewis2013 Punitha2013 Osman2017">(<a href="#ref-Lewis2013"
role="doc-biblioref">Lewis et al. 2013</a>; <a href="#ref-Punitha2013"
role="doc-biblioref">Punitha and Chitra 2013</a>; <a
href="#ref-Osman2017" role="doc-biblioref">Osman et al.
2017</a>)</span>. This idea has conceptual similarities to the work we
are doing in this paper where we are using similar metrics to predict
whether or not a change which has been implicated by culprit finding is
in fact the change that introduced the bug. Our methods could be
potentially improved by incorporating the additional features used in
the bug prediction work such as the change complexity, code complexity,
and object-oriented complexity metrics.</p>
<p>Test Case Selection <span class="citation"
data-cites="Leon2003 Engstrom2010 Zhou2010 Mondal2015 Musa2015 pan2022">(<a
href="#ref-Leon2003" role="doc-biblioref">Leon and Podgurski 2003</a>;
<a href="#ref-Engstrom2010" role="doc-biblioref">Engström, Runeson, and
Skoglund 2010</a>; <a href="#ref-Zhou2010" role="doc-biblioref">Z. Q.
Zhou 2010</a>; <a href="#ref-Mondal2015" role="doc-biblioref">Mondal,
Hemmati, and Durocher 2015</a>; <a href="#ref-Musa2015"
role="doc-biblioref">Musa et al. 2015</a>; <a href="#ref-pan2022"
role="doc-biblioref">Pan et al. 2022</a>)</span> and Test Case
Prioritization <span class="citation"
data-cites="Singh2012 DeS.CamposJunior2017 DeCastro-Cabrera2020 pan2022">(<a
href="#ref-Singh2012" role="doc-biblioref">Singh et al. 2012</a>; <a
href="#ref-DeS.CamposJunior2017" role="doc-biblioref">de S. Campos
Junior et al. 2017</a>; <a href="#ref-DeCastro-Cabrera2020"
role="doc-biblioref">De Castro-Cabrera, García-Dominguez, and
Medina-Bulo 2020</a>; <a href="#ref-pan2022" role="doc-biblioref">Pan et
al. 2022</a>)</span> are related problems to the change reversion
problem we study. Instead of predicting whether or not a change caused a
known test failure in Test Case Selection/Prioritization often the
change is used to predict whether a given test will fail before it is
run. There is a large body work in that uses dynamic information from
past test executions (such as code coverage) to inform the selection
process. We believe this hints that such information could be highly
informative for the change reversion problem as well.</p>
<p>Finally, there are a family of methods for finding bug inducing
commits for the purpose of supporting studies that data mine software
repositories <span class="citation"
data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019 An2023">(<a
href="#ref-Sliwerski2005" role="doc-biblioref">Śliwerski, Zimmermann,
and Zeller 2005</a>; <a href="#ref-Rodriguez-Perez2018"
role="doc-biblioref">Rodríguez-Pérez, Robles, and González-Barahona
2018</a>; <a href="#ref-Borg2019" role="doc-biblioref">Borg et al.
2019</a>; <a href="#ref-Wen2019" role="doc-biblioref">Wen et al.
2019</a>; <a href="#ref-An2023" role="doc-biblioref">An et al.
2023</a>)</span>. These methods typically used to conduct a historical
analysis of a repository rather than as an online detection as in
culprit finding.</p>
<h1 id="sec:conclusion">Conclusion</h1>
<p>We presented: SafeRevert a method for improving systems that
automatically revert changes that break tests. SafeRevert was developed
as a way to improve the number bad changes automatically reverted while
maintaining safety (rarely reverting good changes). To evaluate
SafeRevert, we performed an empirical evaluation comparing the
performance of SafeRevert against the baseline method that is currently
utilized in production which utilizes a simple heuristic to determine if
a change is safe to revert: the number of tests which “blame” the
culprit change. When evaluating RQ3 in Section <a href="#sec:results"
data-reference-type="ref" data-reference="sec:results">5</a>, it was
observed that the XGB(ALL) configuration of SafeRevert doubled the
number changes reverted while reducing the number of bad reverts
performed (see Table <a href="#table:validation-evaluation"
data-reference-type="ref"
data-reference="table:validation-evaluation">[table:validation-evaluation]</a>).</p>
<p>While it is unlikely that a replication study in a different
development environment would reproduce our exact results we do expect
based on the robust difference observed between SafeRevert and BASELINE
that SafeRevert (or similar ML based method) will be able to improve the
number of changes eligible for automatic reversion. We hope that by
introducing this problem to the larger software engineering community
that new and innovative approaches to solving it will be developed.</p>


<h1 id="sec:References">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<ol>
<li><div id="ref-Agarwal2014" class="csl-entry" role="doc-biblioentry">
Agarwal, Pragya, and Arun Prakash Agrawal. 2014.
<span>“Fault-Localization <span>Techniques</span> for <span>Software
Systems</span>: <span>A Literature Review</span>.”</span> <em>SIGSOFT
Softw. Eng. Notes</em> 39 (5): 1–8. <a
href="https://doi.org/10.1145/2659118.2659125">https://doi.org/10.1145/2659118.2659125</a>.
</div></li>
<li><div id="ref-An2023" class="csl-entry" role="doc-biblioentry">
An, Gabin, Jingun Hong, Naryeong Kim, and Shin Yoo. 2023. <span>“Fonte:
<span>Finding Bug Inducing Commits</span> from
<span>Failures</span>.”</span> <span>arXiv</span>. <a
href="http://arxiv.org/abs/2212.06376">http://arxiv.org/abs/2212.06376</a>.
</div></li>
<li><div id="ref-An2021" class="csl-entry" role="doc-biblioentry">
An, Gabin, and Shin Yoo. 2021. <span>“Reducing the Search Space of Bug
Inducing Commits Using Failure Coverage.”</span> In <em>Proceedings of
the 29th <span>ACM Joint Meeting</span> on <span>European Software
Engineering Conference</span> and <span>Symposium</span> on the
<span>Foundations</span> of <span>Software Engineering</span></em>,
1459–62. <span>Athens Greece</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3468264.3473129">https://doi.org/10.1145/3468264.3473129</a>.
</div></li>
<li><div id="ref-Ananthanarayanan2019" class="csl-entry"
role="doc-biblioentry">
Ananthanarayanan, Sundaram, Masoud Saeida Ardekani, Denis Haenikel,
Balaji Varadarajan, Simon Soriano, Dhaval Patel, and Ali Reza
Adl-Tabatabai. 2019. <span>“Keeping Master Green at Scale.”</span>
<em>Proceedings of the 14th EuroSys Conference 2019</em>. <a
href="https://doi.org/10.1145/3302424.3303970">https://doi.org/10.1145/3302424.3303970</a>.
</div></li>
<li><div id="ref-Beheshtian2022" class="csl-entry" role="doc-biblioentry">
Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby.
2022. <span>“Software <span>Batch Testing</span> to <span>Save Build
Test Resources</span> and to <span>Reduce Feedback Time</span>.”</span>
<em>IEEE Transactions on Software Engineering</em> 48 (8): 2784–2801. <a
href="https://doi.org/10.1109/TSE.2021.3070269">https://doi.org/10.1109/TSE.2021.3070269</a>.
</div></li>
<li><div id="ref-Ben-Or2008" class="csl-entry" role="doc-biblioentry">
Ben-Or, Michael, and Avinatan Hassidim. 2008. <span>“The <span>Bayesian
Learner</span> Is <span>Optimal</span> for <span>Noisy Binary
Search</span> (and <span>Pretty Good</span> for <span>Quantum</span> as
<span>Well</span>).”</span> In <em>2008 49th <span>Annual IEEE
Symposium</span> on <span>Foundations</span> of <span>Computer
Science</span></em>, 221–30. <span>Philadelphia, PA, USA</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/FOCS.2008.58">https://doi.org/10.1109/FOCS.2008.58</a>.
</div></li>
<li><div id="ref-Bland2012" class="csl-entry" role="doc-biblioentry">
Bland, Mike. 2012. <span>“The <span>Chris</span>/<span>Jay Continuous
Build</span>.”</span> Personal {{Website}}. <em>Mike Bland’s Blog</em>.
<a href="https://mike-bland.com/2012/06/21/chris-jay-continuous-
                  build.html">https://mike-bland.com/2012/06/21/chris-jay-continuous-
build.html</a>.
</div></li>
<li><div id="ref-Borg2019" class="csl-entry" role="doc-biblioentry">
Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
<span>“<span>SZZ</span> Unleashed: An Open Implementation of the
<span>SZZ</span> Algorithm - Featuring Example Usage in a Study of
Just-in-Time Bug Prediction for the <span>Jenkins</span>
Project.”</span> In <em>Proceedings of the 3rd <span>ACM SIGSOFT
International Workshop</span> on <span>Machine Learning
Techniques</span> for <span>Software Quality Evaluation</span> -
<span>MaLTeSQuE</span> 2019</em>, 7–12. <span>Tallinn, Estonia</span>:
<span>ACM Press</span>. <a
href="https://doi.org/10.1145/3340482.3342742">https://doi.org/10.1145/3340482.3342742</a>.
</div></li>
<li><div id="ref-Chen2016a" class="csl-entry" role="doc-biblioentry">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“<span>XGBoost</span>:
<span>A Scalable Tree Boosting System</span>.”</span> In <em>Proceedings
of the 22nd <span>ACM SIGKDD International Conference</span> on
<span>Knowledge Discovery</span> and <span>Data Mining</span></em>,
785–94. <span>San Francisco California USA</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>.
</div></li>
<li><div id="ref-Ciborowska2022" class="csl-entry" role="doc-biblioentry">
Ciborowska, Agnieszka, and Kostadin Damevski. 2022. <span>“Fast
Changeset-Based Bug Localization with <span>BERT</span>.”</span> In
<em>Proceedings of the 44th <span>International Conference</span> on
<span>Software Engineering</span></em>, 946–57. <span>Pittsburgh
Pennsylvania</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3510003.3510042">https://doi.org/10.1145/3510003.3510042</a>.
</div></li>
<li><div id="ref-Couder2008" class="csl-entry" role="doc-biblioentry">
Couder, Christian. 2008. <span>“Fighting Regressions with Git
Bisect.”</span> <em>The Linux Kernel Archives</em> 4 (5). <a
href="https://www. kernel. org/pub/software/scm/git/doc s/git-
                  bisect-lk2009.html">https://www. kernel.
org/pub/software/scm/git/doc s/git- bisect-lk2009.html</a>.
</div></li>
<li><div id="ref-Cox2023" class="csl-entry" role="doc-biblioentry">
Cox, Russ, Brian Ness, and Larry McVoy. 2023. <span>“Comp.lang.compilers
"Binary Search Debugging of Compilers".”</span> <a
href="https://groups.google.com/g/comp.compilers/c/vGh4s3HBQ-s/m/
                  Chvpu7vTAgAJ">https://groups.google.com/g/comp.compilers/c/vGh4s3HBQ-s/m/
Chvpu7vTAgAJ</a>.
</div></li>
<li><div id="ref-DeCastro-Cabrera2020" class="csl-entry"
role="doc-biblioentry">
De Castro-Cabrera, M. Del Carmen, Antonio García-Dominguez, and
Inmaculada Medina-Bulo. 2020. <span>“Trends in Prioritization of Test
Cases: 2017-2019.”</span> <em>Proceedings of the ACM Symposium on
Applied Computing</em>, 2005–11. <a
href="https://doi.org/10.1145/3341105.3374036">https://doi.org/10.1145/3341105.3374036</a>.
</div></li>
<li><div id="ref-DeS.CamposJunior2017" class="csl-entry"
role="doc-biblioentry">
de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David,
Regina Braga, Fernanda Campos, and Victor Ströele. 2017. <span>“Test
<span>Case Prioritization</span>: <span>A Systematic Review</span> and
<span>Mapping</span> of the <span>Literature</span>.”</span> In
<em>Proceedings of the 31st <span>Brazilian Symposium</span> on
<span>Software Engineering</span></em>, 34–43. <span>New York, NY,
USA</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3131151.3131170">https://doi.org/10.1145/3131151.3131170</a>.
</div></li>
<li><div id="ref-Engstrom2010" class="csl-entry" role="doc-biblioentry">
Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. <span>“A
Systematic Review on Regression Test Selection Techniques.”</span>
<em>Information and Software Technology</em> 52 (1): 14–30. <a
href="https://doi.org/10.1016/j.infsof.2009.07.001">https://doi.org/10.1016/j.infsof.2009.07.001</a>.
</div></li>
<li><div id="ref-Fowler2006" class="csl-entry" role="doc-biblioentry">
Fowler, Martin. 2006. <span>“Continuous
<span>Integration</span>.”</span> <a
href="https://martinfowler.com/articles/
                  continuousIntegration.html">https://martinfowler.com/articles/
continuousIntegration.html</a>.
</div></li>
<li><div id="ref-Gupta2011" class="csl-entry" role="doc-biblioentry">
Gupta, Pooja, Mark Ivey, and John Penix. 2011. <span>“Testing at the
Speed and Scale of <span>Google</span>.”</span> <a
href="http://google-engtools.blogspot.com/2011/06/testing-at-
                  speed-and-scale-of-google.html">http://google-engtools.blogspot.com/2011/06/testing-at-
speed-and-scale-of-google.html</a>.
</div></li>
<li><div id="ref-Henderson2023" class="csl-entry" role="doc-biblioentry">
Henderson, Tim A. D., Bobby Dorward, Eric Nickell, Collin Johnston, and
Avi Kondareddy. 2023. <span>“Flake <span>Aware Culprit
Finding</span>.”</span> In <em>2023 <span>IEEE Conference</span> on
<span>Software Testing</span>, <span>Verification</span> and
<span>Validation</span> (<span>ICST</span>)</em>. <span>IEEE</span>. 
<a href="https://hackthology.com/flake-aware-culprit-finding.html">https://hackthology.com/flake-aware-culprit-finding.html</a>.
<a href="https://doi.org/10.1109/ICST57152.2023.00041">https://doi.org/10.1109/ICST57152.2023.00041</a>.
</div></li>
<li><div id="ref-Henderson2018" class="csl-entry" role="doc-biblioentry">
Henderson, Tim A. D., and Andy Podgurski. 2018. <span>“Behavioral
<span>Fault Localization</span> by <span>Sampling Suspicious Dynamic
Control Flow Subgraphs</span>.”</span> In <em>2018 <span>IEEE</span>
11th <span>International Conference</span> on <span>Software
Testing</span>, <span>Verification</span> and <span>Validation</span>
(<span>ICST</span>)</em>, 93–104. <span>Vasteras</span>:
<span>IEEE</span>. 
<a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html">https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html</a>.
<a href="https://doi.org/10.1109/ICST.2018.00019">https://doi.org/10.1109/ICST.2018.00019</a>.
</div></li>
<li><div id="ref-Henderson2019" class="csl-entry" role="doc-biblioentry">
Henderson, Tim A. D., Andy Podgurski, and Yigit Kucuk. 2019.
<span>“Evaluating <span>Automatic Fault Localization Using Markov
Processes</span>.”</span> In <em>2019 19th <span>International Working
Conference</span> on <span>Source Code Analysis</span> and
<span>Manipulation</span> (<span>SCAM</span>)</em>, 115–26.
<span>Cleveland, OH, USA</span>: <span>IEEE</span>.
<a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html">https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html</a>
<a href="https://doi.org/10.1109/SCAM.2019.00021">https://doi.org/10.1109/SCAM.2019.00021</a>.
</div></li>
<li><div id="ref-Herzig2015a" class="csl-entry" role="doc-biblioentry">
Herzig, Kim, and Nachiappan Nagappan. 2015. <span>“Empirically
<span>Detecting False Test Alarms Using Association
Rules</span>.”</span> In <em>2015 <span>IEEE</span>/<span>ACM</span>
37th <span>IEEE International Conference</span> on <span>Software
Engineering</span></em>, 39–48. <span>Florence, Italy</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE.2015.133">https://doi.org/10.1109/ICSE.2015.133</a>.
</div></li>
<li><div id="ref-Horwitz1992" class="csl-entry" role="doc-biblioentry">
Horwitz, S, and T Reps. 1992. <span>“The Use of Program Dependence
Graphs in Software Engineering.”</span> In <em>International
<span>Conference</span> on <span>Software Engineering</span></em>,
9:349. <span>Springer</span>. <a
href="http://portal.acm.org/citation.cfm?id=24041&amp;amp;dl=">http://portal.acm.org/citation.cfm?id=24041&amp;amp;dl=</a>.
</div></li>
<li><div id="ref-Jones2002" class="csl-entry" role="doc-biblioentry">
Jones, J.a., M. J. Harrold, and J. Stasko. 2002. <span>“Visualization of
Test Information to Assist Fault Localization.”</span> <em>Proceedings
of the 24th International Conference on Software Engineering. ICSE
2002</em>. <a
href="https://doi.org/10.1145/581339.581397">https://doi.org/10.1145/581339.581397</a>.
</div></li>
<li><div id="ref-Jones2005" class="csl-entry" role="doc-biblioentry">
Jones, James A., and Mary Jean Harrold. 2005. <span>“Empirical
<span>Evaluation</span> of the <span class="nocase">Tarantula Automatic
Fault-localization Technique</span>.”</span> In <em>Proceedings of the
20th <span>IEEE</span>/<span>ACM International Conference</span> on
<span>Automated Software Engineering</span></em>, 273–82. <span>New
York, NY, USA</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/1101908.1101949">https://doi.org/10.1145/1101908.1101949</a>.
</div></li>
<li><div id="ref-keenan2019" class="csl-entry" role="doc-biblioentry">
Keenan, James. 2019. <span>“James <span>E</span>. <span>Keenan</span> -
"<span>Multisection</span>: <span>When Bisection Isn</span>’t
<span>Enough</span> to <span>Debug</span> a
<span>Problem</span>".”</span> <a
href="https://www.youtube.com/watch?v=05CwdTRt6AM">https://www.youtube.com/watch?v=05CwdTRt6AM</a>.
</div></li>
<li><div id="ref-Kucuk2021" class="csl-entry" role="doc-biblioentry">
Kucuk, Yigit, Tim A. D. Henderson, and Andy Podgurski. 2021.
<span>“Improving <span>Fault Localization</span> by <span>Integrating
Value</span> and <span>Predicate Based Causal Inference
Techniques</span>.”</span> In <em>2021
<span>IEEE</span>/<span>ACM</span> 43rd <span>International
Conference</span> on <span>Software Engineering</span>
(<span>ICSE</span>)</em>, 649–60. <span>Madrid, ES</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE43902.2021.00066">https://doi.org/10.1109/ICSE43902.2021.00066</a>.
</div></li>
<li><div id="ref-Leon2003" class="csl-entry" role="doc-biblioentry">
Leon, D., and A. Podgurski. 2003. <span>“A Comparison of Coverage-Based
and Distribution-Based Techniques for Filtering and Prioritizing Test
Cases.”</span> In <em>14th <span>International Symposium</span> on
<span>Software Reliability Engineering</span>, 2003. <span>ISSRE</span>
2003.</em>, 2003-Janua:442–53. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ISSRE.2003.1251065">https://doi.org/10.1109/ISSRE.2003.1251065</a>.
</div></li>
<li><div id="ref-Leong2019" class="csl-entry" role="doc-biblioentry">
Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John
Micco. 2019. <span>“Assessing <span>Transition-Based Test Selection
Algorithms</span> at <span>Google</span>.”</span> In <em>2019
<span>IEEE</span>/<span>ACM</span> 41st <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span>
(<span>ICSE-SEIP</span>)</em>, 101–10. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00019">https://doi.org/10.1109/ICSE-SEIP.2019.00019</a>.
</div></li>
<li><div id="ref-Lewis2013" class="csl-entry" role="doc-biblioentry">
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, and Xiaoyan Zhu. 2013.
<span>“Does Bug Prediction Support Human Developers? Findings from a
Google Case Study.”</span> In <em>Proceedings of the …</em>, 372–81. <a
href="http://dl.acm.org/citation.cfm?id=2486838">http://dl.acm.org/citation.cfm?id=2486838</a>.
</div></li>
<li><div id="ref-Lucia2014" class="csl-entry" role="doc-biblioentry">
Lucia, David Lo, Lingxiao Jiang, Ferdian Thung, and Aditya Budi. 2014.
<span>“Extended Comprehensive Study of Association Measures for Fault
Localization.”</span> <em>Journal of Software: Evolution and
Process</em> 26 (2): 172–219. <a
href="https://doi.org/10.1002/smr.1616">https://doi.org/10.1002/smr.1616</a>.
</div></li>
<li><div id="ref-Machalica2019" class="csl-entry" role="doc-biblioentry">
Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra.
2019. <span>“Predictive <span>Test Selection</span>.”</span> In <em>2019
<span>IEEE</span>/<span>ACM</span> 41st <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span>
(<span>ICSE-SEIP</span>)</em>, 91–100. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00018">https://doi.org/10.1109/ICSE-SEIP.2019.00018</a>.
</div></li>
<li><div id="ref-McVoy1996" class="csl-entry" role="doc-biblioentry">
McVoy, Larry. 1996. <span>“<span>BUG-HUNTING</span>.”</span>
<span>linux/1.3.73</span>. <a
href="https://elixir.bootlin.com/linux/1.3.73/source/
                  Documentation/BUG-HUNTING">https://elixir.bootlin.com/linux/1.3.73/source/
Documentation/BUG-HUNTING</a>.
</div></li>
<li><div id="ref-Memon2017" class="csl-entry" role="doc-biblioentry">
Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob
Siemborski, and John Micco. 2017. <span>“Taming <span
class="nocase">Google-scale</span> Continuous Testing.”</span> In
<em>2017 <span>IEEE</span>/<span>ACM</span> 39th <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice Track</span>
(<span>ICSE-SEIP</span>)</em>, 233–42. <span>Piscataway, NJ, USA</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2017.16">https://doi.org/10.1109/ICSE-SEIP.2017.16</a>.
</div></li>
<li><div id="ref-Micco2012" class="csl-entry" role="doc-biblioentry">
Micco, John. 2012. <span>“Tools for <span>Continuous Integration</span>
at <span>Google Scale</span>.”</span> Tech {{Talk}}. <span>Google
NYC</span>. <a
href="https://youtu.be/KH2_sB1A6lA">https://youtu.be/KH2_sB1A6lA</a>.
</div></li>
<li><div id="ref-Micco2013" class="csl-entry" role="doc-biblioentry">
———. 2013. <span>“Continuous <span>Integration</span> at <span>Google
Scale</span>.”</span> Lecture. <span>EclipseCon 2013</span>. <a
href="https://web.archive.org/web/20140705215747/https://
                  www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
                  03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf">https://web.archive.org/web/20140705215747/https://
www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf</a>.
</div></li>
<li><div id="ref-Mondal2015" class="csl-entry" role="doc-biblioentry">
Mondal, Debajyoti, Hadi Hemmati, and Stephane Durocher. 2015.
<span>“Exploring Test Suite Diversification and Code Coverage in
Multi-Objective Test Case Selection.”</span> <em>2015 IEEE 8th
International Conference on Software Testing, Verification and
Validation, ICST 2015 - Proceedings</em>, 1–10. <a
href="https://doi.org/10.1109/ICST.2015.7102588">https://doi.org/10.1109/ICST.2015.7102588</a>.
</div></li>
<li><div id="ref-Musa2015" class="csl-entry" role="doc-biblioentry">
Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi
Baharom. 2015. <span>“Regression <span>Test Cases</span> Selection for
<span>Object-Oriented Programs</span> Based on <span>Affected
Statements</span>.”</span> <em>International Journal of Software
Engineering and Its Applications</em> 9 (10): 91–108. <a
href="https://doi.org/10.14257/ijseia.2015.9.10.10">https://doi.org/10.14257/ijseia.2015.9.10.10</a>.
</div></li>
<li><div id="ref-Najafi2019a" class="csl-entry" role="doc-biblioentry">
Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. <span>“Bisecting
Commits and Modeling Commit Risk During Testing.”</span> In
<em>Proceedings of the 2019 27th <span>ACM Joint Meeting</span> on
<span>European Software Engineering Conference</span> and
<span>Symposium</span> on the <span>Foundations</span> of <span>Software
Engineering</span></em>, 279–89. <span>New York, NY, USA</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3338906.3338944">https://doi.org/10.1145/3338906.3338944</a>.
</div></li>
<li><div id="ref-Najafi2019" class="csl-entry" role="doc-biblioentry">
Najafi, Armin, Weiyi Shang, and Peter C. Rigby. 2019. <span>“Improving
<span>Test Effectiveness Using Test Executions History</span>: <span>An
Industrial Experience Report</span>.”</span> In <em>2019
<span>IEEE</span>/<span>ACM</span> 41st <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span>
(<span>ICSE-SEIP</span>)</em>, 213–22. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00031">https://doi.org/10.1109/ICSE-SEIP.2019.00031</a>.
</div></li>
<li><div id="ref-Ness1997" class="csl-entry" role="doc-biblioentry">
Ness, B., and V. Ngo. 1997. <span>“Regression Containment Through Source
Change Isolation.”</span> In <em>Proceedings <span>Twenty-First Annual
International Computer Software</span> and <span>Applications
Conference</span> (<span>COMPSAC</span>’97)</em>, 616–21.
<span>Washington, DC, USA</span>: <span>IEEE Comput. Soc</span>. <a
href="https://doi.org/10.1109/CMPSAC.1997.625082">https://doi.org/10.1109/CMPSAC.1997.625082</a>.
</div></li>
<li><div id="ref-Ocariza2022" class="csl-entry" role="doc-biblioentry">
Ocariza, Frolin S. 2022. <span>“On the <span>Effectiveness</span> of
<span>Bisection</span> in <span>Performance Regression
Localization</span>.”</span> <em>Empirical Software Engineering</em> 27
(4): 95. <a
href="https://doi.org/10.1007/s10664-022-10152-3">https://doi.org/10.1007/s10664-022-10152-3</a>.
</div></li>
<li><div id="ref-Osman2017" class="csl-entry" role="doc-biblioentry">
Osman, Haidar, Mohammad Ghafari, Oscar Nierstrasz, and Mircea Lungu.
2017. <span>“An <span>Extensive Analysis</span> of <span>Efficient Bug
Prediction Configurations</span>.”</span> In <em>Proceedings of the 13th
<span>International Conference</span> on <span>Predictive Models</span>
and <span>Data Analytics</span> in <span>Software
Engineering</span></em>, 107–16. <span>Toronto Canada</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3127005.3127017">https://doi.org/10.1145/3127005.3127017</a>.
</div></li>
<li><div id="ref-pan2022" class="csl-entry" role="doc-biblioentry">
Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand.
2022. <span>“Test Case Selection and Prioritization Using Machine
Learning: A Systematic Literature Review.”</span> <em>Empirical Software
Engineering</em> 27 (2): 29. <a
href="https://doi.org/10.1007/s10664-021-10066-6">https://doi.org/10.1007/s10664-021-10066-6</a>.
</div></li>
<li><div id="ref-parry2022" class="csl-entry" role="doc-biblioentry">
Parry, Owain, Gregory M. Kapfhammer, Michael Hilton, and Phil McMinn.
2022. <span>“A <span>Survey</span> of <span>Flaky Tests</span>.”</span>
<em>ACM Transactions on Software Engineering and Methodology</em> 31
(1): 1–74. <a
href="https://doi.org/10.1145/3476105">https://doi.org/10.1145/3476105</a>.
</div></li>
<li><div id="ref-scikit-learn" class="csl-entry" role="doc-biblioentry">
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.
Grisel, M. Blondel, et al. 2011. <span>“Scikit-Learn:
<span>Machine</span> Learning in <span>Python</span>.”</span>
<em>Journal of Machine Learning Research</em> 12: 2825–30.
</div></li>
<li><div id="ref-Pelc2002" class="csl-entry" role="doc-biblioentry">
Pelc, Andrzej. 2002. <span>“Searching Games with Errorsfifty Years of
Coping with Liars.”</span> <em>Theoretical Computer Science</em> 270
(1-2): 71–109. <a
href="https://doi.org/10.1016/S0304-3975(01)00303-6">https://doi.org/10.1016/S0304-3975(01)00303-6</a>.
</div></li>
<li><div id="ref-Podgurski1990" class="csl-entry" role="doc-biblioentry">
Podgurski, A, and L A Clarke. 1990. <span>“A <span>Formal Model</span>
of <span>Program Dependences</span> and <span>Its Implications</span>
for <span>Software Testing</span>, <span>Debugging</span>, and
<span>Maintenance</span>.”</span> <em>IEEE Transactions of Software
Engineering</em> 16 (9): 965–79. <a
href="https://doi.org/10.1109/32.58784">https://doi.org/10.1109/32.58784</a>.
</div></li>
<li><div id="ref-Potvin2016" class="csl-entry" role="doc-biblioentry">
Potvin, Rachel, and Josh Levenberg. 2016. <span>“Why Google Stores
Billions of Lines of Code in a Single Repository.”</span>
<em>Communications of the ACM</em> 59 (7): 78–87. <a
href="https://doi.org/10.1145/2854146">https://doi.org/10.1145/2854146</a>.
</div></li>
<li><div id="ref-Punitha2013" class="csl-entry" role="doc-biblioentry">
Punitha, K., and S. Chitra. 2013. <span>“Software Defect Prediction
Using Software Metrics - <span>A</span> Survey.”</span> In <em>2013
<span>International Conference</span> on <span>Information
Communication</span> and <span>Embedded Systems</span>
(<span>ICICES</span>)</em>, 555–58. <span>Chennai</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICICES.2013.6508369">https://doi.org/10.1109/ICICES.2013.6508369</a>.
</div></li>
<li><div id="ref-Ren2004" class="csl-entry" role="doc-biblioentry">
Ren, Xiaoxia, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia
Chesley. 2004. <span>“Chianti: A Tool for Change Impact Analysis of Java
Programs.”</span> In <em>Proceedings of the 19th Annual <span>ACM
SIGPLAN</span> Conference on <span class="nocase">Object-oriented</span>
Programming, Systems, Languages, and Applications</em>, 432–48.
<span>Vancouver BC Canada</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/1028976.1029012">https://doi.org/10.1145/1028976.1029012</a>.
</div></li>
<li><div id="ref-Rivest1978" class="csl-entry" role="doc-biblioentry">
Rivest, R. L., A. R. Meyer, and D. J. Kleitman. 1978. <span>“Coping with
Errors in Binary Search Procedures (<span>Preliminary
Report</span>).”</span> In <em>Proceedings of the Tenth Annual
<span>ACM</span> Symposium on <span>Theory</span> of Computing -
<span>STOC</span> ’78</em>, 227–32. <span>San Diego, California, United
States</span>: <span>ACM Press</span>. <a
href="https://doi.org/10.1145/800133.804351">https://doi.org/10.1145/800133.804351</a>.
</div></li>
<li><div id="ref-Rodriguez-Perez2018" class="csl-entry"
role="doc-biblioentry">
Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona.
2018. <span>“Reproducibility and Credibility in Empirical Software
Engineering: <span>A</span> Case Study Based on a Systematic Literature
Review of the Use of the <span>SZZ</span> Algorithm.”</span>
<em>Information and Software Technology</em> 99 (July): 164–76. <a
href="https://doi.org/10.1016/j.infsof.2018.03.009">https://doi.org/10.1016/j.infsof.2018.03.009</a>.
</div></li>
<li><div id="ref-Saha2017" class="csl-entry" role="doc-biblioentry">
Saha, Ripon, and Milos Gligoric. 2017. <span>“Selective <span>Bisection
Debugging</span>.”</span> In <em>Fundamental <span>Approaches</span> to
<span>Software Engineering</span></em>, edited by Marieke Huisman and
Julia Rubin, 10202:60–77. <span>Berlin, Heidelberg</span>:
<span>Springer Berlin Heidelberg</span>. <a
href="https://doi.org/10.1007/978-3-662-54494-5_4">https://doi.org/10.1007/978-3-662-54494-5_4</a>.
</div></li>
<li><div id="ref-Singh2012" class="csl-entry" role="doc-biblioentry">
Singh, Yogesh, Arvinder Kaur, Bharti Suri, and Shweta Singhal. 2012.
<span>“Systematic Literature Review on Regression Test Prioritization
Techniques.”</span> <em>Informatica (Slovenia)</em> 36 (4): 379–408. <a
href="https://doi.org/10.31449/inf.v36i4.420">https://doi.org/10.31449/inf.v36i4.420</a>.
</div></li>
<li><div id="ref-Sliwerski2005" class="csl-entry" role="doc-biblioentry">
Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005.
<span>“When Do Changes Induce Fixes?”</span> <em>ACM SIGSOFT Software
Engineering Notes</em> 30 (4): 1. <a
href="https://doi.org/10.1145/1082983.1083147">https://doi.org/10.1145/1082983.1083147</a>.
</div></li>
<li><div id="ref-Wang2020" class="csl-entry" role="doc-biblioentry">
Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and
Daniel Rall. 2020. <span>“Scalable Build Service System with Smart
Scheduling Service.”</span> In <em>Proceedings of the 29th <span>ACM
SIGSOFT International Symposium</span> on <span>Software Testing</span>
and <span>Analysis</span></em>, 452–62. <span>Virtual Event USA</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3395363.3397371">https://doi.org/10.1145/3395363.3397371</a>.
</div></li>
<li><div id="ref-Wen2019" class="csl-entry" role="doc-biblioentry">
Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi
Cheung, and Zhendong Su. 2019. <span>“Exploring and Exploiting the
Correlations Between Bug-Inducing and Bug-Fixing Commits.”</span> In
<em>Proceedings of the 2019 27th <span>ACM Joint Meeting</span> on
<span>European Software Engineering Conference</span> and
<span>Symposium</span> on the <span>Foundations</span> of <span>Software
Engineering</span></em>, 326–37. <span>Tallinn Estonia</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3338906.3338962">https://doi.org/10.1145/3338906.3338962</a>.
</div></li>
<li><div id="ref-Wong2016" class="csl-entry" role="doc-biblioentry">
Wong, W. Eric, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016.
<span>“A <span>Survey</span> on <span>Software Fault
Localization</span>.”</span> <em>IEEE Transactions on Software
Engineering</em> 42 (8): 707–40. <a
href="https://doi.org/10.1109/TSE.2016.2521368">https://doi.org/10.1109/TSE.2016.2521368</a>.
</div></li>
<li><div id="ref-Youm2015" class="csl-entry" role="doc-biblioentry">
Youm, Klaus Changsun, June Ahn, Jeongho Kim, and Eunseok Lee. 2015.
<span>“Bug <span>Localization Based</span> on <span>Code Change
Histories</span> and <span>Bug Reports</span>.”</span> In <em>2015
<span>Asia-Pacific Software Engineering Conference</span>
(<span>APSEC</span>)</em>, 190–97. <span>New Delhi</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/APSEC.2015.23">https://doi.org/10.1109/APSEC.2015.23</a>.
</div></li>
<li><div id="ref-Yourdon1975" class="csl-entry" role="doc-biblioentry">
Yourdon, E. 1975. <em>Techniques of <span>Program Design</span></em>.
<span>New Jersey</span>: <span>Prentice-Hall</span>.
</div></li>
<li><div id="ref-Zeller1999" class="csl-entry" role="doc-biblioentry">
Zeller, Andreas. 1999. <span>“Yesterday, <span>My Program Worked</span>.
<span>Today</span>, <span>It Does Not</span>. <span>Why</span>?”</span>
<em>SIGSOFT Softw. Eng. Notes</em> 24 (6): 253–67. <a
href="https://doi.org/10.1145/318774.318946">https://doi.org/10.1145/318774.318946</a>.
</div></li>
<li><div id="ref-Zhou2012" class="csl-entry" role="doc-biblioentry">
Zhou, Jian, Hongyu Zhang, and David Lo. 2012. <span>“Where Should the
Bugs Be Fixed? <span>More</span> Accurate Information Retrieval-Based
Bug Localization Based on Bug Reports.”</span> <em>Proceedings -
International Conference on Software Engineering</em>, 14–24. <a
href="https://doi.org/10.1109/ICSE.2012.6227210">https://doi.org/10.1109/ICSE.2012.6227210</a>.
</div></li>
<li><div id="ref-Zhou2010" class="csl-entry" role="doc-biblioentry">
Zhou, Zhi Quan. 2010. <span>“Using Coverage Information to Guide Test
Case Selection in <span>Adaptive Random Testing</span>.”</span>
<em>Proceedings - International Computer Software and Applications
Conference</em>, 208–13. <a
href="https://doi.org/10.1109/COMPSACW.2010.43">https://doi.org/10.1109/COMPSACW.2010.43</a>.
</div></li>
<li><div id="ref-Ziftci2013a" class="csl-entry" role="doc-biblioentry">
Ziftci, Celal, and Vivek Ramavajjala. 2013. <span>“Finding
<span>Culprits Automatically</span> in <span>Failing Builds</span> -
i.e. <span>Who Broke</span> the <span>Build</span>?”</span> <a
href="https://www.youtube.com/watch?v=SZLuBYlq3OM">https://www.youtube.com/watch?v=SZLuBYlq3OM</a>.
</div></li>
<li><div id="ref-Ziftci2017" class="csl-entry" role="doc-biblioentry">
Ziftci, Celal, and Jim Reardon. 2017. <span>“Who Broke the Build?
<span>Automatically</span> Identifying Changes That Induce Test Failures
in Continuous Integration at Google Scale.”</span> <em>Proceedings -
2017 IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track, ICSE-SEIP 2017</em>, 113–22. <a
href="https://doi.org/10.1109/ICSE-SEIP.2017.13">https://doi.org/10.1109/ICSE-SEIP.2017.13</a>.
</div></li>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Both the <em>per-test</em> error rate
and the <em>per-change</em> error rates quoted above are drawn from the
same 28 day window. The Postsubmit result which triggers culprit finding
may itself have been a flake, so there were spurious suspect ranges in
the same period. These rates may differ slightly from the numbers
reported in <span class="citation" data-cites="Henderson2023">(<a
href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
2023</a>)</span> as they reflect our most recent data as of
2023-11-12.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>There are some legacy tests which are
allowed to use the network but there is an on-going effort to clean up
their usage.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Previously, we referred to these
cycles as “Milestones” in most of the previous literature but we are
gradually changing our internal nomenclature as we evolve TAP
Postsubmit’s testing model.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>https://scikit-learn.org/<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a
href="https://xgboost.readthedocs.io/en/stable/index.html"
class="uri">https://xgboost.readthedocs.io/en/stable/index.html</a><a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
