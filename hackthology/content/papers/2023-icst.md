Title: Flake Aware Culprit Finding
Author: <a href="http://hackthology.com">Tim Henderson</a>, Bobby Dorward, Eric Nickell, Collin Johnston, Avi Kondareddy
Citation: <strong>Tim A. D. Henderson</strong>, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy. <i>Flake Aware Culprit Finding</i>. ICST Industry Track 2023.
CiteAuthor: <strong>Tim A. D. Henderson</strong>, Bobby Dorward, Eric Nickell, Collin Johnston, Avi Kondareddy
Publication: ICST Industry Track 2023
Date: 2023-04-17
Category: Paper
MainPage: show


**Tim A. D. Henderson**, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy.
*Flake Aware Culprit Finding*.  [ICST Industry Track 2023](https://conf.researchr.org/home/icst-2023).
<br/>
[DOI](http://tba).
[PDF]({static}/pdfs/icst-2023.pdf).
[WEB]({filename}/papers/2023-icst.md).

<h4>Note</h4>
> This is a conversion from a latex paper I wrote. If you want all formatting
> correct you should read the
> [pdf version]({static}/pdfs/icst-2023.pdf).

#### Abstract

When a change introduces a bug into a large software repository, there is
often a delay between when the change is committed and when bug is detected.
This is true even when the bug causes an existing test to fail! These delays
are caused by resource constraints which prevent the organization from running
all of the tests on every change. Due to the delay, a Continuous Integration
system needs to locate buggy commits. Locating them is complicated by flaky
tests that pass and fail non-deterministically. The flaky tests introduce
noise into the CI system requiring costly reruns to determine if a failure was
caused by a bad code change or caused by non-deterministic test behavior. This
paper presents an algorithm, *Flake Aware Culprit Finding*, that locates
buggy commits more accurately than a traditional bisection search. The
algorithm is based on Bayesian inference and noisy binary search, utilizing
prior information about which changes are most likely to contain the bug. A
large scale empirical study was conducted at Google on 13,000+ test breakages.
The study evaluates the accuracy and cost of the new algorithm versus a
traditional deflaked bisection search.

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


<h1 id="sec:introduction">Introduction</h1>
<p>Fast, collaborative, large scale development is enabled by the use of
Continuous Integration in a <em>mono-repository</em> (monorepo)
environment where all of the source code is stored in a single shared
repository <span class="citation" data-cites="Potvin2016">(<a
href="#ref-Potvin2016" role="doc-biblioref">Potvin and Levenberg
2016</a>)</span>. In a monorepo, all developers share a single source of
truth for the state of the code and most builds are made from “head”
(the most recent commit to the repository) using the source code for all
libraries and dependencies rather than versioned pre-compiled archives.
This enables (among other things) unified versioning, atomic changes,
large scale refactorings, code re-use, cross team collaboration, and
flexible code ownership boundaries.</p>
<p>Very large scale monorepos (such as those at Google <span
class="citation" data-cites="Potvin2016 Memon2017">(<a
href="#ref-Potvin2016" role="doc-biblioref">Potvin and Levenberg
2016</a>; <a href="#ref-Memon2017" role="doc-biblioref">Memon et al.
2017</a>)</span>, Microsoft <span class="citation"
data-cites="Herzig2015a">(<a href="#ref-Herzig2015a"
role="doc-biblioref">Herzig and Nagappan 2015</a>)</span>, and Facebook
<span class="citation" data-cites="Machalica2019">(<a
href="#ref-Machalica2019" role="doc-biblioref">Machalica et al.
2019</a>)</span>) require advanced systems for ensuring changes
submitted to the repository are properly tested and validated.
<em>Continuous Integration</em> (CI) <span class="citation"
data-cites="Fowler2006">(<a href="#ref-Fowler2006"
role="doc-biblioref">Fowler 2006</a>)</span> is a system and practice of
automatically integrating changes into the code repository that serves
as the source of truth for the organization. In modern environments,
integration involves not only performing the textual merge required to
add the change but also verification tasks such as ensuring the software
compiles and the automated tests pass both on the new change before it
is integrated <em>and after</em> it has been incorporated into the main
development branch.</p>

<span id="fig:ci-timeline" label="fig:ci-timeline">
[![Figure 1](images/icst-2023/fig1.png)](images/icst-2023/fig1.png)
</span>

<p>Due to resource constraints, tests are only run at specific versions
(called <em>milestones</em> at Google) and many tests may be skipped
because they were not affected by the intervening changes (as determined
by static build dependencies) <span class="citation"
data-cites="Gupta2011">(<a href="#ref-Gupta2011"
role="doc-biblioref">Gupta, Ivey, and Penix 2011</a>)</span>. This
process is illustrated in Figure <a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a>. A test is
considered passing at a version <span class="math inline">\(b\)</span>
if and only if it has a passing result at the most recent preceding
affecting version. Formally, if there exists a version <span
class="math inline">\(a\)</span> where the test is passing and <span
class="math inline">\(a \le b\)</span> and there does not exist an
intervening version <span class="math inline">\(c\)</span>, <span
class="math inline">\(a &lt; c \le b\)</span>, which affects the test
then the test is passing at version <span
class="math inline">\(a\)</span>. A project’s status is considered
passing at a version <span class="math inline">\(b\)</span> if all tests
are passing (using the above definition) at version <span
class="math inline">\(b\)</span>.</p>
<p>If a test <span class="math inline">\(t\)</span> is failing
(consistently) at a milestone version <span
class="math inline">\(m\)</span>, it may not necessarily mean that
version <span class="math inline">\(m\)</span> introduced a change which
broke test <span class="math inline">\(t\)</span>. As shown in Figure <a
href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a> one or more
changes between <span class="math inline">\(m\)</span> and the previous
milestone at which test <span class="math inline">\(t\)</span> was run
may have introduced a breaking change. Figuring out which version
introduced the <em>breakage</em> is called <em>culprit finding</em>.</p>
<p>Many companies now employ automated culprit finding systems. These
systems typically implement a (possibly n-way) “bisect search” similar
to the one built into the Git version control system <span
class="citation" data-cites="Couder2008">(<a href="#ref-Couder2008"
role="doc-biblioref">Couder 2008</a>)</span>. Git’s bisect command runs
a binary search on the versions between the first known breaking version
and the last known passing version to identify the version which broke
the test.</p>
<p>In the Google environment, there are two problems with a traditional
bisection algorithm <span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span>: (1)
it does not account for flaky (non-deterministic) tests, and (2) when
using <span class="math inline">\(k\)</span>-reruns to deflake the
search accuracy plateaus while build cost continues to increase
linearly. <strong>This paper proposes</strong> a new method for culprit
finding which is both robust to non-deterministic (flaky) test behavior
and can identify the culprit with only logarithmic builds in the best
case by using prior information to accelerate the search.</p>
<h2 id="contributions">Contributions</h2>
<ol>
<li><p>A Flake Aware Culprit Finding (FACF) algorithm.</p></li>
<li><p>A mathematical model for culprit finding adjusting for
non-deterministic test behavior.</p></li>
<li><p>A large scale empirical study of FACF on Google’s mono repository
comparing its performance against the traditional bisect
algorithm.</p></li>
<li><p>The study also evaluates the effectiveness: (a) optimizations for
FACF, and (b) adding deflaking runs to Bisect.</p></li>
</ol>
<h1 id="background">Background</h1>
<p>At Google, the Test Automation Platform (TAP) runs most tests that
can run on a single machine (and do not use the network except for the
loopback interface)<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> and can run in under 15 minutes.<a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> Figure <a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline">[fig:ci-timeline]</a> illustrates at a
high level of how TAP works. First, (not illustrated) tests are grouped
into logical projects by the teams that own them. Then, (for accounting
and quota allocation purposes) those projects are grouped into very high
level groupings called Quota Buckets. Each Quota Bucket is scheduled
independently when build resources are available. This means that TAP
does not run every test at every version. Instead it only runs tests
periodically at select versions called “Milestones” <span
class="citation" data-cites="Memon2017 Micco2013 Leong2019">(<a
href="#ref-Memon2017" role="doc-biblioref">Memon et al. 2017</a>; <a
href="#ref-Micco2013" role="doc-biblioref">Micco 2013</a>; <a
href="#ref-Leong2019" role="doc-biblioref">Leong et al.
2019</a>)</span>.</p>
<p>When scheduling a Milestone, all the tests that are built and run
with the same command line flags to the build system<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> are
grouped into a set. That set is filtered to remove tests that haven’t
been affected by a change to themselves or a build level dependency
<span class="citation" data-cites="Gupta2011 Micco2013 Micco2012">(<a
href="#ref-Gupta2011" role="doc-biblioref">Gupta, Ivey, and Penix
2011</a>; <a href="#ref-Micco2013" role="doc-biblioref">Micco 2013</a>,
<a href="#ref-Micco2012" role="doc-biblioref">2012</a>)</span>. This is
a very coarse grained form of static dependence-based test selection
<span class="citation" data-cites="Binkley1992 Horwitz1992 Tip1995">(<a
href="#ref-Binkley1992" role="doc-biblioref">Binkley 1992</a>; <a
href="#ref-Horwitz1992" role="doc-biblioref">Horwitz and Reps 1992</a>;
<a href="#ref-Tip1995" role="doc-biblioref">Tip 1995</a>)</span>.
Google’s use of build dependence test selection has been influential and
it is now also used at a number of other corporations who have adopted
Bazel or similar tools <span class="citation"
data-cites="Ananthanarayanan2019 Machalica2019">(<a
href="#ref-Ananthanarayanan2019" role="doc-biblioref">Ananthanarayanan
et al. 2019</a>; <a href="#ref-Machalica2019"
role="doc-biblioref">Machalica et al. 2019</a>)</span>. Finally, that
(potentially very large) set of “affected” tests is sent to the Build
System <span class="citation" data-cites="Wang2020">(<a
href="#ref-Wang2020" role="doc-biblioref">Wang et al. 2020</a>)</span>
which batches them into efficiently sized groups and then builds and
runs them when there is capacity.</p>
<p>Since tests are only run periodically even with accurate build level
dependence analysis to drive test selection, there are usually multiple
changes which may have introduced a fault in the software or a bug in
the test. Finding these “bug inducing changes” – which we call
<strong>culprits</strong> – is the subject of this paper. The process of
finding these changes is aggravated by any non-deterministic behavior of
the tests and the build system running them.</p>
<h2 id="flaky-tests">Flaky Tests</h2>
<p>When a test behaves non-deterministically, we refer to it, using the
standard term, as a <em>flaky test</em>. A flaky test is one that passes
or fails non-deterministically with no changes to the code of either the
test or the system under test. At Google, we consider all tests to be at
least <em>slightly</em> flaky as the machines that tests are running on
could fail in ways that get classified as a test failure (such as
resource exhaustion due to a noisy neighbor resulting in a test
exceeding its time limit). We encourage everyone to adopt this
conservative view for the purposes of identifying culprits.</p>
<p>An important input to the culprit finding algorithm we are presenting
is an estimate of how flaky a test might be, as represented by an
estimated probability of the test failing due to a flaky failure. We
will denote the estimated probability of failure due to a flake for a
test <span class="math inline">\(t\)</span> as <span
class="math inline">\(\hat{f}_t\)</span>. The actual probability of
failure due to a flake will be denoted <span
class="math inline">\(f_t\)</span> (it is impossible in practice to know
<span class="math inline">\(f_t\)</span>). At Google, we estimate <span
class="math inline">\(\hat{f}_t\)</span> based on the historical failure
rate of the test. Apple and Facebook recently published on their work in
on estimating flake rates <span class="citation"
data-cites="Kowalczyk2020 Machalica2020">(<a href="#ref-Kowalczyk2020"
role="doc-biblioref">Kowalczyk et al. 2020</a>; <a
href="#ref-Machalica2020" role="doc-biblioref">Machalica et al.
2020</a>)</span>. One challenge with flake rate estimation is that a
single change to the code can completely change the flakiness
characteristics of a test in unpredictable ways. Making the flake rate
estimation sensitive to recent changes is an ongoing challenge and one
we partially solve in this work by doing live re-estimation of the flake
rate during the culprit finding process itself.</p>
<h2 id="sec:background:cf">Culprit Finding</h2>
<p>Informally, Culprit Finding is the problem of identifying a version
<span class="math inline">\(c\)</span> between two versions <span
class="math inline">\(a\)</span> and <span
class="math inline">\(b\)</span> such that <span class="math inline">\(a
&lt; c \le b\)</span> where a given test <span
class="math inline">\(t\)</span> was passing at <span
class="math inline">\(a\)</span>, failing at <span
class="math inline">\(b\)</span>, and <span
class="math inline">\(c\)</span> is the version which introduced the
fault. At Google we consider the passing status of a test to be 100%
reliable and therefore version <span class="math inline">\(a\)</span>
cannot be a culprit but version <span class="math inline">\(b\)</span>
could be the culprit. For simplicity, this paper only considers
searching for the culprit for a single test <span
class="math inline">\(t\)</span> (which failed at <span
class="math inline">\(b\)</span>) but in principle it can be extended to
a set of tests <span class="math inline">\(T\)</span>.</p>
<p>Google’s version control system, Piper <span class="citation"
data-cites="Potvin2016">(<a href="#ref-Potvin2016"
role="doc-biblioref">Potvin and Levenberg 2016</a>)</span>, stores the
mainline history as a linear list of versions. Each version gets a
monotonically, <em>but not sequentially</em>, increasing number as its
version number called the <em>Changelist Number</em> or CL number for
short. Thus, our implementation and experiments do not consider branches
and merges as would be necessary for a Distributed Version Control
System (DVCS) such as git. However, only trivial changes to the
mathematics are needed to apply our approach to such an environment
<span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span>.</p>
<p>Thus, the versions larger than <span class="math inline">\(a\)</span>
and smaller or equal to <span class="math inline">\(b\)</span> are an
ordered list of items we will call the <em>search range</em> and denote
as <span class="math inline">\(S =
\left\{s_1, ..., s_{n-1}, s_n\right\}\)</span> where <span
class="math inline">\(s_n = b\)</span>. During the search, test
executions will be run (and may be run in parallel) at various versions.
The unordered set of executions at version <span
class="math inline">\(s_i\)</span> will be denoted <span
class="math inline">\(X_i\)</span>.</p>
<p>For the purposes of culprit finding, based on the status of failure
detected, the build/test execution statuses are mapped to one of 2
possible states: <code>PASS</code> or <code>FAIL</code>. We denote (for
version <span class="math inline">\(s_i\)</span>): the number of
statuses that map to <code>PASS</code> as <span
class="math inline">\(\texttt{\small PASSES}(X_i)\)</span> and the
number of statuses that map to <code>FAIL</code> as <span
class="math inline">\(\texttt{\small FAILS}(X_i)\)</span>.</p>

<h2 id="bisection">Bisection</h2>

<span id="alg:bisect" label="alg:bisect">
[![Algorithm Bisect](images/icst-2023/alg1.png)](images/icst-2023/alg1.png)
</span>

<p>The Bisect algorithm is a specialized form of binary search. It runs
tests (or more generally any operation on the code that can return PASS
or FAIL) to attempt to identify which commit introduced a bug. Algorithm
<a href="#alg:bisect" data-reference-type="ref"
data-reference="alg:bisect">[alg:bisect]</a> gives the psuedo-code for
this procedure. Bisect is now commonly implemented in most version
control systems (see <code>git bisect</code> and <code>hg bisect</code>
as common examples). In the case of Distributed Version Control Systems
(DVCS) systems such as ‘git‘ and ‘hg‘ the implementation of bisect is
somewhat more complex as they model version history as a directed
acyclic graph <span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span>.</p>
<p>The algorithm presented in Alg <a href="#alg:bisect"
data-reference-type="ref" data-reference="alg:bisect">[alg:bisect]</a>
has two important changes from the standard binary search. First, it
takes into account that a test can fail and pass at the same version and
the <code>firstFail</code> function specifically looks for a failure
after the <code>lastPass</code>. Second, it takes a parameter <span
class="math inline">\(k\)</span> which allows the user to specify that
each test execution be run <span class="math inline">\(k\)</span> times.
This allows the user to “deflake” the test executions by rerunning
failures. Our empirical evaluation (Section <a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation">5</a>)
examines both the cost and effectiveness of this simple modification.
While this simple modification to bisect may be reasonably effective for
moderately flaky tests, it does not explicitly model the effect of
non-deterministic failures on the likelihood of a suspect to be the
culprit.</p>
<h2 id="guessing-games-and-noisy-searches">Guessing Games and Noisy
Searches</h2>
<div class="algorithm">

</div>
<p>One can view the culprit finding scenario as a kind of guessing game.
The questioner guesses the test always fails at a given suspect version.
A test execution at that suspect acts as a kind of “responder” or
“oracle” which answers. The test execution is an unreliable oracle which
may answer FAIL when it should have answered PASS. But, the oracle never
answers PASS when it should have answered FAIL – note the asymmetry.</p>
<p>This kind of guessing game is a Rényi-Ulam game, which has been well
characterized by the community <span class="citation"
data-cites="Pelc2002">(<a href="#ref-Pelc2002" role="doc-biblioref">Pelc
2002</a>)</span>. The main differences from the “classical” games
presented by Rényi and separately by Ulam is (a) the format of the
questions and (b) the limitations placed on the oracle responding
incorrectly. In a common formulation of the Rényi-Ulam game, the
questioner is trying to “guess the number” between (for instance) 1 and
1,000,000. Each question is allowed to be any subset of numbers in the
search range. The responder or oracle is allowed to lie at most a fixed
number of times (for instance once or twice). This formulation has
allowed the community to find analytic solutions to determine the number
of questions in the optimal strategy. We refer the reader to the survey
from Pelc for an overview of the field and its connection with error
correcting codes <span class="citation" data-cites="Pelc2002">(<a
href="#ref-Pelc2002" role="doc-biblioref">Pelc 2002</a>)</span>. The
culprit finding scenario is “noisy binary search” scenario where the
oracle only lies for one type of response. Rivest <em>et al.</em> <span
class="citation" data-cites="Rivest1978">(<a href="#ref-Rivest1978"
role="doc-biblioref">Rivest, Meyer, and Kleitman 1978</a>)</span>
provide a theoretical treatment of this “half-lie” variant (as well as
the usual variation).</p>
<h2 id="bayesian-binary-search">Bayesian Binary Search</h2>
<p>Ben Or and Hassidim <span class="citation"
data-cites="Ben-Or2008">(<a href="#ref-Ben-Or2008"
role="doc-biblioref">Ben-Or and Hassidim 2008</a>)</span> noted the
connection between these “Noisy Binary Search” problems and Bayesian
inference. Our work builds off of their formulation and applies it to
the culprit finding setting. In the Bayesian setting, consider the
probability each suspect is the culprit and the probability there is no
culprit (i.e. the original failure was caused by a flake). These
probabilities together form a distribution: <span
class="math display">\[(\textrm{Pr}\left[{\text{there is no
culprit}}\right]) +
  \sum_{i=1}^{n} \textrm{Pr}\left[{s_i \text{ is the culprit}}\right] =
1\]</span></p>
<p>We represent this distribution with <span
class="math inline">\(\textrm{Pr}\left[{C_i}\right]\)</span> with <span
class="math inline">\(i=1..(n+1)\)</span> where <span
class="math display">\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i}\right] &amp;= \textrm{Pr}\left[{s_i \text{ is
the culprit}}\right], \; \forall \; i \in [1, n]
   \\
  \textrm{Pr}\left[{C_{n+1}}\right] &amp;= \textrm{Pr}\left[{\text{there
is no culprit}}\right] \\
\end{split}\end{aligned}\]</span></p>
<p>Initially when the original failure is first detected at <span
class="math inline">\(s_n\)</span> and the search range is created, the
probability distribution is uniform: <span
class="math inline">\(\textrm{Pr}\left[{C_i}\right] =
\frac{1}{(n+1)}\)</span> for all <span class="math inline">\(i \in [1,
n+1]\)</span>. Now consider new evidence <span
class="math inline">\(E\)</span> arriving. In the Bayesian model, this
would <em>update</em> our probability given this new evidence: <span
class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{C_i | E}\right] = \frac{\textrm{Pr}\left[{E |
C_i}\right] \cdot
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{E}\right]}
  \label{eq:bayes}\end{aligned}\]</span></p>
<p>In the above equation, the <span
class="math inline">\(\textrm{Pr}\left[{C_i}\right]\)</span> is the
<em>prior</em> probability and the probability <span
class="math inline">\(\textrm{Pr}\left[{C_i | E}\right]\)</span> is the
<em>posterior</em> probability. In the culprit finding context, the new
evidence is typically newly observed test executions. However, there is
no reason we cannot take into account other information as well.</p>
<p>In the Bayesian framework, new evidence is applied iteratively. In
each iteration, the prior probability is the posterior probability of
the previous iteration. Since multiplication is commutative, multiple
pieces of evidence can be applied in any order to arrive at the same
posterior probability: <span class="math display">\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i | E_1, E_2}\right] &amp;=
\frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]}
                         \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]} \textrm{Pr}\left[{Ci}\right]
\\
                      &amp;= \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]}
                         \frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]} \textrm{Pr}\left[{Ci}\right]
\end{split}\end{aligned}\]</span></p>
<p>After the current posterior probability has been calculated, it is
used to select the next suspect to execute a test at. In the traditional
Bayes approach, the suspect to examine is the one with the maximum
posterior probability. However, in the context of a noisy binary search,
the suspects are ordered so a PASS at suspect <span
class="math inline">\(s_i\)</span> indicates a PASS at all suspects
prior to <span class="math inline">\(s_i\)</span>. Following Ben Or and
Hassidim <span class="citation" data-cites="Ben-Or2008">(<a
href="#ref-Ben-Or2008" role="doc-biblioref">Ben-Or and Hassidim
2008</a>)</span>, we convert the posterior distribution to a cumulative
probability distribution and select the first suspect with a cumulative
probability <span class="math inline">\(\geq 0.5\)</span>.</p>
<h1 id="flake-aware-culprit-finding">Flake Aware Culprit Finding</h1>
<p>Now we introduce our algorithm, Flake Aware Culprit Finding (FACF),
in the Bayesian framework of Ben Or and Hassidim <span class="citation"
data-cites="Ben-Or2008">(<a href="#ref-Ben-Or2008"
role="doc-biblioref">Ben-Or and Hassidim 2008</a>)</span>. As above, let
<span class="math inline">\(\textrm{Pr}\left[{C_i}\right]\)</span> be
the probability that suspect <span class="math inline">\(s_i\)</span> is
the culprit and <span
class="math inline">\(\textrm{Pr}\left[{C_{n+1}}\right]\)</span> be the
probability there is no culprit. These probabilities are initialized to
some suitable initial distribution such as the uniform distribution.</p>
<p>To update the distribution with new evidence <span
class="math inline">\(E\)</span> in the form of a pass or fail for some
suspect <span class="math inline">\(k\)</span>, apply Bayes rule
(Equation <a href="#eq:bayes" data-reference-type="ref"
data-reference="eq:bayes">[eq:bayes]</a>). Note that <span
class="math inline">\(\textrm{Pr}\left[{E}\right]\)</span> can be
rewritten by Bayes rule in terms of a sum by marginalizing over the
likelihood that each suspect <span class="math inline">\(j\)</span>
could be the culprit: <span class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{E}\right] = \sum_{j=1}^{n} \textrm{Pr}\left[{E |
C_j}\right] \textrm{Pr}\left[{C_j}\right]\end{aligned}\]</span></p>
<p>There are two types of evidence: test executions with state
<code>PASS</code> or <code>FAIL</code> at suspect <span
class="math inline">\(i\)</span>. We indicate these as the events <span
class="math inline">\(P_i\)</span> and <span
class="math inline">\(F_i\)</span> respectively. The marginalized
probability of a <code>PASS</code> at <span
class="math inline">\(i\)</span> given there is a culprit at <span
class="math inline">\(j\)</span> is: <span
class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{P_i | C_j}\right] =
    \begin{cases}
      1 - \hat{f}_t  &amp; \text{if } i &lt; j \\
      0              &amp; \text{otherwise}
    \end{cases}
  \label{eq:pass}\end{aligned}\]</span></p>
<p>The marginalized probability of a <code>FAIL</code> at <span
class="math inline">\(i\)</span> given there is a culprit at <span
class="math inline">\(j\)</span> is: <span
class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{F_i | C_j}\right] =
    \begin{cases}
      \hat{f}_t      &amp; \text{if } i &lt; j \\
      1              &amp; \text{otherwise}
    \end{cases}
  \label{eq:fail}\end{aligned}\]</span></p>
<p>With Equation <a href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass">[eq:pass]</a>, we write a Bayesian update for
<code>PASS</code> at suspect <span class="math inline">\(k\)</span> as
the following. Given <span class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] = \frac{\textrm{Pr}\left[{P_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
\textrm{Pr}\left[{P_k}\right]
  }\end{aligned}\]</span></p>
<p>let <span class="math display">\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{P_k}\right] &amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{P_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;= \sum_{j=k+1}^{n} (1 - \hat{f}_t)
\textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]</span></p>
<div class="algorithm">

</div>
<div class="algorithm">
<p>cdf = cumulativeDistribution(dist) thresholds = <span
class="math inline">\(\left\{ \frac{i}{(k+1)} \; | \; i=1..k
\right\}\)</span> tidx = 0 runs = list() runs</p>
</div>
<p>then <span class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] =
  \begin{cases}
    \frac{(1 - \hat{f}_t)
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{P_k}\right]} &amp;
\text{if } k &lt; i \\
    0                                         &amp; \text{otherwise} \\
  \end{cases}\end{aligned}\]</span></p>
<p>The update using Equation <a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail">[eq:fail]</a> for
<code>FAIL</code> at suspect <span class="math inline">\(k\)</span> is
similar. Given <span class="math display">\[\begin{aligned}
  \textrm{Pr}\left[{C_i | F_k}\right] = \frac{\textrm{Pr}\left[{F_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
    \sum_{j=1}^{n} \textrm{Pr}\left[{F_k | C_j}\right]
\textrm{Pr}\left[{C_j}\right]
  }\end{aligned}\]</span></p>
<p>let <span class="math display">\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{F_k}\right] &amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{F_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;= \sum_{j=1}^{k} \textrm{Pr}\left[{C_j}\right] +
\sum_{j=k+1}^{n} \hat{f}_t \textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]</span></p>
<p>then <span class="math display">\[\begin{aligned}
\textrm{Pr}\left[{C_i | F_k}\right] =
  \begin{cases}
    \frac{\hat{f}_t
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]} &amp;
\text{if } k &lt; i \\
    \frac{\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]}           &amp;
\text{otherwise} \\
  \end{cases}\end{aligned}\]</span></p>

<h2 id="the-facf-algorithm">The FACF Algorithm</h2>

<span id="alg:facf" label="alg:facf">
[![Algorithm FACF](images/icst-2023/alg2.png)](images/icst-2023/alg2.png)
</span>

<span id="alg:nextRuns" label="alg:nextRuns">
[![Algorithm nextRuns](images/icst-2023/alg3.png)](images/icst-2023/alg3.png)
</span>

<p>Algorithm <a href="#alg:facf" data-reference-type="ref"
data-reference="alg:facf">[alg:facf]</a> presents the pseudo code for a
culprit finding algorithm based on the above Bayesian formulation. It
takes a prior distribution (the uniform distribution can be used), an
estimated flake rate <span class="math inline">\(\hat{f}_t\)</span>, and
the suspects. It returns the most likely culprit or <code>NONE</code>
(in the case that it is most likely there is no culprit). The FACF
algorithm builds a set of evidence <span
class="math inline">\(X\)</span> containing the observed passes and
failures so far. At each step, that evidence is converted to a
distribution using the math presented above. Then, FACF calls Algorithm
<a href="#alg:nextRuns" data-reference-type="ref"
data-reference="alg:nextRuns">[alg:nextRuns]</a> <code>NextRuns</code>
to compute the next suspect to run.</p>
<p><code>NextRuns</code> converts the probability distribution into a
cumulative distribution. It then selects the suspects with cumulative
probability over a set of thresholds and returns them. The version of
<code>NextRuns</code> we present here has been generalized to allow for
<span class="math inline">\(k\)</span> parallel runs and also contains a
minor optimization on lines 10-11. If the suspect <span
class="math inline">\(s_{i-1}\)</span> prior to the selected suspect
<span class="math inline">\(s_i\)</span> hasn’t been selected to run and
its cumulative probability is above <span
class="math inline">\(0\)</span> then the prior suspect <span
class="math inline">\(s_{i-1}\)</span> is selected in favor of <span
class="math inline">\(s_i\)</span>. This optimization ensures that we
look for the passing run preceding the most likely culprit before
looking for a failing run at the culprit.</p>
<h2 id="sec:thresholds">Choosing the Thresholds in NextRuns</h2>
<p>Another optimization can be made to NextRuns by changing the
thresholds used to select suspects. Instead of simply dividing the
probability space up evenly it uses the observation that the information
gain from a PASS is more than the gain from a FAIL. Modeling that as
<span class="math inline">\(I(P_i) = 1\)</span> and <span
class="math inline">\(I(F_i) = 1 - \hat{f}_t\)</span>, we can compute
the expected information gain for new evidence at suspect <span
class="math inline">\(s_i\)</span> as <span
class="math inline">\(\textrm{E}\left[{I(s_i)}\right] = I(P_i)P_i +
I(F_i)F_i\)</span>. Expanding from the marginalized equations <a
href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass">[eq:pass]</a> and <a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail">[eq:fail]</a> gives
<span class="math display">\[\begin{aligned}
  \textrm{E}\left[{I(s_i)}\right] = (1 - \hat{f}_t)(n + i \hat{f}_t -
1)\end{aligned}\]</span></p>
<p>Computing the cumulative expectation over <span
class="math inline">\(\{ \textrm{E}\left[{I(s_1)}\right], ...,
\textrm{E}\left[{I(s_{n+1})}\right] \}\)</span> and normalizing allows
us to select the suspect that maximizes the information gain. The
empirical evaluation (Section <a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation">5</a>)
examines the effect of this optimization. The importance of the supplied
flake rate estimate (<span class="math inline">\(\hat{f}_t\)</span>) is
also examined in Section <a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation">5</a>.</p>
<h2 id="sec:priors">Prior Probabilities for FACF</h2>
<p>The prior distribution supplied to Algorithm <a href="#alg:facf"
data-reference-type="ref" data-reference="alg:facf">[alg:facf]</a> does
not have to be a uniform distribution. Instead, it can be informed by
any other data the culprit finding system might find useful. At Google,
we are exploring many such sources of data, but so far are only using
two primary sources in production.</p>
<p>First, we have always used the static build dependence graph to
filter out non-affecting suspects before culprit finding <span
class="citation" data-cites="Gupta2011">(<a href="#ref-Gupta2011"
role="doc-biblioref">Gupta, Ivey, and Penix 2011</a>)</span>. In FACF,
we take that a step further and, using a stale snapshot for scalability,
we compute the minimum distance in the build graph between the files
changed in each suspect and the test being culprit-found <span
class="citation" data-cites="Memon2017">(<a href="#ref-Memon2017"
role="doc-biblioref">Memon et al. 2017</a>)</span>. Using these
distances, we form a probability distribution where the suspects with
files closer to the test have a higher probability of being the culprit
<span class="citation" data-cites="Ziftci2013a Ziftci2017">(<a
href="#ref-Ziftci2013a" role="doc-biblioref">Ziftci and Ramavajjala
2013</a>; <a href="#ref-Ziftci2017" role="doc-biblioref">Ziftci and
Reardon 2017</a>)</span>. Second, we look at the historical likelihood
of a true breakage (versus a flaky one) to determine the probability
that there is no culprit. Combining that distribution with the one from
the minimum build distance forms a prior distribution we use for culprit
finding.</p>
<p>Using this scheme, flaky tests tend to get reruns at the <span
class="math inline">\(s_n\)</span> (the suspect where the failure was
detected) as their first test executions during culprit finding. This is
intentional as a large fraction of the search ranges we culprit find
tend to be flaky and this reduces the cost of proving that. In the
empirical evaluation (Section <a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation">5</a>), we
examine the effect of the prior distribution on both cost and
accuracy.</p>
<h1 id="sec:verification">Culprit Verification for Accuracy
Evaluation</h1>
<p>The previous sections detailed our culprit finding algorithm (FACF)
and the baseline algorithm we compare against (deflaked bisection). In
order to evaluate the accuracy of a culprit finding system for a
particular dataset, we need to know which commits are the actual culprit
commits. If the actual culprit commits are known, then we can compute
the usual measures (True Positive Rate, False Positive Rate, Accuracy,
Precision, Recall, etc...) <span class="citation"
data-cites="Tharwat2021">(<a href="#ref-Tharwat2021"
role="doc-biblioref">Tharwat 2021</a>)</span>. Previous work in culprit
finding, test case selection, fault localization and related research
areas have often used curated datasets (e.g. Defects4J <span
class="citation" data-cites="Just2014">(<a href="#ref-Just2014"
role="doc-biblioref">Just, Jalali, and Ernst 2014</a>)</span>), bug
databases <span class="citation"
data-cites="Bhattacharya2010 Herzig2015 Najafi2019a">(<a
href="#ref-Bhattacharya2010" role="doc-biblioref">Bhattacharya and
Neamtiu 2010</a>; <a href="#ref-Herzig2015" role="doc-biblioref">Herzig
et al. 2015</a>; <a href="#ref-Najafi2019a" role="doc-biblioref">Najafi,
Rigby, and Shang 2019</a>)</span>, and synthetic benchmarks with
injected bugs <span class="citation"
data-cites="Henderson2018 Henderson2019">(<a href="#ref-Henderson2018"
role="doc-biblioref">Henderson and Podgurski 2018</a>; <a
href="#ref-Henderson2019" role="doc-biblioref">Henderson, Podgurski, and
Kucuk 2019</a>)</span>. But our primary interest is in the accuracy of
the algorithms in the Google environment, not the accuracy on open
source bug databases, curated datasets, or synthetic bugs.</p>
<p>At Google, we do not have a comprehensive database of developer
investigations of every continuous integration test failure – indeed
there are far too many failures per day for developers to examine and
evaluate every single one. We also didn’t want to completely rely on
human feedback to tell us when our culprit finding systems have
identified the wrong culprit – as had been previously done <span
class="citation" data-cites="Ziftci2017">(<a href="#ref-Ziftci2017"
role="doc-biblioref">Ziftci and Reardon 2017</a>)</span>.</p>
<p>Instead, we wanted to be able to verify whether or not an identified
culprit for a particular test and search range is correct or not. The
verification needs to be automated such that the we can operationally
monitor the accuracy of the deployed system and catch any performance
regressions in the culprit finder. Continuous monitoring ensures a high
quality user experience by alerting our team to accuracy
degradations.</p>
<h2 id="culprit-finding-conclusions">Culprit Finding Conclusions</h2>
<p>A culprit finder may draw one of two conclusions:</p>
<ol>
<li><p><strong>FI</strong>: <code>FLAKE_IDENTIFIED</code> <em>the
culprit finder determined the status transition was caused by
non-deterministic behavior.</em></p></li>
<li><p><strong>CVI</strong>:
<code>CULPRIT_VERSION_IDENTIFIED(version)</code> <em>the culprit finder
determined a specific <code>version</code> was identified as the
culprit</em>.</p></li>
</ol>
<p>The verification system will label each conclusion examined with
either <code>CORRECT</code> or <code>INCORRECT</code> from which we can
define the standard counts for True and False Positives and Negatives.
Accuracy, Precision, and Recall can then be directly computed.</p>
<h2 id="when-is-a-culprit-incorrect">When is a Culprit Incorrect?</h2>
<p>For ease of discussion, we will use the notation introduced in
Section <a href="#sec:background:cf" data-reference-type="ref"
data-reference="sec:background:cf">2.2</a> for the search range <span
class="math inline">\(S\)</span> with <span
class="math inline">\(s_0\)</span> indicating the previous passing
version, <span class="math inline">\(s_n\)</span> indicating the version
where the failure was first detected, and <span
class="math inline">\(s_k\)</span> indicating the identified culprit.
Now, consider the two cases</p>
<ol>
<li><p><code>FLAKE_IDENTIFIED</code> these are incorrect if no
subsequent runs pass at the version where the original failure was
detected (no flaky behavior can be reproduced).</p></li>
<li><p><code>CULPRIT_VERSION_IDENTIFIED(version)</code> these are
incorrect if we can prove any of the following:</p>
<ol>
<li><p>A pass at <span class="math inline">\(s_n\)</span> indicates the
failure was a flake.</p></li>
<li><p>A pass at the culprit <span class="math inline">\(s_k\)</span>
indicates that the culprit was incorrectly identified.</p></li>
<li><p>No passes detected at the version prior to the culprit, <span
class="math inline">\(s_{k-1}\)</span>, indicates that the culprit was
incorrectly identified.</p></li>
</ol>
<p>Additionally the following conditions indicate the culprit may have
been misidentified due to a test dependency on either time or some
unknown external factor:</p>
<ol>
<li><p>No passes are identified at <span
class="math inline">\(s_0\)</span>, indicating the test failure may be
time or environment dependent (and the conclusion cannot be
trusted).</p></li>
<li><p>No pass at <span class="math inline">\(s_0\)</span> is detected
prior (in the time dimension) to a failure at <span
class="math inline">\(s_n\)</span>.</p></li>
<li><p>No failure at <span class="math inline">\(s_n\)</span> is
detected prior (in the time dimension) to a pass at <span
class="math inline">\(s_0\)</span>.</p></li>
<li><p>No pass at <span class="math inline">\(s_{k-1}\)</span> is
detected prior (in the time dimension) to a failure at <span
class="math inline">\(s_k\)</span>.</p></li>
<li><p>No failure at <span class="math inline">\(s_k\)</span> is
detected prior (in the time dimension) to a pass at <span
class="math inline">\(s_{k-1}\)</span>.</p></li>
<li><p>Verification is not reproducible 17 hours later.<a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a></p></li>
</ol></li>
</ol>
<p>Thus, our approach for determining whether or not a culprit is
correct is to do extensive reruns for a particular conclusion.</p>
<h2 id="how-many-reruns-to-conduct-to-verify-a-conclusion">How Many
Reruns to Conduct to Verify a Conclusion</h2>
<p>A test run <span class="math inline">\(x\)</span> times fails every
time with the rate <span class="math inline">\(\hat{f}_t^x\)</span>. If
we want to achieve confidence level <span
class="math inline">\(C\)</span> (ex. <span class="math inline">\(C =
.99999999\)</span>) of verifier correctness, we can compute the number
of reruns to conduct for each check with <span class="math inline">\(x =
\left\lceil{\frac{\,\textrm{log}\!\!\left({1 -
C}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)</span>.</p>
<h2 id="sampling-strategy">Sampling Strategy</h2>
<p>While it would be ideal to verify every single culprit for every
single test, the expense, in practice, of such an operation is too high.
We conduct verification with a very high confidence level (<span
class="math inline">\(C = 0.99999999\)</span>) and a minimum on the
estimated flakiness rate of <span class="math inline">\(0.1\)</span> to
ensure a minimum of 8 reruns for each check even for targets that have
not been historically flaky. With such an expensive configuration, we
can only afford the following sampling strategy.</p>
<ol>
<li><p>Randomly sample at <span class="math inline">\(1\%\)</span> of
culprit finding conclusions.</p></li>
<li><p>Sample the first conclusion for each unique culprit
version.</p></li>
</ol>
<p>We tag which sampling strategy was used to sample a particular
conclusion allowing us to compute statistics for each sampling frame
independently.</p>
<h2 id="the-verified-culprits-dataset">The Verified Culprits
Dataset</h2>
<p>Using our production culprit finders based on the FACF algorithm and
a verifier implementing the above verification strategy, we have
produced an internal dataset with 144,130 verified conclusions in the
last 60 days, which we use in Section <a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation">5</a> to
evaluate the performance of the algorithms presented in this paper.
Note, because the evaluation of an algorithm is also expensive, we only
conduct the evaluation on a subset of the whole dataset.</p>
<p>While we are unable to make the dataset public, we encourage others
to use our verification approach on their own culprit finding systems
and when possible share these datasets with the external community for
use in future studies. Such datasets are not only useful for culprit
finding research but also for fault localization, test prioritization,
test selection, automatic program repair, and other topics.</p>

<h1 id="sec:evaluation">Empirical Evaluation</h1>

<p>We empirically evaluated the behavior of the probabilistic Flake
Aware Culprit Finding (FACF) algorithm, compared it to the traditional
Bisect algorithm <span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span>,
evaluated the effectiveness of adding deflaking runs to Bisect, and
evaluated the effectiveness of two optimizations for FACF.</p>
<h2 id="research-questions">Research Questions</h2>
<p>The following research questions were considered. All questions were
considered with respect to the Google environment (see Section <a
href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset">5.5</a> for more details about the dataset
used).</p>
<ol>
<li><p>Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?</p></li>
<li><p>What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to accuracy.</p></li>
<li><p>How efficient is each algorithm as measured in number of test
executions?</p></li>
<li><p>What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to efficiency.</p></li>
<li><p>Does flakiness affect culprit finding accuracy and cost?</p></li>
</ol>
<h2 id="measuring-culprit-finding-accuracy">Measuring Culprit Finding
Accuracy</h2>
<p>To measure the accuracy of the culprit finders we produce a binary
variable (“correct”) which is true if the culprit finder identified
version matched the culprit in the dataset. Accuracy is then defined as
the sample proportion: <span class="math inline">\(\textbf{ACC} =
\frac{\text{\# correct}}{\text{Total}}\)</span>. Culprit finder “A” is
judged more accurate than culprit finder “B” if A’s <strong>ACC</strong>
value is higher than B’s and a statistical test (described in Section <a
href="#sec:stat-tests" data-reference-type="ref"
data-reference="sec:stat-tests">5.4</a>) determines the difference is
significant.</p>
<h2 id="measuring-culprit-finding-cost">Measuring Culprit Finding
Cost</h2>
<p>We measure cost as the number of test executions performed by the
algorithm. Culprit finder “A” is judged lower cost than culprit finder
“B” if the mean number of test executions is less than “B” and a
statistical test (described in Section <a href="#sec:stat-tests"
data-reference-type="ref" data-reference="sec:stat-tests">5.4</a>)
determines the difference is significant.</p>
<h2 id="sec:stat-tests">On Our Usage of Statistical Tests</h2>
<p>For answering RQ1 and RQ2, we observe the distribution per
culprit-finding algorithm of whether or not the algorithm identified the
culprit (0 or 1). For answering RQ3 and RQ4, we observe the distribution
per culprit-finding algorithm of the number of test executions required
to complete culprit finding.</p>
<p>We follow the recommendations of McCrum-Gardner <span
class="citation" data-cites="McCrum-Gardner2008">(<a
href="#ref-McCrum-Gardner2008" role="doc-biblioref">McCrum-Gardner
2008</a>)</span> as well as Walpole <em>et al.</em> <span
class="citation" data-cites="Walpole2007">(<a href="#ref-Walpole2007"
role="doc-biblioref">Walpole 2007</a>)</span> as to the proper test
statistics for our analysis. Since data points for a specific culprit
range have a correspondence across algorithms, we perform paired
statistical tests when feasible. The analysis is complicated by the
observation that neither the dichotomous distribution of correctness nor
the numerical distribution of number of test executions are normally
distributed. Following McCrum-Gardner we opt to use non-parametric
tests. Additionally different tests are used for accuracy (dichotomous
nominal variable) and number of executions (numerical variable)
respectively.</p>
<p>As we have more than 2 groups to consider (10 algorithms), we
initially perform omnibus tests over all algorithms to justify the
existence of statistical significance over all algorithms. In response
to the multiple comparisons problem <span class="citation"
data-cites="Walpole2007">(<a href="#ref-Walpole2007"
role="doc-biblioref">Walpole 2007</a>)</span>, we compute the corrected
significance bound using the formula for the family-wise error rate. Our
experimental design has us first conduct the ANOVA significance test to
determine if any of the algorithms differed in performance. Then we
conduct post-hoc testing to determine which algorithm’s performance
differed. For each considered question we conduct <span
class="math inline">\(\binom{10}{2} + 1\)</span> significance tests. We
have <span class="math inline">\(6\)</span> considered questions which
we use significance testing to answer, giving us a total <span
class="math inline">\(r =
6\left(\binom{10}{2} + 1\right) = 276\)</span> tests.</p>
<p>Solving for the <span class="math inline">\(\alpha\)</span> in the
experiment-wise error rate formula from Walpole (pg. 529 <span
class="citation" data-cites="Walpole2007">(<a href="#ref-Walpole2007"
role="doc-biblioref">Walpole 2007</a>)</span>) with <span
class="math inline">\(p\)</span> standing for the probability of a false
rejection of at least one hypothesis in the experiment gives: <span
class="math inline">\(\alpha = 1 - (1 - p)^{\frac{1}{r}}\)</span>. Given
<span class="math inline">\(r = 276\)</span> tests and an intended
experiment-wise error rate of <span class="math inline">\(p =
0.001\)</span>, we conservatively arrive at <span
class="math inline">\(\alpha =
10^{-6}\)</span> after rounding down to control for family-wise error
rate. We use <span class="math inline">\(\alpha\)</span> as our
significance level in all tests.</p>
<p>Cochran’s Q Test is used for significance testing of the accuracy
distributions and Friedman’s <span
class="math inline">\(\chi^{2}\)</span> Test for number of executions.
Post-hoc testing was conducted with McNemar’s test and Wilcoxon Signed
Rank Test respectively.</p>
<h2 id="sec:dataset">Evaluation Dataset</h2>
<p>For the study we used a subset of the dataset produced by the
production Culprit Verifier (described in Section <a
href="#sec:verification" data-reference-type="ref"
data-reference="sec:verification">4</a>). The evaluation dataset
consists of 13600 test breakages with their required build flags, search
range of versions, and the verified culprit. The verified culprit may be
an indicator that there was no culprit and detected transition was
caused by a flake. Approximately 60% of the items had a culprit version
and 40% did not (as these were caused by a flaky failure).</p>

<p>Multiple tests may be broken by the same version. When looking at
unique search ranges, we actually see approximately twice as many flaky
ranges as non-flaky ones. This hints towards having tighter bounds on
which ranges are worth culprit finding as all of the runs for two-thirds
of ranges would have been better utilized on more deflaking runs if
these two categories are accurately discernable.</p>
<p>We therefore examine the overall dataset for discrepancies between
the two categories. For example, flaky ranges have, on average, 8 tests
failing while non-flaky ranges have 224. For the objective of finding
unique real culprits, we are interested in the minimum flake rate over
targets in a range for flaky and non-flaky ranges, respectively. Fig <a
href="#fig:min-flake-rate" data-reference-type="ref"
data-reference="fig:min-flake-rate">[fig:min-flake-rate]</a> shows us
that, in fact, 99% of real culprit ranges have targets below a flake
rate of 75%.</p>

<h2 id="results">Results</h2>

<span id="fig:intervals" label="fig:intervals">
[![Figure 2-4](images/icst-2023/fig2-4.png)](images/icst-2023/fig2-4.png)
</span>

<span id="fig:min-flake-rate" label="fig:min-flake-rate">
[![Figure 5](images/icst-2023/fig5.png)](images/icst-2023/fig5.png)
</span>

<h3
id="rq1-which-algorithm-facf-or-bisect-was-more-accurate-in-identifying-culprit-commits">RQ1:
Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?</h3>
<p>All versions of FACF were more accurate overall than all versions of
Bisect (Figure <a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc">2</a>). The difference was
significant with a p-value <span class="math inline">\(&lt;
10^{-12}\)</span>, below the adjusted <span
class="math inline">\(10^{-6}\)</span> significance level required.</p>
<h3
id="rq2-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-accuracy">RQ2:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to accuracy?</h3>
<p>The production version of FACF, FACF(all), was most accurate and
significantly different than all other algorithms (Figure <a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc">2</a>). This was followed by
FACF(fr=.1) which set the floor of 10% on the estimated flake rate for
tests. The other FACF variants were equally accurate.</p>
<h3
id="rq3-how-efficient-is-each-algorithm-as-measured-in-number-of-test-executions">RQ3:
How efficient is each algorithm as measured in number of test
executions?</h3>
<p>Figure <a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:builds">2</a> shows the most efficient
algorithm was unsurprisingly Bisect(1) which does not attempt to do any
extra executions to deflake test failures. The next most efficient
algorithm was FACF(all) which was also the most accurate. As can be seen
in the boxplot in Figure <a href="#fig:intervals"
data-reference-type="ref" data-reference="fig:intervals:builds">2</a>,
the FACF variants tend to have many more extreme outliers in their cost
distribution than the Bisect variants. This is because they account for
the prior flake rate. An organization may control their cost by not
culprit finding tests which are more than (for instance) 50% flaky. No
such upper bound was established for this experiment.</p>
<h3
id="rq4-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-efficiency">RQ4:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to efficiency?</h3>
<p>First, both FACF(all) and FACF(priors) had less cost than the
standard FACF. This indicates that while FACF(priors) does not improve
accuracy it does improve efficiency of the algorithm. In particular many
more culprits were found with just two builds.</p>
<p>Second, FACF(heur-thresholds) had no significant effect on cost nor
did it show any improvement accuracy. We conclude that although the
mathematical motivation may be sound, in the Google environment, we
cannot validate a benefit to this optimization at this time.</p>
<h3
id="rq5-does-flakiness-affect-culprit-finding-accuracy-and-cost">RQ5:
Does flakiness affect culprit finding accuracy and cost?</h3>
<p>Figures <a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals">3</a> and <a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals">4</a> show the
accuracy and cost of culprit finding flaky and non-flaky culprits. For
search ranges without a culprit, FACF(all) clearly dominates Bisect in
accuracy and even has a small advantage in cost over Bisect(1).</p>
<p>When there is a culprit, the FACF variants are all more accurate than
the Bisect variants according to the statistical tests. However, the
difference is much less pronounced with Bisect(1) accuracy <span
class="math inline">\(\sim\)</span>96.1% and FACF(all) accuracy <span
class="math inline">\(\sim\)</span>97.5%. In terms of cost, Bisect(1)
and Bisect(2) are the most efficient followed by the FACF variants.</p>
<p>We will note, a production culprit finder cannot know a priori if a
breakage was caused by a flake. It would need to do deflaking runs to
establish that, costing approximately <span
class="math inline">\(\left\lceil{\frac{\,\textrm{log}\!\!\left({p}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)</span>
where <span class="math inline">\(p\)</span> is your desired confidence
level. For <span class="math inline">\(p=10^{-4}\)</span> and <span
class="math inline">\(\hat{f} = .2\)</span>, this amounts to 6 extra
runs, which would significantly increase the cost of bisection. However,
as noted in Section <a href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset">5.5</a>, prior flakiness rates could be
used to prune out a significant portion of flaky ranges (lowering cost)
with a minimal hit to accuracy.</p>

<h1 id="related-work">Related Work</h1>
<h2 id="culprit-finding">Culprit Finding</h2>
<p>Despite its industrial importance, culprit finding has been
relatively less studied in the literature in comparison to topics such
as Fault Localization <span class="citation"
data-cites="Agarwal2014 Wong2016">(<a href="#ref-Agarwal2014"
role="doc-biblioref">Agarwal and Agrawal 2014</a>; <a
href="#ref-Wong2016" role="doc-biblioref">Wong et al. 2016</a>)</span>
(coverage based <span class="citation"
data-cites="Jones2002 Jones2005 Lucia2014 Henderson2018 Henderson2019 Kucuk2021">(<a
href="#ref-Jones2002" role="doc-biblioref">J. a. Jones, Harrold, and
Stasko 2002</a>; <a href="#ref-Jones2005" role="doc-biblioref">J. A.
Jones and Harrold 2005</a>; <a href="#ref-Lucia2014"
role="doc-biblioref">Lucia et al. 2014</a>; <a href="#ref-Henderson2018"
role="doc-biblioref">Henderson and Podgurski 2018</a>; <a
href="#ref-Henderson2019" role="doc-biblioref">Henderson, Podgurski, and
Kucuk 2019</a>; <a href="#ref-Kucuk2021" role="doc-biblioref">Kucuk,
Henderson, and Podgurski 2021</a>)</span>, information retrieval based
<span class="citation" data-cites="Zhou2012 Youm2015 Ciborowska2022">(<a
href="#ref-Zhou2012" role="doc-biblioref">J. Zhou, Zhang, and Lo
2012</a>; <a href="#ref-Youm2015" role="doc-biblioref">Youm et al.
2015</a>; <a href="#ref-Ciborowska2022" role="doc-biblioref">Ciborowska
and Damevski 2022</a>)</span>, or slicing based <span class="citation"
data-cites="Podgurski1990 Horwitz1992 Ren2004">(<a
href="#ref-Podgurski1990" role="doc-biblioref">Podgurski and Clarke
1990</a>; <a href="#ref-Horwitz1992" role="doc-biblioref">Horwitz and
Reps 1992</a>; <a href="#ref-Ren2004" role="doc-biblioref">Ren et al.
2004</a>)</span>), Bug Prediction <span class="citation"
data-cites="Lewis2013 Punitha2013 Osman2017">(<a href="#ref-Lewis2013"
role="doc-biblioref">Lewis et al. 2013</a>; <a href="#ref-Punitha2013"
role="doc-biblioref">Punitha and Chitra 2013</a>; <a
href="#ref-Osman2017" role="doc-biblioref">Osman et al.
2017</a>)</span>, Test Selection <span class="citation"
data-cites="Engstrom2010 pan2022">(<a href="#ref-Engstrom2010"
role="doc-biblioref">Engström, Runeson, and Skoglund 2010</a>; <a
href="#ref-pan2022" role="doc-biblioref">Pan et al. 2022</a>)</span>,
Test Prioritization <span class="citation"
data-cites="DeS.CamposJunior2017 pan2022">(<a
href="#ref-DeS.CamposJunior2017" role="doc-biblioref">de S. Campos
Junior et al. 2017</a>; <a href="#ref-pan2022" role="doc-biblioref">Pan
et al. 2022</a>)</span> and the SZZ algorithm for identifying bug
inducing commits in the context of research studies <span
class="citation"
data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019">(<a
href="#ref-Sliwerski2005" role="doc-biblioref">Śliwerski, Zimmermann,
and Zeller 2005</a>; <a href="#ref-Rodriguez-Perez2018"
role="doc-biblioref">Rodríguez-Pérez, Robles, and González-Barahona
2018</a>; <a href="#ref-Borg2019" role="doc-biblioref">Borg et al.
2019</a>; <a href="#ref-Wen2019" role="doc-biblioref">Wen et al.
2019</a>)</span>. This is perhaps understandable as the problem only
emerges in environments were it becomes to costly to run every test at
every code submission.</p>
<p>The idea of using a binary search to locate the culprit is common,
but it is not completely clear who invented it first. The Christian
Couder of <code>git bisect</code> wrote a comprehensive paper on its
implementation <span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span> but
indicates there are other similar tools: “So what are the tools used to
fight regressions? [...] The only specific tools are test suites and
tools similar to <code>git bisect</code>.” The first version of the
<code>git bisect</code> command appears to have been written by Linus
Torvalds in 2005.<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a> Torvald’s commit message simply
states “This is useful for doing binary searching for problems.” The
source code for the mercurial bisect command <code>hg bisect</code>
states that it was inspired by the git command.<a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> The
Subversion (SVN) command, <code>svn bisect</code>, also cites the git
command as inspiration.<a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> Therefore, as far as the authors can
tell bisection for finding culprits appears to have been invented by
Torvalds.</p>
<p>The Couder paper <span class="citation" data-cites="Couder2008">(<a
href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>)</span>
cites a GitHub repository for a program called “BBChop,” which is
lightly documented and appears not entirely completed by a user
enigmatically named Ealdwulf Wuffinga (likely a pseudonym, as that was a
name of the King of East Anglia from 664-713).<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
This program claims to have also implemented a Bayesian search for
culprits. We gladly cede the throne for first invention of applying
Bayes rule to culprit finding to Ealdwulf and note that our invention
was independent of the work in BBChop.</p>
<p>More recently, work has been emerging on the industrial practice of
Culprit Finding. In 2013, Ziftci and Ramavajjala gave a public talk at
the Google Test Automation Conference in on Culprit Finding at Google
<span class="citation" data-cites="Ziftci2013a">(<a
href="#ref-Ziftci2013a" role="doc-biblioref">Ziftci and Ramavajjala
2013</a>)</span>. They note, that by 2013 Google has already been using
<span class="math inline">\(m\)</span>-ary bisection based culprit
finding for “small” and “medium” tests. They then give a preview of an
approach elaborated on in Ziftci and Reardon’s 2017 paper <span
class="citation" data-cites="Ziftci2017">(<a href="#ref-Ziftci2017"
role="doc-biblioref">Ziftci and Reardon 2017</a>)</span> for culprit
finding integration tests where test executions take a long time to run.
The authors propose a suspiciousness metric based on the minimum build
dependence distance and conduct a case study on the efficacy of using
such a metric to surface potential culprits to developers. We utilize a
similar build dependence distance as input to construct a prior
probability (see Section <a href="#sec:priors" data-reference-type="ref"
data-reference="sec:priors">3.3</a>) in this work but do not directly
surface the metric to our end users. Finally, for ground truth they rely
on the developers to report what the culprit version is. In contrast,
the empirical evaluation we present in this new work is fully automated
and does not rely on developers.</p>
<p>In 2017, Saha and Gligoric <span class="citation"
data-cites="Saha2017">(<a href="#ref-Saha2017" role="doc-biblioref">Saha
and Gligoric 2017</a>)</span> proposed accelerating the traditional
bisect algorithm via Test Selection techniques. They use Coverage Based
Test Selection <span class="citation" data-cites="Zhou2010 Musa2015">(<a
href="#ref-Zhou2010" role="doc-biblioref">Z. Q. Zhou 2010</a>; <a
href="#ref-Musa2015" role="doc-biblioref">Musa et al. 2015</a>)</span>
to choose which commits are most likely to affect the test results at
the failing commit. This technique can be viewed as fine grained,
dynamic version of the static build dependence selection strategy we
employ at Google.</p>
<p>In 2019, Najafi <em>et al.</em> <span class="citation"
data-cites="Najafi2019a">(<a href="#ref-Najafi2019a"
role="doc-biblioref">Najafi, Rigby, and Shang 2019</a>)</span> looked at
a similar problem to the traditional culprit finding problem we examine
here. In their scenario, there is a submission queue that queues up
commits to integrate into the main development branch. It batches these
waiting commits and tests them together. If a test fails, they need to
culprit find to determine the culprit commit. The authors evaluate using
a ML-driven risk based batching approach. In 2022, the same research
group re-evaluated the 2019 paper on an open source dataset in
Beheshtian <em>et al.</em>’s 2022 paper <span class="citation"
data-cites="Beheshtian2022">(<a href="#ref-Beheshtian2022"
role="doc-biblioref">Beheshtian, Bavand, and Rigby 2022</a>)</span>.
They found that in the new environment the risk based batching didn’t do
as well as a new and improved combinatorics based batching scheme for
identifying the culprit commits.</p>
<p>James Keenan gave a presentation in 2019 at the Perl Conference in
which he presented a concept, <em>multisection</em>, for dealing with
multiple bugs in the same search range <span class="citation"
data-cites="keenan2019">(<a href="#ref-keenan2019"
role="doc-biblioref">Keenan 2019</a>)</span>. Ideally, industrial
culprit finders would automatically detect that there are multiple
distinct bug inducing commits.</p>
<p>In 2021, two papers looked at using coverage based data for
accelerating bug localization. An and Yoo <span class="citation"
data-cites="An2021">(<a href="#ref-An2021" role="doc-biblioref">An and
Yoo 2021</a>)</span> used coverage data to accelerate both bisection and
SZZ <span class="citation" data-cites="Sliwerski2005">(<a
href="#ref-Sliwerski2005" role="doc-biblioref">Śliwerski, Zimmermann,
and Zeller 2005</a>)</span> and found significant speedups. Wen <em>et
al.</em> <span class="citation" data-cites="Wen2021">(<a
href="#ref-Wen2021" role="doc-biblioref">Wen et al. 2021</a>)</span>
combined traditional Coverage Based Statistical Fault Localization <span
class="citation" data-cites="Jones2002">(<a href="#ref-Jones2002"
role="doc-biblioref">J. a. Jones, Harrold, and Stasko 2002</a>)</span>
with historical information to create a new suspiciousness score <span
class="citation" data-cites="Sun2016">(<a href="#ref-Sun2016"
role="doc-biblioref">Sun and Podgurski 2016</a>)</span>. This fault
localization method could be used in a culprit finding context by using
the line rankings to rank the commits.</p>
<p>Finally, in 2022 Frolin Ocariza <span class="citation"
data-cites="Ocariza2022">(<a href="#ref-Ocariza2022"
role="doc-biblioref">Ocariza 2022</a>)</span> published a paper on
bisecting performance regressions. There are strong connections between
a flaky test and a performance regression as a performance regression
may not happen 100% of the time. Furthermore, the “baseline” version may
randomly have poor performance due to environmental or other factors.
Ocariza also arrives at a probabilistic approach, using a Bayesian model
to determine whether a run at a particular version is exhibiting the
performance regression or not. While mathematically related to our work
here and complementary, the approach is distinct. The authors note that
their technique can be combined with a Noisy Binary Search citing Karp
<span class="citation" data-cites="Karp2007">(<a href="#ref-Karp2007"
role="doc-biblioref">Karp and Kleinberg 2007</a>)</span> or with a
multisection search citing Keenan <span class="citation"
data-cites="keenan2019">(<a href="#ref-keenan2019"
role="doc-biblioref">Keenan 2019</a>)</span>.</p>
<h2 id="noisy-binary-search">Noisy Binary Search</h2>
<p>There is a vast literature on Rényi-Ulam games and Noisy Searching
problems (doubly so when considering their connection to error
correcting codes)! We direct readers to the Pelc survey <span
class="citation" data-cites="Pelc2002">(<a href="#ref-Pelc2002"
role="doc-biblioref">Pelc 2002</a>)</span> as a starting point. We will
highlight a few articles here. First and foremost, the Ben Or and
Hassidim paper <span class="citation" data-cites="Ben-Or2008">(<a
href="#ref-Ben-Or2008" role="doc-biblioref">Ben-Or and Hassidim
2008</a>)</span> from 2008 most clearly explains the problem in terms of
a Bayesian inference, which is the formulation we use here. Second,
Waeber <em>et al.</em> in 2013 <span class="citation"
data-cites="Waeber2013">(<a href="#ref-Waeber2013"
role="doc-biblioref">Waeber, Frazier, and Henderson 2013</a>)</span>
discuss the theoretical foundations of these <em>probabilistic bisection
algorithms</em>. Finally, according to Waeber, Horstein first described
this algorithm in the context of error correcting codes in 1963 <span
class="citation" data-cites="Horstein1963">(<a href="#ref-Horstein1963"
role="doc-biblioref">Horstein 1963</a>)</span>.</p>
<h1 id="sec:conclusion">Conclusion</h1>
<p>Flake Aware Culprit Finding (FACF) is both accurate, efficient, and
able to flexibly incorporate prior information on the location of a
culprit. A large scale empirical study on real test and build breakages
found FACF to be significantly more effective than a traditional
bisection search while not increasing cost over a deflaked
bisection.</p>



<h1 id="sec:refereces">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<ol>
<li><div id="ref-Agarwal2014" class="csl-entry" role="doc-biblioentry">
Agarwal, Pragya, and Arun Prakash Agrawal. 2014.
<span>“Fault-Localization <span>Techniques</span> for <span>Software
Systems</span>: <span>A Literature Review</span>.”</span> <em>SIGSOFT
Softw. Eng. Notes</em> 39 (5): 1–8. <a
href="https://doi.org/10.1145/2659118.2659125">https://doi.org/10.1145/2659118.2659125</a>.
</div></li>
<li><div id="ref-An2021" class="csl-entry" role="doc-biblioentry">
An, Gabin, and Shin Yoo. 2021. <span>“Reducing the Search Space of Bug
Inducing Commits Using Failure Coverage.”</span> In <em>Proceedings of
the 29th <span>ACM Joint Meeting</span> on <span>European Software
Engineering Conference</span> and <span>Symposium</span> on the
<span>Foundations</span> of <span>Software Engineering</span></em>,
1459–62. <span>Athens Greece</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3468264.3473129">https://doi.org/10.1145/3468264.3473129</a>.
</div></li>
<li><div id="ref-Ananthanarayanan2019" class="csl-entry"
role="doc-biblioentry">
Ananthanarayanan, Sundaram, Masoud Saeida Ardekani, Denis Haenikel,
Balaji Varadarajan, Simon Soriano, Dhaval Patel, and Ali Reza
Adl-Tabatabai. 2019. <span>“Keeping Master Green at Scale.”</span>
<em>Proceedings of the 14th EuroSys Conference 2019</em>. <a
href="https://doi.org/10.1145/3302424.3303970">https://doi.org/10.1145/3302424.3303970</a>.
</div></li>
<li><div id="ref-Beheshtian2022" class="csl-entry" role="doc-biblioentry">
Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby.
2022. <span>“Software <span>Batch Testing</span> to <span>Save Build
Test Resources</span> and to <span>Reduce Feedback Time</span>.”</span>
<em>IEEE Transactions on Software Engineering</em> 48 (8): 2784–2801. <a
href="https://doi.org/10.1109/TSE.2021.3070269">https://doi.org/10.1109/TSE.2021.3070269</a>.
</div></li>
<li><div id="ref-Ben-Or2008" class="csl-entry" role="doc-biblioentry">
Ben-Or, Michael, and Avinatan Hassidim. 2008. <span>“The <span>Bayesian
Learner</span> Is <span>Optimal</span> for <span>Noisy Binary
Search</span> (and <span>Pretty Good</span> for <span>Quantum</span> as
<span>Well</span>).”</span> In <em>2008 49th <span>Annual IEEE
Symposium</span> on <span>Foundations</span> of <span>Computer
Science</span></em>, 221–30. <span>Philadelphia, PA, USA</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/FOCS.2008.58">https://doi.org/10.1109/FOCS.2008.58</a>.
</div></li>
<li><div id="ref-Bhattacharya2010" class="csl-entry" role="doc-biblioentry">
Bhattacharya, Pamela, and Iulian Neamtiu. 2010. <span>“Fine-Grained
Incremental Learning and Multi-Feature Tossing Graphs to Improve Bug
Triaging.”</span> In <em>2010 <span>IEEE International Conference</span>
on <span>Software Maintenance</span></em>, 1–10. <span>Timi oara,
Romania</span>: <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSM.2010.5609736">https://doi.org/10.1109/ICSM.2010.5609736</a>.
</div></li>
<li><div id="ref-Binkley1992" class="csl-entry" role="doc-biblioentry">
Binkley, D. 1992. <span>“Using Semantic Differencing to Reduce the Cost
of Regression Testing.”</span> In <em>Proceedings
<span>Conference</span> on <span>Software Maintenance</span> 1992</em>,
41–50. <span>Orlando, FL, USA</span>: <span>IEEE Comput. Soc.
Press</span>. <a
href="https://doi.org/10.1109/ICSM.1992.242560">https://doi.org/10.1109/ICSM.1992.242560</a>.
</div></li>
<li><div id="ref-Borg2019" class="csl-entry" role="doc-biblioentry">
Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
<span>“<span>SZZ</span> Unleashed: An Open Implementation of the
<span>SZZ</span> Algorithm - Featuring Example Usage in a Study of
Just-in-Time Bug Prediction for the <span>Jenkins</span>
Project.”</span> In <em>Proceedings of the 3rd <span>ACM SIGSOFT
International Workshop</span> on <span>Machine Learning
Techniques</span> for <span>Software Quality Evaluation</span> -
<span>MaLTeSQuE</span> 2019</em>, 7–12. <span>Tallinn, Estonia</span>:
<span>ACM Press</span>. <a
href="https://doi.org/10.1145/3340482.3342742">https://doi.org/10.1145/3340482.3342742</a>.
</div></li>
<li><div id="ref-Ciborowska2022" class="csl-entry" role="doc-biblioentry">
Ciborowska, Agnieszka, and Kostadin Damevski. 2022. <span>“Fast
Changeset-Based Bug Localization with <span>BERT</span>.”</span> In
<em>Proceedings of the 44th <span>International Conference</span> on
<span>Software Engineering</span></em>, 946–57. <span>Pittsburgh
Pennsylvania</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3510003.3510042">https://doi.org/10.1145/3510003.3510042</a>.
</div></li>
<li><div id="ref-Couder2008" class="csl-entry" role="doc-biblioentry">
Couder, Christian. 2008. <span>“Fighting Regressions with Git
Bisect.”</span> <em>The Linux Kernel Archives</em> 4 (5). <a
href="https://www. kernel. org/pub/software/scm/git/doc s/git-
                  bisect-lk2009.html">https://www. kernel.
org/pub/software/scm/git/doc s/git- bisect-lk2009.html</a>.
</div></li>
<li><div id="ref-DeS.CamposJunior2017" class="csl-entry"
role="doc-biblioentry">
de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David,
Regina Braga, Fernanda Campos, and Victor Ströele. 2017. <span>“Test
<span>Case Prioritization</span>: <span>A Systematic Review</span> and
<span>Mapping</span> of the <span>Literature</span>.”</span> In
<em>Proceedings of the 31st <span>Brazilian Symposium</span> on
<span>Software Engineering</span></em>, 34–43. <span>New York, NY,
USA</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3131151.3131170">https://doi.org/10.1145/3131151.3131170</a>.
</div></li>
<li><div id="ref-Engstrom2010" class="csl-entry" role="doc-biblioentry">
Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. <span>“A
Systematic Review on Regression Test Selection Techniques.”</span>
<em>Information and Software Technology</em> 52 (1): 14–30. <a
href="https://doi.org/10.1016/j.infsof.2009.07.001">https://doi.org/10.1016/j.infsof.2009.07.001</a>.
</div></li>
<li><div id="ref-Fowler2006" class="csl-entry" role="doc-biblioentry">
Fowler, Martin. 2006. <span>“Continuous
<span>Integration</span>.”</span> <a
href="https://martinfowler.com/articles/
                  continuousIntegration.html">https://martinfowler.com/articles/
continuousIntegration.html</a>.
</div></li>
<li><div id="ref-Gupta2011" class="csl-entry" role="doc-biblioentry">
Gupta, Pooja, Mark Ivey, and John Penix. 2011. <span>“Testing at the
Speed and Scale of <span>Google</span>.”</span> <a
href="http://google-engtools.blogspot.com/2011/06/testing-at-
                  speed-and-scale-of-google.html">http://google-engtools.blogspot.com/2011/06/testing-at-
speed-and-scale-of-google.html</a>.
</div></li>
<li><div id="ref-Henderson2018" class="csl-entry" role="doc-biblioentry">
Henderson, Tim A. D., and Andy Podgurski. 2018. <span>“Behavioral
<span>Fault Localization</span> by <span>Sampling Suspicious Dynamic
Control Flow Subgraphs</span>.”</span> In <em>2018 <span>IEEE</span>
11th <span>International Conference</span> on <span>Software
Testing</span>, <span>Verification</span> and <span>Validation</span>
(<span>ICST</span>)</em>, 93–104. <span>Vasteras</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICST.2018.00019">https://doi.org/10.1109/ICST.2018.00019</a>.
</div></li>
<li><div id="ref-Henderson2019" class="csl-entry" role="doc-biblioentry">
Henderson, Tim A. D., Andy Podgurski, and Yigit Kucuk. 2019.
<span>“Evaluating <span>Automatic Fault Localization Using Markov
Processes</span>.”</span> In <em>2019 19th <span>International Working
Conference</span> on <span>Source Code Analysis</span> and
<span>Manipulation</span> (<span>SCAM</span>)</em>, 115–26.
<span>Cleveland, OH, USA</span>: <span>IEEE</span>. <a
href="https://doi.org/10.1109/SCAM.2019.00021">https://doi.org/10.1109/SCAM.2019.00021</a>.
</div></li>
<li><div id="ref-Herzig2015" class="csl-entry" role="doc-biblioentry">
Herzig, Kim, Michaela Greiler, Jacek Czerwonka, and Brendan Murphy.
2015. <span>“The <span>Art</span> of <span>Testing Less</span> Without
<span>Sacrificing Quality</span>.”</span> In <em>2015
<span>IEEE</span>/<span>ACM</span> 37th <span>IEEE International
Conference</span> on <span>Software Engineering</span></em>, 1:483–93.
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE.2015.66">https://doi.org/10.1109/ICSE.2015.66</a>.
</div></li>
<li><div id="ref-Herzig2015a" class="csl-entry" role="doc-biblioentry">
Herzig, Kim, and Nachiappan Nagappan. 2015. <span>“Empirically
<span>Detecting False Test Alarms Using Association
Rules</span>.”</span> In <em>2015 <span>IEEE</span>/<span>ACM</span>
37th <span>IEEE International Conference</span> on <span>Software
Engineering</span></em>, 39–48. <span>Florence, Italy</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE.2015.133">https://doi.org/10.1109/ICSE.2015.133</a>.
</div></li>
<li><div id="ref-Horstein1963" class="csl-entry" role="doc-biblioentry">
Horstein, M. 1963. <span>“Sequential Transmission Using Noiseless
Feedback.”</span> <em>IEEE Transactions on Information Theory</em> 9
(3): 136–43. <a
href="https://doi.org/10.1109/TIT.1963.1057832">https://doi.org/10.1109/TIT.1963.1057832</a>.
</div></li>
<li><div id="ref-Horwitz1992" class="csl-entry" role="doc-biblioentry">
Horwitz, S, and T Reps. 1992. <span>“The Use of Program Dependence
Graphs in Software Engineering.”</span> In <em>International
<span>Conference</span> on <span>Software Engineering</span></em>,
9:349. <span>Springer</span>. <a
href="http://portal.acm.org/citation.cfm?id=24041&amp;amp;dl=">http://portal.acm.org/citation.cfm?id=24041&amp;amp;dl=</a>.
</div></li>
<li><div id="ref-Jones2002" class="csl-entry" role="doc-biblioentry">
Jones, J.a., M. J. Harrold, and J. Stasko. 2002. <span>“Visualization of
Test Information to Assist Fault Localization.”</span> <em>Proceedings
of the 24th International Conference on Software Engineering. ICSE
2002</em>. <a
href="https://doi.org/10.1145/581339.581397">https://doi.org/10.1145/581339.581397</a>.
</div></li>
<li><div id="ref-Jones2005" class="csl-entry" role="doc-biblioentry">
Jones, James A., and Mary Jean Harrold. 2005. <span>“Empirical
<span>Evaluation</span> of the <span class="nocase">Tarantula Automatic
Fault-localization Technique</span>.”</span> In <em>Proceedings of the
20th <span>IEEE</span>/<span>ACM International Conference</span> on
<span>Automated Software Engineering</span></em>, 273–82. <span>New
York, NY, USA</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/1101908.1101949">https://doi.org/10.1145/1101908.1101949</a>.
</div></li>
<li><div id="ref-Just2014" class="csl-entry" role="doc-biblioentry">
Just, René, Darioush Jalali, and Michael D Ernst. 2014.
<span>“<span>Defects4J</span>: <span>A Database</span> of <span>Existing
Faults</span> to <span>Enable Controlled Testing Studies</span> for
<span>Java Programs</span>.”</span> In <em>Proceedings of the 2014
<span>International Symposium</span> on <span>Software Testing</span>
and <span>Analysis</span></em>, 437–40. <span>New York, NY, USA</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/2610384.2628055">https://doi.org/10.1145/2610384.2628055</a>.
</div></li>
<li><div id="ref-Karp2007" class="csl-entry" role="doc-biblioentry">
Karp, Richard M., and Robert Kleinberg. 2007. <span>“Noisy Binary Search
and Its Applications.”</span> In <em>Proceedings of the Eighteenth
Annual <span>ACM-SIAM</span> Symposium on Discrete Algorithms</em>,
881–90. <span>SODA</span> ’07. <span>USA</span>: <span>Society for
Industrial and Applied Mathematics</span>. <a
href="https://dl.acm.org/doi/abs/10.5555/1283383.1283478">https://dl.acm.org/doi/abs/10.5555/1283383.1283478</a>.
</div></li>
<li><div id="ref-keenan2019" class="csl-entry" role="doc-biblioentry">
Keenan, James. 2019. <span>“James <span>E</span>. <span>Keenan</span> -
"<span>Multisection</span>: <span>When Bisection Isn</span>’t
<span>Enough</span> to <span>Debug</span> a
<span>Problem</span>".”</span> <a
href="https://www.youtube.com/watch?v=05CwdTRt6AM">https://www.youtube.com/watch?v=05CwdTRt6AM</a>.
</div></li>
<li><div id="ref-Kowalczyk2020" class="csl-entry" role="doc-biblioentry">
Kowalczyk, Emily, Karan Nair, Zebao Gao, Leo Silberstein, Teng Long, and
Atif Memon. 2020. <span>“Modeling and Ranking Flaky Tests at
<span>Apple</span>.”</span> In <em>Proceedings of the
<span>ACM</span>/<span>IEEE</span> 42nd <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span></em>, 110–19. <span>Seoul
South Korea</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/3377813.3381370">https://doi.org/10.1145/3377813.3381370</a>.
</div></li>
<li><div id="ref-Kucuk2021" class="csl-entry" role="doc-biblioentry">
Kucuk, Yigit, Tim A. D. Henderson, and Andy Podgurski. 2021.
<span>“Improving <span>Fault Localization</span> by <span>Integrating
Value</span> and <span>Predicate Based Causal Inference
Techniques</span>.”</span> In <em>2021
<span>IEEE</span>/<span>ACM</span> 43rd <span>International
Conference</span> on <span>Software Engineering</span>
(<span>ICSE</span>)</em>, 649–60. <span>Madrid, ES</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE43902.2021.00066">https://doi.org/10.1109/ICSE43902.2021.00066</a>.
</div></li>
<li><div id="ref-Leong2019" class="csl-entry" role="doc-biblioentry">
Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John
Micco. 2019. <span>“Assessing <span>Transition-Based Test Selection
Algorithms</span> at <span>Google</span>.”</span> In <em>2019
<span>IEEE</span>/<span>ACM</span> 41st <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span>
(<span>ICSE-SEIP</span>)</em>, 101–10. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00019">https://doi.org/10.1109/ICSE-SEIP.2019.00019</a>.
</div></li>
<li><div id="ref-Lewis2013" class="csl-entry" role="doc-biblioentry">
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, and Xiaoyan Zhu. 2013.
<span>“Does Bug Prediction Support Human Developers? Findings from a
Google Case Study.”</span> In <em>Proceedings of the …</em>, 372–81. <a
href="http://dl.acm.org/citation.cfm?id=2486838">http://dl.acm.org/citation.cfm?id=2486838</a>.
</div></li>
<li><div id="ref-Lucia2014" class="csl-entry" role="doc-biblioentry">
Lucia, David Lo, Lingxiao Jiang, Ferdian Thung, and Aditya Budi. 2014.
<span>“Extended Comprehensive Study of Association Measures for Fault
Localization.”</span> <em>Journal of Software: Evolution and
Process</em> 26 (2): 172–219. <a
href="https://doi.org/10.1002/smr.1616">https://doi.org/10.1002/smr.1616</a>.
</div></li>
<li><div id="ref-Machalica2020" class="csl-entry" role="doc-biblioentry">
Machalica, Mateusz, Wojtek Chmiel, Stanislaw Swierc, and Ruslan
Sakevych. 2020. <span>“Probabilistic <span>Flakiness</span>:
<span>How</span> Do You Test Your Tests?”</span> <em>DEVINFRA</em>. <a
href="https://engineering.fb.com/2020/12/10/developer-tools/
                  probabilistic-flakiness/">https://engineering.fb.com/2020/12/10/developer-tools/
probabilistic-flakiness/</a>.
</div></li>
<li><div id="ref-Machalica2019" class="csl-entry" role="doc-biblioentry">
Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra.
2019. <span>“Predictive <span>Test Selection</span>.”</span> In <em>2019
<span>IEEE</span>/<span>ACM</span> 41st <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice</span>
(<span>ICSE-SEIP</span>)</em>, 91–100. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00018">https://doi.org/10.1109/ICSE-SEIP.2019.00018</a>.
</div></li>
<li><div id="ref-McCrum-Gardner2008" class="csl-entry"
role="doc-biblioentry">
McCrum-Gardner, Evie. 2008. <span>“Which Is the Correct Statistical Test
to Use?”</span> <em>British Journal of Oral and Maxillofacial
Surgery</em> 46 (1): 38–41. <a
href="https://doi.org/10.1016/j.bjoms.2007.09.002">https://doi.org/10.1016/j.bjoms.2007.09.002</a>.
</div></li>
<li><div id="ref-Memon2017" class="csl-entry" role="doc-biblioentry">
Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob
Siemborski, and John Micco. 2017. <span>“Taming <span
class="nocase">Google-scale</span> Continuous Testing.”</span> In
<em>2017 <span>IEEE</span>/<span>ACM</span> 39th <span>International
Conference</span> on <span>Software Engineering</span>: <span>Software
Engineering</span> in <span>Practice Track</span>
(<span>ICSE-SEIP</span>)</em>, 233–42. <span>Piscataway, NJ, USA</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICSE-SEIP.2017.16">https://doi.org/10.1109/ICSE-SEIP.2017.16</a>.
</div></li>
<li><div id="ref-Micco2012" class="csl-entry" role="doc-biblioentry">
Micco, John. 2012. <span>“Tools for <span>Continuous Integration</span>
at <span>Google Scale</span>.”</span> Tech {{Talk}}. <span>Google
NYC</span>. <a
href="https://youtu.be/KH2_sB1A6lA">https://youtu.be/KH2_sB1A6lA</a>.
</div></li>
<li><div id="ref-Micco2013" class="csl-entry" role="doc-biblioentry">
———. 2013. <span>“Continuous <span>Integration</span> at <span>Google
Scale</span>.”</span> Lecture. <span>EclipseCon 2013</span>. <a
href="https://web.archive.org/web/20140705215747/https://
                  www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
                  03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf">https://web.archive.org/web/20140705215747/https://
www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf</a>.
</div></li>
<li><div id="ref-Musa2015" class="csl-entry" role="doc-biblioentry">
Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi
Baharom. 2015. <span>“Regression <span>Test Cases</span> Selection for
<span>Object-Oriented Programs</span> Based on <span>Affected
Statements</span>.”</span> <em>International Journal of Software
Engineering and Its Applications</em> 9 (10): 91–108. <a
href="https://doi.org/10.14257/ijseia.2015.9.10.10">https://doi.org/10.14257/ijseia.2015.9.10.10</a>.
</div></li>
<li><div id="ref-Najafi2019a" class="csl-entry" role="doc-biblioentry">
Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. <span>“Bisecting
Commits and Modeling Commit Risk During Testing.”</span> In
<em>Proceedings of the 2019 27th <span>ACM Joint Meeting</span> on
<span>European Software Engineering Conference</span> and
<span>Symposium</span> on the <span>Foundations</span> of <span>Software
Engineering</span></em>, 279–89. <span>New York, NY, USA</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3338906.3338944">https://doi.org/10.1145/3338906.3338944</a>.
</div></li>
<li><div id="ref-Ocariza2022" class="csl-entry" role="doc-biblioentry">
Ocariza, Frolin S. 2022. <span>“On the <span>Effectiveness</span> of
<span>Bisection</span> in <span>Performance Regression
Localization</span>.”</span> <em>Empirical Software Engineering</em> 27
(4): 95. <a
href="https://doi.org/10.1007/s10664-022-10152-3">https://doi.org/10.1007/s10664-022-10152-3</a>.
</div></li>
<li><div id="ref-Osman2017" class="csl-entry" role="doc-biblioentry">
Osman, Haidar, Mohammad Ghafari, Oscar Nierstrasz, and Mircea Lungu.
2017. <span>“An <span>Extensive Analysis</span> of <span>Efficient Bug
Prediction Configurations</span>.”</span> In <em>Proceedings of the 13th
<span>International Conference</span> on <span>Predictive Models</span>
and <span>Data Analytics</span> in <span>Software
Engineering</span></em>, 107–16. <span>Toronto Canada</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3127005.3127017">https://doi.org/10.1145/3127005.3127017</a>.
</div></li>
<li><div id="ref-pan2022" class="csl-entry" role="doc-biblioentry">
Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand.
2022. <span>“Test Case Selection and Prioritization Using Machine
Learning: A Systematic Literature Review.”</span> <em>Empirical Software
Engineering</em> 27 (2): 29. <a
href="https://doi.org/10.1007/s10664-021-10066-6">https://doi.org/10.1007/s10664-021-10066-6</a>.
</div></li>
<li><div id="ref-Pelc2002" class="csl-entry" role="doc-biblioentry">
Pelc, Andrzej. 2002. <span>“Searching Games with Errorsfifty Years of
Coping with Liars.”</span> <em>Theoretical Computer Science</em> 270
(1-2): 71–109. <a
href="https://doi.org/10.1016/S0304-3975(01)00303-6">https://doi.org/10.1016/S0304-3975(01)00303-6</a>.
</div></li>
<li><div id="ref-Podgurski1990" class="csl-entry" role="doc-biblioentry">
Podgurski, A, and L A Clarke. 1990. <span>“A <span>Formal Model</span>
of <span>Program Dependences</span> and <span>Its Implications</span>
for <span>Software Testing</span>, <span>Debugging</span>, and
<span>Maintenance</span>.”</span> <em>IEEE Transactions of Software
Engineering</em> 16 (9): 965–79. <a
href="https://doi.org/10.1109/32.58784">https://doi.org/10.1109/32.58784</a>.
</div></li>
<li><div id="ref-Potvin2016" class="csl-entry" role="doc-biblioentry">
Potvin, Rachel, and Josh Levenberg. 2016. <span>“Why Google Stores
Billions of Lines of Code in a Single Repository.”</span>
<em>Communications of the ACM</em> 59 (7): 78–87. <a
href="https://doi.org/10.1145/2854146">https://doi.org/10.1145/2854146</a>.
</div></li>
<li><div id="ref-Punitha2013" class="csl-entry" role="doc-biblioentry">
Punitha, K., and S. Chitra. 2013. <span>“Software Defect Prediction
Using Software Metrics - <span>A</span> Survey.”</span> In <em>2013
<span>International Conference</span> on <span>Information
Communication</span> and <span>Embedded Systems</span>
(<span>ICICES</span>)</em>, 555–58. <span>Chennai</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/ICICES.2013.6508369">https://doi.org/10.1109/ICICES.2013.6508369</a>.
</div></li>
<li><div id="ref-Ren2004" class="csl-entry" role="doc-biblioentry">
Ren, Xiaoxia, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia
Chesley. 2004. <span>“Chianti: A Tool for Change Impact Analysis of Java
Programs.”</span> In <em>Proceedings of the 19th Annual <span>ACM
SIGPLAN</span> Conference on <span class="nocase">Object-oriented</span>
Programming, Systems, Languages, and Applications</em>, 432–48.
<span>Vancouver BC Canada</span>: <span>ACM</span>. <a
href="https://doi.org/10.1145/1028976.1029012">https://doi.org/10.1145/1028976.1029012</a>.
</div></li>
<li><div id="ref-Rivest1978" class="csl-entry" role="doc-biblioentry">
Rivest, R. L., A. R. Meyer, and D. J. Kleitman. 1978. <span>“Coping with
Errors in Binary Search Procedures (<span>Preliminary
Report</span>).”</span> In <em>Proceedings of the Tenth Annual
<span>ACM</span> Symposium on <span>Theory</span> of Computing -
<span>STOC</span> ’78</em>, 227–32. <span>San Diego, California, United
States</span>: <span>ACM Press</span>. <a
href="https://doi.org/10.1145/800133.804351">https://doi.org/10.1145/800133.804351</a>.
</div></li>
<li><div id="ref-Rodriguez-Perez2018" class="csl-entry"
role="doc-biblioentry">
Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona.
2018. <span>“Reproducibility and Credibility in Empirical Software
Engineering: <span>A</span> Case Study Based on a Systematic Literature
Review of the Use of the <span>SZZ</span> Algorithm.”</span>
<em>Information and Software Technology</em> 99 (July): 164–76. <a
href="https://doi.org/10.1016/j.infsof.2018.03.009">https://doi.org/10.1016/j.infsof.2018.03.009</a>.
</div></li>
<li><div id="ref-Saha2017" class="csl-entry" role="doc-biblioentry">
Saha, Ripon, and Milos Gligoric. 2017. <span>“Selective <span>Bisection
Debugging</span>.”</span> In <em>Fundamental <span>Approaches</span> to
<span>Software Engineering</span></em>, edited by Marieke Huisman and
Julia Rubin, 10202:60–77. <span>Berlin, Heidelberg</span>:
<span>Springer Berlin Heidelberg</span>. <a
href="https://doi.org/10.1007/978-3-662-54494-5_4">https://doi.org/10.1007/978-3-662-54494-5_4</a>.
</div></li>
<li><div id="ref-Sliwerski2005" class="csl-entry" role="doc-biblioentry">
Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005.
<span>“When Do Changes Induce Fixes?”</span> <em>ACM SIGSOFT Software
Engineering Notes</em> 30 (4): 1. <a
href="https://doi.org/10.1145/1082983.1083147">https://doi.org/10.1145/1082983.1083147</a>.
</div></li>
<li><div id="ref-Sun2016" class="csl-entry" role="doc-biblioentry">
Sun, Shih-Feng, and Andy Podgurski. 2016. <span>“Properties of
<span>Effective Metrics</span> for <span>Coverage-Based Statistical
Fault Localization</span>.”</span> In <em>2016 <span>IEEE International
Conference</span> on <span>Software Testing</span>,
<span>Verification</span> and <span>Validation</span>
(<span>ICST</span>)</em>, 124–34. <span>IEEE</span>. <a
href="https://doi.org/10.1109/ICST.2016.31">https://doi.org/10.1109/ICST.2016.31</a>.
</div></li>
<li><div id="ref-Tharwat2021" class="csl-entry" role="doc-biblioentry">
Tharwat, Alaa. 2021. <span>“Classification Assessment Methods.”</span>
<em>Applied Computing and Informatics</em> 17 (1): 168–92. <a
href="https://doi.org/10.1016/j.aci.2018.08.003">https://doi.org/10.1016/j.aci.2018.08.003</a>.
</div></li>
<li><div id="ref-Tip1995" class="csl-entry" role="doc-biblioentry">
Tip, Frank. 1995. <span>“A Survey of Program Slicing Techniques.”</span>
<em>Journal of Programming Languages</em> 3 (3): 121–89.
</div></li>
<li><div id="ref-Waeber2013" class="csl-entry" role="doc-biblioentry">
Waeber, Rolf, Peter I. Frazier, and Shane G. Henderson. 2013.
<span>“Bisection <span>Search</span> with <span>Noisy
Responses</span>.”</span> <em>SIAM Journal on Control and
Optimization</em> 51 (3): 2261–79. <a
href="https://doi.org/10.1137/120861898">https://doi.org/10.1137/120861898</a>.
</div></li>
<li><div id="ref-Walpole2007" class="csl-entry" role="doc-biblioentry">
Walpole, Ronald E., ed. 2007. <em>Probability &amp; Statistics for
Engineers &amp; Scientists</em>. 8th ed. <span>Upper Saddle River,
NJ</span>: <span>Pearson Prentice Hall</span>.
</div></li>
<li><div id="ref-Wang2020" class="csl-entry" role="doc-biblioentry">
Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and
Daniel Rall. 2020. <span>“Scalable Build Service System with Smart
Scheduling Service.”</span> In <em>Proceedings of the 29th <span>ACM
SIGSOFT International Symposium</span> on <span>Software Testing</span>
and <span>Analysis</span></em>, 452–62. <span>Virtual Event USA</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3395363.3397371">https://doi.org/10.1145/3395363.3397371</a>.
</div></li>
<li><div id="ref-Wen2021" class="csl-entry" role="doc-biblioentry">
Wen, Ming, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao, Shi Han,
and Shing-Chi Cheung. 2021. <span>“Historical <span>Spectrum Based Fault
Localization</span>.”</span> <em>IEEE Transactions on Software
Engineering</em> 47 (11): 2348–68. <a
href="https://doi.org/10.1109/TSE.2019.2948158">https://doi.org/10.1109/TSE.2019.2948158</a>.
</div></li>
<li><div id="ref-Wen2019" class="csl-entry" role="doc-biblioentry">
Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi
Cheung, and Zhendong Su. 2019. <span>“Exploring and Exploiting the
Correlations Between Bug-Inducing and Bug-Fixing Commits.”</span> In
<em>Proceedings of the 2019 27th <span>ACM Joint Meeting</span> on
<span>European Software Engineering Conference</span> and
<span>Symposium</span> on the <span>Foundations</span> of <span>Software
Engineering</span></em>, 326–37. <span>Tallinn Estonia</span>:
<span>ACM</span>. <a
href="https://doi.org/10.1145/3338906.3338962">https://doi.org/10.1145/3338906.3338962</a>.
</div></li>
<li><div id="ref-Wong2016" class="csl-entry" role="doc-biblioentry">
Wong, W. Eric, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016.
<span>“A <span>Survey</span> on <span>Software Fault
Localization</span>.”</span> <em>IEEE Transactions on Software
Engineering</em> 42 (8): 707–40. <a
href="https://doi.org/10.1109/TSE.2016.2521368">https://doi.org/10.1109/TSE.2016.2521368</a>.
</div></li>
<li><div id="ref-Youm2015" class="csl-entry" role="doc-biblioentry">
Youm, Klaus Changsun, June Ahn, Jeongho Kim, and Eunseok Lee. 2015.
<span>“Bug <span>Localization Based</span> on <span>Code Change
Histories</span> and <span>Bug Reports</span>.”</span> In <em>2015
<span>Asia-Pacific Software Engineering Conference</span>
(<span>APSEC</span>)</em>, 190–97. <span>New Delhi</span>:
<span>IEEE</span>. <a
href="https://doi.org/10.1109/APSEC.2015.23">https://doi.org/10.1109/APSEC.2015.23</a>.
</div></li>
<li><div id="ref-Zhou2012" class="csl-entry" role="doc-biblioentry">
Zhou, Jian, Hongyu Zhang, and David Lo. 2012. <span>“Where Should the
Bugs Be Fixed? <span>More</span> Accurate Information Retrieval-Based
Bug Localization Based on Bug Reports.”</span> <em>Proceedings -
International Conference on Software Engineering</em>, 14–24. <a
href="https://doi.org/10.1109/ICSE.2012.6227210">https://doi.org/10.1109/ICSE.2012.6227210</a>.
</div></li>
<li><div id="ref-Zhou2010" class="csl-entry" role="doc-biblioentry">
Zhou, Zhi Quan. 2010. <span>“Using Coverage Information to Guide Test
Case Selection in <span>Adaptive Random Testing</span>.”</span>
<em>Proceedings - International Computer Software and Applications
Conference</em>, 208–13. <a
href="https://doi.org/10.1109/COMPSACW.2010.43">https://doi.org/10.1109/COMPSACW.2010.43</a>.
</div></li>
<li><div id="ref-Ziftci2013a" class="csl-entry" role="doc-biblioentry">
Ziftci, Celal, and Vivek Ramavajjala. 2013. <span>“Finding
<span>Culprits Automatically</span> in <span>Failing Builds</span> -
i.e. <span>Who Broke</span> the <span>Build</span>?”</span> <a
href="https://www.youtube.com/watch?v=SZLuBYlq3OM">https://www.youtube.com/watch?v=SZLuBYlq3OM</a>.
</div></li>
<li><div id="ref-Ziftci2017" class="csl-entry" role="doc-biblioentry">
Ziftci, Celal, and Jim Reardon. 2017. <span>“Who Broke the Build?
<span>Automatically</span> Identifying Changes That Induce Test Failures
in Continuous Integration at Google Scale.”</span> <em>Proceedings -
2017 IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track, ICSE-SEIP 2017</em>, 113–22. <a
href="https://doi.org/10.1109/ICSE-SEIP.2017.13">https://doi.org/10.1109/ICSE-SEIP.2017.13</a>.
</div></li>
</ol>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>In practice the restriction on
network usage on TAP is somewhat loose. There are legacy tests that have
been tagged to allow them to utilize the network and there is an on
going effort to fully eliminate network usage on TAP.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Other CI platforms exist for
integration tests that span multiple machines, use large amounts of RAM,
or take a very long time to run. The algorithm presented in this paper
has also been implemented for one of those CI systems but we will not
present an empirical evaluation of its performance in that environment
in this paper. In general, Culprit Finding is much more difficult in
such environments as tests may use a mixture of code from different
versions and network resources may be shared across tests.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>An internal version of Bazel <a
href="https://bazel.build/" class="uri">https://bazel.build/</a>.<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>The exact number of hours isn’t so
important. The main thing is to repeat the verification at a suitable
interval from the original attempt.<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a
href="https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87"
class="uri">https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87</a><a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><a
href="https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py"
class="uri">https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py</a>,
<a href="https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28"
class="uri">https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28</a><a
href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p><a
href="https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README"
class="uri">https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README</a><a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><a
href="https://github.com/Ealdwulf/bbchop"
class="uri">https://github.com/Ealdwulf/bbchop</a><a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

