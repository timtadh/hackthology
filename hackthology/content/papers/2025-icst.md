Title: Speculative Testing at Google with Transition Prediction
Author: Avi Kondareddy, Sushmita Azad, Abhayendra Singh, and <a href="http://hackthology.com">Tim Henderson</a>
Citation: Avi Kondareddy, Sushmita Azad, Abhayendra Singh, and <strong>Tim A. D. Henderson</strong> <i>Speculative Testing at Google with Transition Prediction</i>. ICST Industry Track 2025.
CiteAuthor: Avi Kondareddy, Sushmita Azad, Abhayendra Singh, and <strong>Tim A. D. Henderson</strong>
Publication: ICST Industry Track 2025
Date: 2025-04-02
Category: Paper
MainPage: show

Avi Kondareddy, Sushmita Azad, Abhayendra Singh, and **Tim A. D. Henderson**.
*Speculative Testing at Google with Transition Prediction*.  [ICST Industry Track 2025](https://conf.researchr.org/details/icst-2025/icst-2025-industry/17/Speculative-Testing-at-Google-with-Transition-Prediction).
<br/>
[DOI](http://tba).
[PDF]({static}/pdfs/icst-2025.pdf).
[WEB]({filename}/papers/2025-icst.md).

<h4>Note</h4>
> This is a conversion from a latex paper I wrote. If you want all formatting
> correct you should read the
> [pdf version]({static}/pdfs/icst-2025.pdf).

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

#### Abstract

Google's approach to testing includes both testing prior to code submission
(for fast validation) and after code submission (for comprehensive
validation). However, Google's ever growing testing demand has lead to
increased continuous integration cycle latency and machine costs. When the
post code submission continuous integration cycles get longer, it delays
detecting breakages in the main repository which increases developer friction
and lowers productivity. To mitigate this without increasing resource demand,
Google is implementing Postsubmit Speculative Cycles in their Test Automation
Platform (TAP). Speculative Cycles prioritize finding novel breakages faster.
In this paper we present our new test scheduling architecture and the machine
learning system (Transition Prediction) driving it. Both the ML system and the
end-to-end test scheduling system are empirically evaluated on 3-months of our
production data (120 billion test$\times$cycle pairs, 7.7 million breaking
targets, with $\sim$20 thousand unique breakages). Using Speculative Cycles we
observed a median (p50) reduction of approximately 65\% (from 107 to 37 
minutes) in the time taken to detect novel breaking targets.

  <h1 id="sec:introduction">Introduction</h1>
  <p>Continuous integration (CI) coordinates the development activity of large numbers of
  developers <span class="citation" data-cites="Fowler2006">(<a href="#ref-Fowler2006" role=
  "doc-biblioref">Fowler 2006</a>)</span>. When two developers are working in the same portion of
  the code base, continuous integration ensures that conflicting changes combine into a
  conflict-free version before reaching the end user. In general, adopters of CI execute builds and
  tests to ensure that the final release artifact (server binary, mobile application, etc...)
  passes all relevant tests. These build and tests are executed frequently to reduce the amount of
  time conflicts are present in the code base. Continuous integration is both a system and a
  practice of automatically merging changes into a source of truth for an organization's source
  code and related artifacts.</p>
  <p>For small software repositories and organizations, the implementation of continuous
  integration is well supported by off-the-shelf software such as Github Actions, CircleCI,
  Jenkins, and numerous other tools. However, as the repository and organization scales up
  challenges emerge.</p>
  <p>As an organization adds projects and engineers, there are two distinct paths that emerge: the
  many-small-repository path and the single-mono-repository path. Both paths have distinct
  challenges and advantages. Neither path will enable organizations to use vanilla off-the-shelf
  solutions for their development environment. This paper is focused on a particular challenge
  faced in a mono-repository environment at Google, but similar problems arise in organizations
  with many small repositories that are strongly coupled together.</p>
  <p>Google, an early adopter of mono-repositories and continuous integration <span class=
  "citation" data-cites="Potvin2016">(<a href="#ref-Potvin2016" role="doc-biblioref">Potvin and
  Levenberg 2016</a>)</span>, faces an enormous ever expanding code base that is centrally tested
  by the Test Automation Platform (TAP) <span class="citation" data-cites="Micco2012">(<a href=
  "#ref-Micco2012" role="doc-biblioref">Micco 2012</a>)</span>. As the code base and company grows,
  so does the <em>demand</em> for compute resources for continuous testing. Left unchecked, we have
  observed a double digit percentage organic demand growth rate year-over-year (compounding). Such
  a growth rate is untenable even for a large company like Google. To prevent this unchecked growth
  in resource demand, TAP has long had features that reduce demand by skipping some tests (before
  submission) and batching many versions together (after submission) to amortize the cost
  <span class="citation" data-cites=
  "Gupta2011 Bland2012 Micco2012 Micco2013 Memon2017 Leong2019 Wang2020 Wang2021 Henderson2023 Henderson2024 Hoang2024">
  (<a href="#ref-Gupta2011" role="doc-biblioref">Gupta, Ivey, and Penix 2011</a>; <a href=
  "#ref-Bland2012" role="doc-biblioref">Bland 2012</a>; <a href="#ref-Micco2012" role=
  "doc-biblioref">Micco 2012</a>, <a href="#ref-Micco2013" role="doc-biblioref">2013</a>; <a href=
  "#ref-Memon2017" role="doc-biblioref">Memon et al. 2017</a>; <a href="#ref-Leong2019" role=
  "doc-biblioref">Leong et al. 2019</a>; <a href="#ref-Wang2020" role="doc-biblioref">K. Wang et
  al. 2020</a>, <a href="#ref-Wang2021" role="doc-biblioref">2021</a>; <a href="#ref-Henderson2023"
  role="doc-biblioref">Henderson et al. 2023</a>, <a href="#ref-Henderson2024" role=
  "doc-biblioref">2024</a>; <a href="#ref-Hoang2024" role="doc-biblioref">Hoang and Berding
  2024</a>)</span>.</p>
  <p><em>In this paper, we are primarily concerned with testing that occurs after the code has been
  submitted. We examine how to improve the developer experience by finding novel build/test
  breakages faster while continuing to control our resource footprint growth rate.</em></p>
  <h2 id="the-google-development-environment">The Google Development Environment</h2>
  <p>Google's development environment uses a centralized repository <span class="citation"
  data-cites="Potvin2016">(<a href="#ref-Potvin2016" role="doc-biblioref">Potvin and Levenberg
  2016</a>)</span> where code changes are managed with Blaze/Bazel<a href="#fn1" class=
  "footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and automatically tested,
  primarily by TAP. Developers write code, build and test locally, then submit it for review via
  Critique <span class="citation" data-cites="Sadowski2018">(<a href="#ref-Sadowski2018" role=
  "doc-biblioref">Sadowski et al. 2018</a>)</span>, triggering Presubmit checks. After review and
  approval, changes undergo further checks before merging. Due to high submission rates, Presubmit
  testing is limited to avoid excessive resource consumption and delays.</p>
  <h2 id="tap-postsubmit">TAP Postsubmit</h2>
  <p>When a developer's change gets submitted it may still have a bug that breaks an existing test
  or causes a compile breakage in another part of the repository. To find these bugs that have
  slipped through the pre-submission testing and validation process, TAP also has a
  "post-submission" mode (TAP Postsubmit).</p>
  <p>In Postsubmit, TAP periodically (subject to compute resource availability in the Build system)
  schedules <em>all</em> tests that have been <em>affected</em> (based on build dependencies) since
  the last run. We refer this execution cycle as <em>Comprehensive Testing Cycle</em>. Previously,
  this cycle has also been referred as Milestones <span class="citation" data-cites=
  "Memon2017">(<a href="#ref-Memon2017" role="doc-biblioref">Memon et al. 2017</a>)</span>. There
  are two primary objectives for the Comprehensive Cycle:</p>
  <ol>
    <li>
      <p>Uncover all test failures at the version comprehensive testing was conducted at.</p>
    </li>
    <li>
      <p>Provide project health signals for downstream services to trigger more expensive testing
      and/or start production rollout by triggering release automation</p>
    </li>
  </ol>
  <p>As of today's writing, it takes <span class="math inline">∼</span>1-2 hours for a test broken
  by a developer's change to start failing in TAP Postsubmit. Once the failure is detected
  automatic culprit finders <span class="citation" data-cites="Henderson2023">(<a href=
  "#ref-Henderson2023" role="doc-biblioref">Henderson et al. 2023</a>)</span> and rollback system
  <span class="citation" data-cites="Henderson2024">(<a href="#ref-Henderson2024" role=
  "doc-biblioref">Henderson et al. 2024</a>)</span> will spring into action to help keep the
  repository healthy and our developers productive. Automatically rolling back the change (after
  the first breakage was detected) might take as little as 30 minutes or as long as several hours
  depending on a number of factors. Thus, it could be 4 hours or more before our developer learns
  they have broken a test and either been notified or automatically had their change rolled
  back.</p>
  <h2 id="cost-of-breakages">Cost of Breakages</h2>
  <p>The longer it takes to identify and fix a code breakage, the more challenging and expensive
  the remediation process becomes. This delay can lead to a loss of context for the original
  developer, potentially requiring teammates to resolve the issue. Additionally, a bad change can
  affect other developers, especially if it occurs in widely used parts of the code base. When
  interrupted by such a failure programmers need to distinguish between the change they are making
  and existing fault in the repository. Ultimately, prolonged breakages can disrupt release
  automation and even delay production releases.</p>
  <h2 id="our-contributions-and-findings">Our Contributions and Findings</h2>
  <p>In this paper we are looking to increase developer productivity by reducing their friction
  with respect to broken code in the main code base. Specifically we are solving the following
  problem.</p>
  <p><strong>How can CI minimize the time between the submission of a faulty change and its
  identification within a continuous integration (CI) system to accelerate the detection and
  mitigation of bugs?</strong></p>
  <p>To solve the problem, we propose a new scheduling mode to our CI system, Speculative Testing
  Cycles, that opportunistically runs a small subset of tests to uncover novel failures and reduce
  the mean time to detection (MTTD) for new failures. Speculative Cycle prioritizes finding novel
  breakages by identifying tests that are very likely to fail. The new scheduling mode is driven by
  a <em>Transition Prediction</em> model that predicts when tests are likely to (newly) fail. It
  utilizes shallow machine-learning to run a smaller batch of tests predicted to be more likely to
  be broken at higher frequency in order to find breakages faster. We discuss both the design and
  constraints on the production Speculative Cycle system and the shallow machine-learning model
  (Transition Prediction) that powers this predictive test selection.</p>
  <p>While the production system is highly tied to Google's unique development environment and
  organizational constraints (i.e.: large monorepos with centralized CI infrastructure), our ML
  model presents a very coarse-grain approach to prediction for test selection using test and
  code-under-test metadata that has equivalents in most development environments and is
  language/framework agnostic.</p>
  <p>Our contributions in this paper are therefore:</p>
  <ol>
    <li>
      <p>Speculative Cycles: A system for frequent / cost-aware batch testing.</p>
    </li>
    <li>
      <p>Transition Prediction (TRANSPRED): Predicting target status transitions (Pass to Fail) for
      arbitrary build &amp; test targets. With a budget of 25% of the total targets, Transition
      Prediction achieves a 85% recall rate in detecting breakages.</p>
    </li>
    <li>
      <p>A very large scale study (utilizing 3-months of production data: 120 billion
      test<span class="math inline">×</span>cycle pairs, 7.7 million breaking targets, and
      <span class="math inline">∼</span>20,000 unique breakages) on the efficacy of Speculative
      Testing using Transition Prediction evaluating its performance against randomized
      testing.</p>
    </li>
    <li>
      <p>A comparative assessment of dataset and training configurations for Transition Prediction,
      determining that the selection of features and the length of the training window
      substantially influence the system's performance.</p>
    </li>
  </ol>
  <h1 id="background">Background</h1>
  <h2 id="developer-services">Developer Services</h2>
  <h3 id="bazel">Bazel</h3>
  <p>Google uses Bazel as its build system monorepo wide. Bazel allows specifying build/test
  "targets" that depend on other targets. Each target is comprised of a set of "actions" to be
  executed. Therefore, the execution of a target corresponds to a Directed Acyclic Graph (DAG) of
  actions. Given Bazel makes dependency management declarative and all dependencies are built at
  the same version of the codebase, builds are ostensibly held to be hermetic - builds at the same
  version of the codebase should produce the same output for each action. This opens up the
  possibility for massive compute savings through caching of intermediate actions across multiple
  builds.</p>
  <h3 id="forge">Forge</h3>
  <p>Build/Test actions at Google run under a centralized compute cluster called Forge which
  attempts to cache these intermediate actions to avoid recomputation at the same version of the
  codebase. For Postsubmit testing, we make use of this caching by batching builds/tests at single
  versions every hour or so.</p>
  <h2 id="comprehensive-testing-cycles">Comprehensive Testing Cycles</h2>
  <p>In the standard case, TAP Postsubmit performs each Comprehensive Testing Cycle once the
  previous cycle's active consumption of resources drops below a threshold of their allocated build
  system resources. This cycle consists of picking a recent change(snapshot of the repository), at
  which we batch and run a "comprehensive" set of tests. Comprehensive cycles run all tests that
  are "affected" since the previous cycle. "Affected" is our term for dependence as implied by the
  build system's package graph, allowing us to perform a degree of static test selection at a
  coarse granularity.</p>
  <h2 id="ancillary-systems-for-build-gardening">Ancillary Systems For Build Gardening</h2>
  <p>The development life cycle explained above relies on Postsubmit testing to make up for the
  lack of comprehensive testing prior to submission. This introduces the concept of <em>Postsubmit
  breakages</em> which are caught sometime after submission. The process of detecting the breakage,
  identifying the cause of the breakage (culprit finding), and fixing the breakage (sometimes via a
  rollback) is called "Build Gardening" at Google.</p>
  <h3 id="culprit-finding">Culprit Finding</h3>
  <p>We have previously outlined Google's culprit finding approach in <span class="citation"
  data-cites="Henderson2023 Henderson2024">(<a href="#ref-Henderson2023" role=
  "doc-biblioref">Henderson et al. 2023</a>, <a href="#ref-Henderson2024" role=
  "doc-biblioref">2024</a>)</span>. To formulate the problem, we are given a test/build "target"
  and must determine at which change <span class="math inline"><em>C</em></span> over a range of
  sequential changes <span class="math inline">[<em>A</em>, <em>B</em>]</span> did the target start
  breaking. The change <em>C</em> is referred as a <em>Culprit Change</em> or simply
  <em>Culprit</em>. Given the presence of build/test non-determinism ("flakes"), we want to find
  the change at which the test was "truly" failing but "truly" passing at the previous change. Our
  previous work explores how we do this in a time and run-efficient manner using a historical flake
  rate aware Bayesian model to sample runs.</p>
  <h3 id="sec:verifier">Culprit Verifier</h3>
  <p>Unfortunately, persistent flakiness and non-determinism can still slip by the culprit finders.
  The pathological case is that of build system non-determinism. Google's build system Bazel
  assumes that builds are hermetic and should produce the same output at the same version. This
  assumption means that unlike tests, failed actions are cached by the build system so immediate
  reruns will continue to assert what could potentially be a flaky build failure. The culprit
  verifier does extensive reruns on a subset of culprit conclusions produced by the culprit
  finders. We sample from two different sampling frames: (1) a simple random sample of all
  conclusions produced and (2) the first conclusion produced for each uniquely blamed culprit. For
  details on the design of the verification system, see the Flake Aware Culprit Finding paper
  <span class="citation" data-cites="Henderson2023">(<a href="#ref-Henderson2023" role=
  "doc-biblioref">Henderson et al. 2023</a>)</span> and the SafeRevert paper <span class="citation"
  data-cites="Henderson2024">(<a href="#ref-Henderson2024" role="doc-biblioref">Henderson et al.
  2024</a>)</span>.</p>
  <p>Although the verifier is fallible, the dataset it produces is still the most accurate
  representation of true breakages at Google. We know it fails because we have seen specific
  examples of its failures. But, from a measurement perspective, bounding its accuracy is
  challenging because its failure rate is low enough that it exceeds our ability to accurately
  measure. We have recently conducted manual verification while launching a new version of our Auto
  Rollback system on all culprits rolled back during the "dogfood" phase (an opt-in beta). Out of
  250+ hand verified culprits: 5 were incorrect culprits from the culprit finder, 2 incorrect
  culprits were incorrectly verified correct by the verifier. This gives us an estimated culprit
  finding accuracy of 98% (matching verifier produced accuracy dashboard) and a verifier accuracy
  of 99.2%. Given the low sample size the confidence interval on that measurement is <span class=
  "math inline">99.2 ± 3.3%</span> which is too wide. To get sufficient power to accurately
  estimate the verifier accuracy, we would need to hand verify at least 5000 culprits.</p>
  <p>Thus, while we are confident in the overall accuracy measurements provided by the verifier, we
  acknowledge that "ground truth" is difficult to obtain. We utilize the dataset produced by the
  verifier to label the culprits to train our prediction model (detailed below). This approach
  vastly reduces the impact of flakiness on our model training data.</p>
  <h3 id="sec:autorollback">Autorollback</h3>
  <p>At Google, there is a developer guideline to prefer rollbacks to fix forwards. Given this
  context, it is appropriate to automatically roll back a change if someone is confident that the
  change broke the build. Unfortunately, given the accuracy issue of the culprit finder described
  above, we can't immediately rollback upon a culprit finding completion for one target. In the
  SafeRevert paper <span class="citation" data-cites="Henderson2024">(<a href="#ref-Henderson2024"
  role="doc-biblioref">Henderson et al. 2024</a>)</span>, we discussed how we attempt to determine
  the amount of evidence needed to proceed with a rollback. In practice, we currently restrict
  rollbacks to changes that break at least 10 targets.</p>
  <h2 id="the-evolution-of-tap">The Evolution of TAP</h2>
  <p>Prior to development of TAP, Google had a more traditional continuous integration system.
  Primarily, the decision to run continuous integration was left up to individual teams and the
  system that existed was federated: teams brought their own capacity (machines, possibly under
  their desks) and had to configure and run the CI system(s). TAP, upon launch in 2009, was a
  massive improvement from setup and continuous maintenance perspective alone. Unlike the previous
  systems, TAP ran all builds centrally and only required a small amount of configuration: it
  simply required users to specify which paths they would like to test and where the (failure)
  notifications should go.</p>
  <p>The central builds and low configuration overhead made TAP immediately successful. Within a
  few years it had completely displaced the prior system. But, success came at a price: it was
  plagued by build capacity limitations and scalability challenges stemming from design decisions
  of running tests on each change in the initial implementation. By 2012 (following a full launch
  in 2010) Google was running low on machine capacity to run TAP. The testing model had naively
  assumed that the build dependence based test selection would be enough to control demand; It was
  not.</p>
  <p>TAP both pre- and post-submit were thus rewritten (multiple times). TAP Presubmit now has
  several different modes, adapts to resource availability, uses machine learning driven test
  selection, and has "advisors" which can ignore or rerun tests on failure <span class="citation"
  data-cites="Hoang2024">(<a href="#ref-Hoang2024" role="doc-biblioref">Hoang and Berding
  2024</a>)</span>. Unfortunately for you (the reader) discussing these innovations is out of scope
  for this manuscript.</p>
  <p>Today, TAP Postsubmit no longer runs on every change. It waits for there to be capacity in the
  Build System <span class="citation" data-cites="Wang2020 Wang2021">(<a href="#ref-Wang2020" role=
  "doc-biblioref">K. Wang et al. 2020</a>, <a href="#ref-Wang2021" role=
  "doc-biblioref">2021</a>)</span> and then enqueues all tests that have been affected since their
  last definitive status. At Google's scale and growth rate, we have had to continuously innovate
  and rewrite core parts of TAP just to maintain this model. However, we have long known that just
  using build-graph dependency based test selection in Postsubmit was untenable in the long term
  <span class="citation" data-cites="Memon2017 Leong2019">(<a href="#ref-Memon2017" role=
  "doc-biblioref">Memon et al. 2017</a>; <a href="#ref-Leong2019" role="doc-biblioref">Leong et al.
  2019</a>)</span>. It is only now that we have both the high-accuracy culprit finding
  infrastructure <span class="citation" data-cites="Henderson2023 Henderson2024">(<a href=
  "#ref-Henderson2023" role="doc-biblioref">Henderson et al. 2023</a>, <a href="#ref-Henderson2024"
  role="doc-biblioref">2024</a>)</span> and the organizational will to fundamentally change (again)
  core assumptions for how TAP works.</p>
  <h2 id="the-current-dilemma">The Current Dilemma</h2>
  <p>The cost of TAP Postsubmit's <em>comprehensive</em> testing continues to rise due to
  ever-increasing compute demand, increasing fleet machine diversity, and evolving development
  practices. This means that with no systemic changes in our approach towards Postsubmit
  testing:</p>
  <ol>
    <li>
      <p>Teams start needing to wait longer and longer for signal on their projects' health (or
      "greenness") from TAP blocking their releases which has significant monetary impact</p>
    </li>
    <li>
      <p>Developers are now alerted hours after they submit of a breaking change, creating
      developer toil</p>
    </li>
  </ol>
  <h1 id="speculative-testing-cycles">Speculative Testing Cycles</h1>
  <p>In order to protect release quality and improve developer productivity, TAP is introducing
  <em>Speculative Testing Cycles</em> - a process by which we non-deterministically select a
  smaller subset of targets that we deem more likely to fail, and run this smaller set more
  frequently than traditional comprehensive cycles. For traditional comprehensive test cycles, we
  attempt to batch as many builds/tests together at the same change as possible to get the caching
  benefits from Forge. We do the same here for Speculative Cycles, picking a single change
  approximately every 20 minutes, Forge build resources allowing. As with Comprehensive Cycles, we
  first perform static build system level dependency analysis to find all targets which could have
  a status change.</p>
  <h2 id="determining-breakage-likelihood">Determining Breakage Likelihood</h2>
  <p>We now have a set of statically-filtered targets <span class="math inline">{<em>T</em>}</span>
  and a sequence of commits <span class="math inline">{<em>C</em>}</span> consisting of all commits
  since the last Speculative Cycle. Our problem is then to determine for a given target
  <span class="math inline"><em>t</em> ∈ <em>T</em></span>: what is the likelihood it was broken by
  a change <span class="math inline"><em>c</em> ∈ <em>C</em></span> affecting it? For compute
  tractability, given the frequency of change submissions and number of targets in our codebase, we
  simplify by choosing to aggregate across C and ask: "Was target <span class=
  "math inline"><em>t</em></span> broken by any change in <span class=
  "math inline"><em>C</em></span>?" We identify this problem formulation and approaches to answer
  it as <em>Transition Prediction</em>, and discuss it in detail in the next section.</p>
  <h2 id="making-scheduling-decisions">Making Scheduling Decisions</h2>
  <p>Our Transition Prediction model gives us "scores" from 0 to 1 indicating the propensity of
  this target to be newly broken. Given that we rely on machine learning models for these
  predictions, we must be careful to note that these scores do not meaningfully represent
  probabilities. They are the product of an algorithm attempting to minimize a loss function
  relative to its training dataset. While they can be colloquially understood to imply likelihood
  and allow relative comparison, moving the threshold at which scores map onto decisions to (not)
  schedule will produce non-linear changes to the actual observed probability of finding a breakage
  ("recall") or scheduling false positives ("false positive rate").</p>
  <p>Given this fact and our desire to have stable execution cost/latency under our scheduling
  scheme, we choose to primarily rank targets and schedule the top-k highest ranking targets under
  some top K parameter. For the purpose of this paper, this is our main parameter for tuning the
  Speculative Cycle system while the remaining knobs exist in feature selection and training
  configuration for the Transition Prediction ML model.</p>
  <h2 id="goal-of-finding-test-breakages">Goal of Finding Test Breakages</h2>
  <p>As we've discussed, Postsubmit testing feeds into both Release pipelines and the breakage
  triage/fixing workflow (what we call "Build Gardening"). Release implications of Speculative
  Cycle(s) depend on many several other projects at play which are outside the scope of this paper.
  The direct outcome is that newly detected build/test failures will now propagate to Build
  Gardening consumers faster, namely:</p>
  <ol>
    <li>
      <p>Developers directly through emails sent upon detection of a failure</p>
    </li>
    <li>
      <p>Automated Culprit Finders that attempt to identify the specific culprit change</p>
    </li>
    <li>
      <p>Auto-Rollback reverts changes once it acquires enough evidence from culprit finders</p>
    </li>
  </ol>

  <span id="fig:culprit-prop" label="fig:culprit-prop">
    [<em><strong>Figure 1</strong></em>
    ![Figure 1](images/icst-2025/fig1.png)](images/icst-2025/fig1.png)
  </span>

  <p>A developer or the automated culprit finders may only need a single target breakage (We will
  denote this criterion as <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(1)</span>)
  to consider investigating and to identify a breaking change that needs to be reverted. But given
  the non-determinism issues discussed above, auto-rollback requires a stronger signal - at least
  <span class="math inline"><em>N</em></span> distinct targets were culprit-found to the same
  change (<span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(<em>N</em>)</span>).
  Both these goals are important to the health of the codebase, and while the former is a simple
  and more intuitive metric, there is a strong argument to be made for the latter - in that the
  number of distinct broken targets is a good proxy for the cost of that bad commit. The larger the
  number of broken targets, the larger the likelihood that it falls under a configured Presubmit
  testing directory, impacting developer productivity like we discussed above. Targets could belong
  to multiple release-level project groups, and more targets being broken is a higher cost to
  overall release health with more automation and production pushes being delayed or blocked.
  Figure <a href="#fig:culprit-prop" data-reference-type="ref" data-reference=
  "fig:culprit-prop">[fig:culprit-prop]</a> presents the total proportion of culprits remaining as
  we increase <span class="math inline"><em>N</em></span> which follows a power law
  distribution.</p>
  <h2 id="evaluating-performance">Evaluating Performance</h2>
  <p>Culprit detection is then split between Comprehensive and Speculative Cycles. Under a specific
  scheduling scheme, Speculative Cycles successfully detect some proportion of culprits before
  Comprehensive Cycles. This proportion is the recall of the system. This recall in combination
  with the actual run frequency and execution runtime of the two cycles determines the actual
  reduction in culprit detection latency, where latency is defined relative to the submission time
  of the culprit change. Depending on the consumer, latency improvements will have non-linear
  marginal payoffs. For example, reduction from half a day to an hour may have measurable impacts
  on development friction caused by failing tests, but reduction from a half hour to 10 minutes may
  be negligible. Therefore, both metrics can be compared to the cost of a specific scheme in order
  to justify value / viability of the system.</p>
  <h1 id="transition-prediction">Transition Prediction</h1>
  <p>Transition Prediction (or <em>TRANSPRED</em> for short) attempts to predict transition
  likelihood every 20 minutes for millions of targets with a submission rate of over tens of
  thousands of changes per day. In order to make this tractable and most importantly
  cost-beneficial relative to simply running more tests, our predictions must be cheap and low
  latency. This informs using simple tree-based model techniques like Gradient Boosted Decision
  Trees and Random Forests. Tree-based models are used across industry for being low cost for
  inference, avoid over-fitting, and achieve high performance with simple coarse grained features
  like change and target metadata. We have found consistently throughout our work that Gradient
  Boosted Tree models perform best for Google's coarse grained test selection problems.</p>
  <h2 id="features">Features</h2>

  <span id="fig:feature-collection" label="fig:feature-collection">
    [<em><strong>Figure 2</strong></em>
    ![Figure 2](images/icst-2025/fig2.png)](images/icst-2025/fig2.png)
  </span>

  <p>Figure <a href="#fig:feature-collection" data-reference-type="ref" data-reference=
  "fig:feature-collection">[fig:feature-collection]</a> shows an overview of the structure of the
  feature vector. In a production system, only features that can be acquired quickly and reliably
  at scale can be used. The end-to-end latency of deciding which tests to run (including fetching
  the data, assembling the feature vectors, and running the prediction) needs to be well under 20
  minutes such that the current Speculative Cycle finishes before the next one begins. Thus, we are
  using simple, cheap to compute, coarse grained metadata features from targets and commits. The
  predictions are done per target so the features for one target are included in each feature
  vector. However, the predictions are not done for a single commit but across a range of commits.
  To increase the robustness of the commit based features, we only include commits that
  <em>affect</em> the target. A commit affects a target if the target is reachable in the build
  graph from the files modified in the change.</p>
  <p>Target level features include static and dynamic characteristics. The static items are
  unchanging (or very rarely changing) and include target's programming language, whether it is a
  test or build target, and its Bazel rule type, etc. The dynamic items are based on historical
  execution data, such as the failure count in Postsubmit and Presubmit over several different time
  windows. Commit level features include: change metadata like lines of code modified, number of
  reviewers, number of linked bugs, description, etc. Finally, some features correspond to the
  relationship between the target and the changes in this specific cycle - specifically its build
  graph distance to the files modified and whether the target was run at Presubmit time.</p>

  <span id="table:featureDescription" label="table:featureDescription">
    [<em><strong>Table 1</strong></em>
    ![Table 1](images/icst-2025/table1.png)](images/icst-2025/table1.png)
  </span>

  <p>Table <a href="#table:featureDescription" data-reference-type="ref" data-reference=
  "table:featureDescription">[table:featureDescription]</a> contains details on the features used
  in the production model ('BASE') and those added for this paper ('AUG'), for augmented).</p>
  <h2 id="labeling">Labeling</h2>
  <p>Our data set has a row/vector for each target in each Speculative Cycle. In order to train the
  model, we need to label each vector with whether or not that target should be run in the given
  Speculative Cycle. Optimally the targets should only be run when a new failure is introduced into
  the repository. We use the verified culprits data set described previously <span class="citation"
  data-cites="Henderson2023 Henderson2024">(<a href="#ref-Henderson2023" role=
  "doc-biblioref">Henderson et al. 2023</a>, <a href="#ref-Henderson2024" role=
  "doc-biblioref">2024</a>)</span> as our source of truth on which commits introduced breakages.
  Unfortunately, the automated culprit finders de-prioritize comprehensiveness in favor of finding
  results quickly for active breakages blocking developers. This can leave some target level
  breakages with no culprit-identifying labels for up to two weeks after the failures was
  detected.</p>
  <p>In addition, some flakiness in tests and builds may be caused by something outside of the code
  and configuration checked into the repository. For example, a buggy compiler action may
  non-deterministically miscompile a file causing the linker to fail when it happens. Because the
  compile action "succeeds" (produces an output and no error code), the output of the compiler (the
  object file) will be cached. Subsequent builds will re-use the cached output and fail when
  attempting to link. However, if the cached item expires or the build happens in a different data
  center with a different cache the linker action may succeed (when the compiler behaved
  correctly). This type of flakiness is difficult to diagnose real-time with only build
  re-executions due to the multiple layers of caching. However, build executions separated in time
  have a better chance of identifying the flaky behavior.</p>
  <p>In order to protect against unavailability of data, we perform labeling a full week after the
  inference examples are acquired. As we've described, this still does not completely shield us
  from both incorrect positives and lack of coverage but is the best ground truth we have.</p>
  <h2 id="super-extreme-class-imbalance">Super Extreme Class Imbalance</h2>
  <p>Postsubmit breakages are exceedingly rare. The only target-vector rows in our dataset that
  need to be scheduled are targets that are newly broken. This only occurs 0.0001% of the time. In
  model training, we refer to the class with fewer examples at the <em>minority class</em> while
  the class with more examples is the <em>majority class</em>. In our problem, the minority class
  is these newly broken targets just described.</p>
  <p>When there are too few examples in the minority class, the effectiveness of the many training
  algorithms is reduced. The "loss" (e.g. the function model training is optimizing) becomes biased
  towards the majority class. A classic pathological case is a model that trivially predicts the
  majority class - that is, predicts the majority class every time and never predicts the minority
  class. These models will actually minimize the loss function even though they never make useful
  predictions. Our approach must guard against these biases if we want to usefully predict which
  tests to run during a Speculative Cycle.</p>
  <p>For our system, we downsample the negative class (targets that are not newly failing). This
  reduces the imbalance between the classes. It also implicitly values the True Positives (new
  breakages) higher than the True Negatives. Every false negative (missed breakage) leads to
  developer toil as the detection of the breakage is delayed. Given the super extreme imbalance
  between the classes, we expect most targets run during a Speculative Cycle are to be False
  Negatives (and not indicate a new breakage).</p>
  <h2 id="training-configuration">Training Configuration</h2>
  <p>For our model, we use the Yggdrasil Decision Forest library (YDF) <span class="citation"
  data-cites="Guillame-Bert2023">(<a href="#ref-Guillame-Bert2023" role=
  "doc-biblioref">Guillame-Bert et al. 2023</a>)</span>. YDF touts superior decision tree sampling
  and splitting algorithms over prior algorithms (such as XGBoost <span class="citation"
  data-cites="chen2016xgboost">(<a href="#ref-chen2016xgboost" role=
  "doc-biblioref"><strong>chen2016xgboost?</strong></a>)</span>) and provides better integration
  with Google's machine learning infrastructure.</p>
  <p>Conventional training schemes randomly shard their dataset between train, validation, and test
  splits or perhaps perform cross-validation with a set of parallel splits. For our problem,
  randomized splits are harmful and lead to us overestimating our performance as we leak
  time-dependent attributes of the repository across sets. Instead, we create time-ordered splits
  where the train set consists of the first <span class="math inline"><em>A</em></span> days, the
  validation set the next <span class="math inline"><em>B</em></span> days, and the test set the
  final <span class="math inline"><em>C</em></span> days.</p>
  <h1 id="sec:evaluation">Empirical Evaluation</h1>
  <h2 id="evaluation">Evaluation</h2>
  <p>We empirically evaluated Speculative Cycles against a baseline random scheduler on
  culprit-detection recall and against an optimal algorithm for evaluating culprit detection
  latency improvements. We evaluated different dataset pruning/selection and training
  configurations to discover important hyper-parameters for the Transition Prediction model on a
  standard set of features. We examined three research questions:</p>
  <h2 id="research-questions">Research Questions</h2>
  <ol>
    <li>
      <p>What is the performance of Speculative Cycles with Transition Prediction in terms of
      percentage of novel breakages detected (recall) in comparison to a baseline randomized
      testing approach?</p>
    </li>
    <li>
      <p>Speculative cycles compete with full testing cycles for machine resources. Do speculative
      cycles improve productivity outcomes for Google developers?</p>
    </li>
    <li>
      <p>What aspects of the model design most impact system performance? Considered aspects
      include features selected, super-extreme class imbalance corrections, and training
      environment.</p>
    </li>
  </ol>
  <h2 id="measuring-performance-recall">Measuring Performance (Recall)</h2>
  <p>Speculative Cycles as a system attempts to detect bug introducing changes (culprits) through
  running and surfacing at least <span class="math inline"><em>N</em></span> newly failing targets.
  Specifically, the goal is to capture the majority of (newly failing) target results that detect
  novel bugs introduced into the repository with the faster Speculative Cycles instead of the
  slower Comprehensive Cycles. This naturally lends itself to measurement in the form of recall -
  the percentage of these events we capture.</p>
  <p>We additionally observe that our dataset is imperfect. Not every commit labeled as a "culprit"
  is indeed a bug introducing change. As discussed above, our datasets contains some inaccuracies
  particularly when a particular culprit commit only a few targets "blaming" it.<a href="#fn2"
  class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The distribution in the
  number of targets blaming a culprit follows a power law distribution. The majority of culprits
  are blamed by fewer than 10 targets. However, the lower the number of blamed targets the more
  likely it is that the culprit finding was inaccurate and that the commit does not in fact
  introduce a bug. In our labels, the signal is highest for culprits with many targets blaming them
  and lowest when there is only a single target. To protect our system from the noise these "small"
  culprits are filtered out during training.</p>
  <p>We use <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(<em>n</em>)</span>
  to denote the subset of culprits that have at least <span class="math inline"><em>n</em></span>
  targets blaming each culprit. In our evaluation, we provide recall over both (a) <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(1)</span> -
  the set of all culprits and (b) <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span> -
  the set of culprits causing at least 10 target breakages where we successfully detect 10
  breakages. <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span>
  also ensures that we meet the (traditional) minimum required threshold (i.e. 10 failing targets)
  to trigger an automatic rollback. Speculative Cycles sees its highest value in terms of mean
  time-to-fix when automatic rollbacks are correctly triggered (especially for commits that have
  wider impact radius). Therefore, by assessing Recall using <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span>
  we also assess whether or not the Speculative Cycle can find enough evidence to trigger
  rollbacks.</p>
  <h2 id="measuring-productivity-outcomes-latency-improvement">Measuring Productivity Outcomes
  (Latency Improvement)</h2>
  <p>Although recall is a concrete metric, our true goal is to improve productivity by reducing
  <em>friction</em> caused by breakages that slipped into Postsubmit. Speculative Cycles focuses on
  early detection of breakages (and their culprit detection and fix) to reduce the friction caused
  by bad commits in Postsubmit. Its impact is measured by the reduction in "breakage detection
  latency". Early detection and fix also improves the mean time to fix latency for bad commits. We
  will evaluate the "breakage detection latency" decrease as a percentage relative to the current
  latency using empirically observed timings for running both Speculative and Comprehensive over
  the days present in our test dataset.</p>
  <h2 id="high-priority-configuration-of-training-scheme">High Priority Configuration of Training
  Scheme</h2>
  <p>In general, it is considered good practice in machine learning to prioritize dataset quality
  over attempting to fine tune model level hyper-parameters. We have similarly observed little
  value in specific training parameters and use YDF's defaults for Gradient Boosted Trees for this
  work to show the general case performance expected. The more interesting questions that pertain
  to our dataset configuration are:</p>
  <h3 id="training-split-window-size">Training Split / Window Size</h3>
  <p>As we use time based splitting to avoid the issue of "time-traveling" across the datasets, our
  training split configuration determines both the training "window size" and the "delay" between
  the training and test datasets.</p>
  <p>We hypothesized that the training data may have some recency bias. A shorter/ more recent
  look-back horizon on the training data may contain more useful signal for predicting the
  likelihood of a target transitioning. A longer window could dilute the usefulness of more recent
  breakages, while introducing noise from old breakages that have since been fixed and are less
  likely to reoccur. In order to test this hypothesis, we ran the model five times using the same
  feature set and configuration, only changing the number of weeks of data used during training.
  Note that our production pipeline uses a 3:1:1 week split for training:test:validation.</p>
  <h3 id="downsampling-and-upweighting">Downsampling and Upweighting</h3>
  <p>Our extreme class imbalance of <span class="math inline">40, 000 : 1</span> necessitates
  downsampling our negative class to a ratio that is compute-efficient, but also avoids trivially
  predicting 0. Upweighting the positive class can be useful to appropriately value positive
  examples with respect to the loss function of the training algorithm. The ratios of down-sampling
  listed above reduced our class imbalance ratio to <span class="math inline">400 : 1</span>,
  <span class="math inline">200 : 1</span>, and <span class="math inline">40 : 1</span>
  respectively. For each case, we then upweighted the positive samples of target breaking commits
  by the new class imbalance ratio to see what effect, if any, it had on improving model
  performance.</p>
  <h2 id="dataset">Dataset</h2>
  <p>Our evaluation dataset consists of roughly 3 months of Transition Prediction data split across
  Train, Validation, and Test sets. In total, the dataset consisted of 120 Billion target-cycle
  pairs, 7.7 million of which were breaking targets. This corresponds to <span class=
  "math inline">∼</span>20 thousand unique culprits. As mentioned above, we ensure that our splits
  are entirely sequential and disjoint to avoid cross contamination. The exact split and training
  configuration is varied while answering RQ3. We then fix our most performant model for evaluating
  RQ1 against the BASELINE random prediction and for RQ2 against latency in the CI environment with
  only Comprehensive Cycles.</p>
  <h1 id="sec:results">Results</h1>
  <h2 id="summary-of-results">Summary of Results</h2>
  <li>
  <div class="tcolorbox">
    <p><em><strong>Note</strong></em>: For RQ1 and RQ2, the model was trained using the 'AUG'
    feature set (Table <a href="#table:featureDescription" data-reference-type="ref"
    data-reference="table:featureDescription">[table:featureDescription]</a>), a 3-week training
    window, a 0.01 downsampling rate for the majority class, and no upweighting for the minority
    class. This configuration matches our production model, except for the feature set. RQ3
    explores performance across configurations.</p>
  </div>
  </li>
  <h2 class="unnumbered" id=
  "rq1-what-is-the-performance-of-speculative-cycles-with-transition-prediction-in-terms-of-percentage-of-novel-breakages-detected-recall-in-comparison-to-a-baseline-randomized-testing-approach">
  <strong>RQ1</strong>: What is the performance of Speculative Cycles with Transition Prediction in
  terms of percentage of novel breakages detected (recall) in comparison to a baseline randomized
  testing approach?</h2>

  <span id="fig:cl-level-recall" label="fig:cl-level-recall">
    [<em><strong>Figure 3</strong></em>
    ![Figure 3](images/icst-2025/fig3.png)](images/icst-2025/fig3.png)
  </span>

  <p><strong>TRANSPRED demonstrates superior recall compared to the BASELINE model across all
  budgets (see Figure <a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall">[fig:cl-level-recall]</a>). At a budget of 25% of the total targets,
  TRANSPRED achieves a recall of 85%, greater than the BASELINE model's recall of 56%. This
  highlights the model's effectiveness in identifying breaking changes with limited
  resources.</strong></p>
  <p>In the context of Transition Prediction, recall measures the percentage of breaking changes
  identified using a given budget. The budget parameterizes the recall metric as shown in Figure
  <a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall">[fig:cl-level-recall]</a>. To simplify the complex accounting of machine
  and test costs, we use the percentage of targets scheduled by TRANSPRED (versus a Comprehensive
  Cycle). Our budget for this assessment is 25% of the targets that would have been used by a
  Comprehensive Cycle. Recall is then defined as percentage of culprits identified out of the total
  set of culprits.</p>
  <p>Figure <a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall">[fig:cl-level-recall]</a> provides a visual representation of how recall
  changes as the percentage of scheduled targets varies, comparing the performance of TRANSPRED
  against the BASELINE model. For each model the figure shows the recall of both <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(1)</span>
  and <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span>.
  <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(1)</span>
  corresponds to finding any target for a breakage or informally, that breakages are at least
  "identified". The recall for <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span>
  concentrates on finding breakages with at least 10 targets broken - including scheduling at least
  10 of those broken targets to ensure auto rollback has enough evidence to trigger.</p>
  <p>For TRANSPRED, the <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(10)</span>
  recall is higher than the <span class=
  "math inline"><em>A</em><em>t</em><em>L</em><em>e</em><em>a</em><em>s</em><em>t</em>(1)</span>
  recall. This indicates the model is better at catching bad commits that break 10 or more targets.
  It's intuitive that larger breakages would be better handled by the model as they are much better
  represented in the dataset, but an interesting result that we have is better coverage of them,
  i.e., consistently finding more individual targets broken by such commits. Culprits that only
  break a few tests are arguably less consequential than the few culprits that break a large number
  of tests. This strong performance on commits that break at least 10 targets indicates that when
  TRANSPRED does find a break, it has a good chance to be rolled back automatically.</p>
  <p>Both configurations for TRANSPRED outperform the BASELINE random model, indicating that the
  model is learning something useful from metadata and that the metadata alone (without considering
  the content of the changes) can be predictive of breakages. We look forward to future experiments
  utilizing the content of commits that compare both the efficacy and the model costs.</p>
  <h2 class="unnumbered" id=
  "rq2-speculative-cycles-compete-with-full-testing-cycles-for-machine-resources-what-configurations-of-speculative-cycles-improve-productivity-outcomes-for-google-developers">
  <strong>RQ2</strong>: Speculative Cycles compete with full testing cycles for machine resources,
  what configurations of Speculative Cycles improve productivity outcomes for Google
  developers?</h2>
  <p><strong>Speculative Cycles reduce the median (p50) breakage detection latency by 65% (70
  minutes) over the existing Comprehensive Cycles.</strong></p>

  <span id="fig:detection-latency" label="fig:detection-latency">
    [<em><strong>Figure 4</strong></em>
    ![Figure 4](images/icst-2025/fig4.png)](images/icst-2025/fig4.png)
  </span>

  <p>Figure <a href="#fig:detection-latency" data-reference-type="ref" data-reference=
  "fig:detection-latency">[fig:detection-latency]</a> visualizes a histogram of the performance of
  the model in terms of <em>breakage detection latency</em>. We consider this metric to be the
  "outcome metric" for this work as it is a proxy for improving overall developer productivity. It
  is known from internal work that developer productivity degrades when they need to debug build
  and test breakages they did not cause during interactive development. Productivity also degrades
  when they are interrupted to troubleshoot releases stalled due to breakages. By reducing the
  total time from culprit commit submission to detection, the model reduces the amount of friction
  developers experience from breakages.</p>
  <p>As with RQ1, we have fixed TRANSPRED to represent Speculative Cycles using the same Transition
  Prediction model and scheduling 25% of targets. In the visualization, the model (in black) is
  compared against two alternatives. The first labeled "Only Comprehensive" shows the distribution
  of time (in minutes) it takes the current TAP Postsubmit scheduling algorithm (Comprehensive
  Cycles) to detect breakages. The second alternative, "Optimal" imagines a perfect model that
  always correctly predicts which target need to be scheduled. Our model, TRANSPRED, falls between
  these two extremes of no-improvement to full-improvement. Observe that in comparison to "Only
  Comprehensive" the variance of breakage detection latency for TRANSPRED is substantially reduced.
  While, the new model isn't perfect (as shown in the figure), it is a welcome result that the
  Speculative Cycles reliably finds culprit commits in 37 minutes in the median case (p50), a 65%
  latency reduction over traditional Comprehensive Cycles that (p50) take 107 minutes.</p>
  <h2 class="unnumbered" id=
  "rq3what-aspects-of-the-model-design-most-impact-performance-considered-aspects-include-features-selected-super-extreme-class-imbalance-corrections-and-training-environment.">
  <strong>RQ3</strong>:What aspects of the model design most impact performance? Considered aspects
  include features selected, super-extreme class imbalance corrections, and training
  environment.</h2>
  <p><strong>The feature set used and the training window length had the greatest impact on model
  performance as measured by ROC AUC. Downsampling is critical for training speed but further
  downsampling hurts performance while upweighting has negligible impact.</strong></p>
  <p>To better understand how model design impacts performance, we ran multiple experiments across
  the different axes of decision making, including 1) feature selection, 2) training environment.
  and 3) different ways to correct for the super-extreme class imbalance</p>
  <h3 id="feature-selection">Feature Selection</h3>

  <span id="table:samplingAndWeightTable" label="table:training-length">
  <span id="table:training-length" label="table:training-length">
    [<em><strong>Tables 2 and 3</strong></em>
    ![Tables 2 and 3](images/icst-2025/table2-3.png)](images/icst-2025/table2-3.png)
  </span>
  </span>

  <p>Table <a href="#table:training-length" data-reference-type="ref" data-reference=
  "table:training-length">[table:training-length]</a> shows an improvement in performance when the
  model is trained on the 'AUGMENTED' feature set vs 'BASE' set, regardless of number of weeks
  considered for the training window. Critically, we see a lot of signal coming from the added
  target history features over longer time intervals.</p>
  <h3 id="training-environment">Training Environment</h3>
  <p>Table <a href="#table:training-length" data-reference-type="ref" data-reference=
  "table:training-length">[table:training-length]</a> indicates our hypothesis was correct -
  performance starts to degrade as we increase or decrease the training window length from the
  optimal 3 weeks.</p>
  <h3 id="correcting-for-super-extreme-class-imbalance">Correcting for Super-Extreme Class
  Imbalance</h3>
  <p>Production systems exhibit extremely low Postsubmit breakage rates (40,000:1 in our case).
  This class imbalance can hinder model training by biasing it towards the majority class. However,
  due to the asymmetric cost of false positives (unnecessary execution) versus false negatives
  (late breakage detection), prioritizing catching newly breaking targets at the cost of
  incorrectly scheduling some healthy targets is acceptable. This lets us experiment with some
  imbalance-correction techniques without being strictly bound to the true breakage rate, as shown
  in Table <a href="#table:samplingAndWeightTable" data-reference-type="ref" data-reference=
  "table:samplingAndWeightTable">[table:samplingAndWeightTable]</a>.</p>
  <h1 id="related-work">Related Work</h1>
  <p>This paper explored the problem of reducing the time to discovery of novel test failures in
  continuous integration. This is a variation of the well studied problem of Test Case
  Prioritization and Selection <span class="citation" data-cites=
  "Bates1993 Rothermel1997 Elbaum2000 Leon2003 Engstrom2010 Zhou2010 Singh2012 Gligoric2014 Mondal2015 Musa2015 DeS.CamposJunior2017 Najafi2019 Machalica2019 DeCastro-Cabrera2020 pan2022 Jin2023">
  (<a href="#ref-Bates1993" role="doc-biblioref">Bates and Horwitz 1993</a>; <a href=
  "#ref-Rothermel1997" role="doc-biblioref">Rothermel and Harrold 1997</a>; <a href=
  "#ref-Elbaum2000" role="doc-biblioref">Elbaum, Malishevsky, and Rothermel 2000</a>; <a href=
  "#ref-Leon2003" role="doc-biblioref">Leon and Podgurski 2003</a>; <a href="#ref-Engstrom2010"
  role="doc-biblioref">Engström, Runeson, and Skoglund 2010</a>; <a href="#ref-Zhou2010" role=
  "doc-biblioref">Zhou 2010</a>; <a href="#ref-Singh2012" role="doc-biblioref">Singh et al.
  2012</a>; <a href="#ref-Gligoric2014" role="doc-biblioref">Gligoric et al. 2014</a>; <a href=
  "#ref-Mondal2015" role="doc-biblioref">Mondal, Hemmati, and Durocher 2015</a>; <a href=
  "#ref-Musa2015" role="doc-biblioref">Musa et al. 2015</a>; <a href="#ref-DeS.CamposJunior2017"
  role="doc-biblioref">de S. Campos Junior et al. 2017</a>; <a href="#ref-Najafi2019" role=
  "doc-biblioref">Najafi, Shang, and Rigby 2019</a>; <a href="#ref-Machalica2019" role=
  "doc-biblioref">Machalica et al. 2019</a>; <a href="#ref-DeCastro-Cabrera2020" role=
  "doc-biblioref">De Castro-Cabrera, García-Dominguez, and Medina-Bulo 2020</a>; <a href=
  "#ref-pan2022" role="doc-biblioref">Pan et al. 2022</a>; <a href="#ref-Jin2023" role=
  "doc-biblioref">Jin and Servant 2023</a>)</span> adapted to the CI environment <span class=
  "citation" data-cites=
  "Chen2020 Saidani2020 Abdalkareem2021 Abdalkareem2021a Al-Sabbagh2022 Saidani2022 Saidani2022a Jin2023 Jin2024 Liu2023 Hong2024 Sun2024 Wang2024 Zeng2024">
  (<a href="#ref-Chen2020" role="doc-biblioref">Chen et al. 2020</a>; <a href="#ref-Saidani2020"
  role="doc-biblioref">Saidani et al. 2020</a>; <a href="#ref-Abdalkareem2021" role=
  "doc-biblioref">Abdalkareem, Mujahid, and Shihab 2021</a>; <a href="#ref-Abdalkareem2021a" role=
  "doc-biblioref">Abdalkareem et al. 2021</a>; <a href="#ref-Al-Sabbagh2022" role=
  "doc-biblioref">Al-Sabbagh, Staron, and Hebig 2022</a>; <a href="#ref-Saidani2022" role=
  "doc-biblioref">Saidani, Ouni, and Mkaouer 2022a</a>, <a href="#ref-Saidani2022a" role=
  "doc-biblioref">2022b</a>; <a href="#ref-Jin2023" role="doc-biblioref">Jin and Servant 2023</a>;
  <a href="#ref-Jin2024" role="doc-biblioref">Jin et al. 2024</a>; <a href="#ref-Liu2023" role=
  "doc-biblioref">Liu et al. 2023</a>; <a href="#ref-Hong2024" role="doc-biblioref">Hong et al.
  2024</a>; <a href="#ref-Sun2024" role="doc-biblioref">Sun, Habchi, and McIntosh 2024</a>;
  <a href="#ref-Wang2024" role="doc-biblioref">G. Wang et al. 2024</a>; <a href="#ref-Zeng2024"
  role="doc-biblioref">Zeng et al. 2024</a>)</span>. Our approach is predicated on having highly
  accurate historical information on what causes breakages in the Google environment. We achieve
  this through culprit finding and verification <span class="citation" data-cites=
  "Henderson2023 Henderson2024">(<a href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
  2023</a>, <a href="#ref-Henderson2024" role="doc-biblioref">2024</a>)</span>. Culprit finding
  <span class="citation" data-cites=
  "Couder2008 Ziftci2013a Ziftci2017 Saha2017 Najafi2019a Beheshtian2022 keenan2019 An2021 Ocariza2022">
  (<a href="#ref-Couder2008" role="doc-biblioref">Couder 2008</a>; <a href="#ref-Ziftci2013a" role=
  "doc-biblioref">Ziftci and Ramavajjala 2013</a>; <a href="#ref-Ziftci2017" role=
  "doc-biblioref">Ziftci and Reardon 2017</a>; <a href="#ref-Saha2017" role="doc-biblioref">Saha
  and Gligoric 2017</a>; <a href="#ref-Najafi2019a" role="doc-biblioref">Najafi, Rigby, and Shang
  2019</a>; <a href="#ref-Beheshtian2022" role="doc-biblioref">Beheshtian, Bavand, and Rigby
  2022</a>; <a href="#ref-keenan2019" role="doc-biblioref">Keenan 2019</a>; <a href="#ref-An2021"
  role="doc-biblioref">An and Yoo 2021</a>; <a href="#ref-Ocariza2022" role="doc-biblioref">Ocariza
  2022</a>)</span> aims to identify the true "bug introducing commits" (BICs) <span class=
  "citation" data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019 An2023">(<a href=
  "#ref-Sliwerski2005" role="doc-biblioref">Śliwerski, Zimmermann, and Zeller 2005</a>; <a href=
  "#ref-Rodriguez-Perez2018" role="doc-biblioref">Rodríguez-Pérez, Robles, and González-Barahona
  2018</a>; <a href="#ref-Borg2019" role="doc-biblioref">Borg et al. 2019</a>; <a href=
  "#ref-Wen2019" role="doc-biblioref">Wen et al. 2019</a>; <a href="#ref-An2023" role=
  "doc-biblioref">An et al. 2023</a>)</span> by rerunning tests over the search space of possible
  versions.</p>
  <p>The extensive amount of work on test selection and prioritization makes it difficult to fully
  summarize outside of a survey <span class="citation" data-cites=
  "Engstrom2010 Singh2012 DeS.CamposJunior2017 DeCastro-Cabrera2020 pan2022">(<a href=
  "#ref-Engstrom2010" role="doc-biblioref">Engström, Runeson, and Skoglund 2010</a>; <a href=
  "#ref-Singh2012" role="doc-biblioref">Singh et al. 2012</a>; <a href="#ref-DeS.CamposJunior2017"
  role="doc-biblioref">de S. Campos Junior et al. 2017</a>; <a href="#ref-DeCastro-Cabrera2020"
  role="doc-biblioref">De Castro-Cabrera, García-Dominguez, and Medina-Bulo 2020</a>; <a href=
  "#ref-pan2022" role="doc-biblioref">Pan et al. 2022</a>)</span>. Here are some highlights. Early
  work in test selection focused on selecting tests while maintaining test suite "adequacy"
  according to some (usually coverage based) criterion. For instance Bates and Horwitz <span class=
  "citation" data-cites="Bates1993">(<a href="#ref-Bates1993" role="doc-biblioref">Bates and
  Horwitz 1993</a>)</span> define a coverage criterion based on coverage of elements in the Program
  Dependence Graph (PDG) <span class="citation" data-cites="Horwitz1988 Podgurski1989">(<a href=
  "#ref-Horwitz1988" role="doc-biblioref">Horwitz, Prins, and Reps 1988</a>; <a href=
  "#ref-Podgurski1989" role="doc-biblioref">Podgurski and Clarke 1989</a>)</span> and Rothermel and
  Harrold developed a regression selection technique based on the PDG <span class="citation"
  data-cites="Rothermel1997">(<a href="#ref-Rothermel1997" role="doc-biblioref">Rothermel and
  Harrold 1997</a>)</span>. By 2000 papers such as Elbaum <em>et al.</em> <span class="citation"
  data-cites="Elbaum2000">(<a href="#ref-Elbaum2000" role="doc-biblioref">Elbaum, Malishevsky, and
  Rothermel 2000</a>)</span>'s were looking at prioritizing test cases based statement coverage to
  speed up test suite execution and evaluating performance based on the now standard <em>average
  percentage of faults detected</em> (APFD) metric. By 2003, the community was starting to look for
  other sources of data to prioritize and filter test cases. For instance, Leon and Podgurski
  <span class="citation" data-cites="Leon2003">(<a href="#ref-Leon2003" role="doc-biblioref">Leon
  and Podgurski 2003</a>)</span> compared the bug finding efficacy (APFD) of coverage techniques
  versus techniques that prioritized diversity of basic block execution profiles (that included
  execution counts). In 2010 Engstrom <em>et al.</em> <span class="citation" data-cites=
  "Engstrom2010">(<a href="#ref-Engstrom2010" role="doc-biblioref">Engström, Runeson, and Skoglund
  2010</a>)</span> surveyed the state of the test selection and documented 28 techniques.</p>
  <p>In 2017 the TAP team at Google began publishing on the problem <span class="citation"
  data-cites="Memon2017 Leong2019">(<a href="#ref-Memon2017" role="doc-biblioref">Memon et al.
  2017</a>; <a href="#ref-Leong2019" role="doc-biblioref">Leong et al. 2019</a>)</span> and
  separately a partner team had put into production an ML based test filtering system for TAP
  Presubmit. In contrast to this prior work by TAP, our current work is better grounded because of
  the verified culprits dataset we created in the last several years. For Memon <em>et al.</em>
  <span class="citation" data-cites="Memon2017">(<a href="#ref-Memon2017" role=
  "doc-biblioref">Memon et al. 2017</a>)</span> TAP did not yet have a robust culprit finding
  system to precisely identify the bug introducing changes. Instead, that paper attempted to infer
  what commits might be the culprit. In Leong <em>et al.</em> <span class="citation" data-cites=
  "Leong2019">(<a href="#ref-Leong2019" role="doc-biblioref">Leong et al. 2019</a>)</span> TAP did
  have a culprit finder but it was not as robust against flakiness and non-determinism as our
  current system. Our current culprit finder uses the Flake Aware algorithm (FACF) that adaptively
  deflakes the tests while performing culprit finding. Additionally, we further <em>verify</em> all
  culprit commits by doing additional executions in patterns designed to catch flaky behavior
  <span class="citation" data-cites="Henderson2023 Henderson2024">(<a href="#ref-Henderson2023"
  role="doc-biblioref">Henderson et al. 2023</a>, <a href="#ref-Henderson2024" role=
  "doc-biblioref">2024</a>)</span>. The FACF paper <span class="citation" data-cites=
  "Henderson2023">(<a href="#ref-Henderson2023" role="doc-biblioref">Henderson et al.
  2023</a>)</span> performed experiments that show the FACF algorithm is statistically
  significantly more accurate than the "deflaked variant" of the standard "bisect" algorithm used
  in the Leong paper. Specifically the experiments indicate FACF is <span class=
  "math inline">&gt;</span>10% more accurate at identifying flaky breakages than Bisect with 8
  additional deflaking runs. By properly deflaking our training and evaluation labels, our model is
  much less likely to prioritize flaky tests and our evaluation more accurately reflects our
  desired productivity outcomes.</p>
  <p>Meanwhile in 2019, Machalica <em>et al.</em> <span class="citation" data-cites=
  "Machalica2019">(<a href="#ref-Machalica2019" role="doc-biblioref">Machalica et al.
  2019</a>)</span> reported on a similar effort at Facebook in 2019 to develop a test selection
  system for their version of Presubmit testing. Conceptually, the Presubmit selection system at
  Facebook and the Presubmit selection system at Google have similarities. Both use shallow machine
  learning models with sets of features not dissimilar to the features presented in this work. Both
  systems (like this work) take a systematic approach to reducing the effect of flaky failures on
  developer productivity. The primary contribution of this paper versus this past work is the
  application and evaluation of selection techniques to the Postsubmit environment. In Postsubmit,
  the features must be necessarily adjusted as the prediction of whether or not a test will change
  behavior is not against a single commit but against a sequence of commits. Evaluation also
  changes: the concern is not just about recall but also latency of breakage detection and
  time-to-fix.</p>
  <p>Recently the wider field (outside of Google and Facebook) has produced compelling work
  <span class="citation" data-cites=
  "Jin2023 Jin2024 Liu2023 Hong2024 Sun2024 Wang2024 Zeng2024">(<a href="#ref-Jin2023" role=
  "doc-biblioref">Jin and Servant 2023</a>; <a href="#ref-Jin2024" role="doc-biblioref">Jin et al.
  2024</a>; <a href="#ref-Liu2023" role="doc-biblioref">Liu et al. 2023</a>; <a href=
  "#ref-Hong2024" role="doc-biblioref">Hong et al. 2024</a>; <a href="#ref-Sun2024" role=
  "doc-biblioref">Sun, Habchi, and McIntosh 2024</a>; <a href="#ref-Wang2024" role=
  "doc-biblioref">G. Wang et al. 2024</a>; <a href="#ref-Zeng2024" role="doc-biblioref">Zeng et al.
  2024</a>)</span>. We will note a few items here. Jin and Servant <span class="citation"
  data-cites="Jin2023">(<a href="#ref-Jin2023" role="doc-biblioref">Jin and Servant
  2023</a>)</span> presented HybridCISave which uses a shallow machine learning based approach to
  do both fine-grained (individual tests) and coarse grained (CI workflow builds) selection. By
  combining both selection types both cost savings and safety are improved. The authors evaluate
  their results using TravisTorrent <span class="citation" data-cites="Beller2022">(<a href=
  "#ref-Beller2022" role="doc-biblioref">Beller, Gousios, and Zaidman 2022</a>)</span>. This work
  is a good representative of the general approach of using shallow machine learning for selection
  outside of the mega-corp environment. Wang <em>et al.</em> <span class="citation" data-cites=
  "Wang2024">(<a href="#ref-Wang2024" role="doc-biblioref">G. Wang et al. 2024</a>)</span> address
  the problem of feature selection by using a transformer based language model, GitSense, that
  automates statistic feature extraction. The authors evaluate their results using the dataset from
  Chen <em>et al.</em> <span class="citation" data-cites="Chen2020">(<a href="#ref-Chen2020" role=
  "doc-biblioref">Chen et al. 2020</a>)</span>. Zeng <em>et al.</em> <span class="citation"
  data-cites="Zeng2024">(<a href="#ref-Zeng2024" role="doc-biblioref">Zeng et al. 2024</a>)</span>
  use mutation testing to better assess the safety of a commercial CI system (YourBase) that skips
  tests based on inferred dependencies. This use of mutation testing highlights one weakness of
  traditional safety evaluation, they only use historical faults and may miss novel bugs. By
  injecting mutation faults a more holistic view of the performance of the model can be obtained.
  Determining how to apply mutation testing to shallow machine learning approaches that use
  metadata features (such as ours) is an open problem.</p>
  <h1 id="sec:conclusion">Conclusion</h1>
  <p>In this paper, we presented a new scheduling system for Google's main test automation platform
  (TAP). The new scheduling system, Speculative Cycles, prioritizes finding novel test breakages
  faster. Speculative Cycles are powered by a new machine learning model, Transition Prediction,
  that predicts when tests are likely to fail. While the results are not perfect, they indicate
  that the new system reduces the breakage detection latency by 65% (70 minutes) on average. This
  reduction in breakage detection time leads to a reduction in developer friction and increased
  productivity. In terms of model design, the most impactful choice made was total amount of prior
  history used to train the model. As the training set size went from 1 week to 3 weeks model
  performance improved. But as the training set size further increased the model performance
  actually began to degrade. Only high level metadata features were used in this paper. In the
  future we look forward to experimenting with more complex models.</p>

  <ul>
  <div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
    <li>
    <div id="ref-Abdalkareem2021" class="csl-entry" role="listitem">
      Abdalkareem, Rabe, Suhaib Mujahid, and Emad Shihab. 2021. <span>"A <span>Machine Learning
      Approach</span> to <span>Improve</span> the <span>Detection</span> of <span>CI Skip
      Commits</span>."</span> <em>IEEE Transactions on Software Engineering</em> 47 (12): 2740-54.
      <a href=
      "https://doi.org/10.1109/TSE.2020.2967380">https://doi.org/10.1109/TSE.2020.2967380</a>.
    </div>
    </li>
    <li>
    <div id="ref-Abdalkareem2021a" class="csl-entry" role="listitem">
      Abdalkareem, Rabe, Suhaib Mujahid, Emad Shihab, and Juergen Rilling. 2021. <span>"Which
      <span>Commits Can Be CI Skipped</span>?"</span> <em>IEEE Transactions on Software
      Engineering</em> 47 (3): 448-63. <a href=
      "https://doi.org/10.1109/TSE.2019.2897300">https://doi.org/10.1109/TSE.2019.2897300</a>.
    </div>
    </li>
    <li>
    <div id="ref-Al-Sabbagh2022" class="csl-entry" role="listitem">
      Al-Sabbagh, Khaled, Miroslaw Staron, and Regina Hebig. 2022. <span>"Predicting Build Outcomes
      in Continuous Integration Using Textual Analysis of Source Code Commits."</span> In
      <em>Proceedings of the 18th <span>International Conference</span> on <span>Predictive
      Models</span> and <span>Data Analytics</span> in <span>Software Engineering</span></em>,
      42-51. Singapore Singapore: ACM. <a href=
      "https://doi.org/10.1145/3558489.3559070">https://doi.org/10.1145/3558489.3559070</a>.
    </div>
    </li>
    <li>
    <div id="ref-An2023" class="csl-entry" role="listitem">
      An, Gabin, Jingun Hong, Naryeong Kim, and Shin Yoo. 2023. <span>"Fonte: <span>Finding Bug
      Inducing Commits</span> from <span>Failures</span>."</span> arXiv. <a href=
      "http://arxiv.org/abs/2212.06376">http://arxiv.org/abs/2212.06376</a>.
    </div>
    </li>
    <li>
    <div id="ref-An2021" class="csl-entry" role="listitem">
      An, Gabin, and Shin Yoo. 2021. <span>"Reducing the Search Space of Bug Inducing Commits Using
      Failure Coverage."</span> In <em>Proceedings of the 29th <span>ACM Joint Meeting</span> on
      <span>European Software Engineering Conference</span> and <span>Symposium</span> on the
      <span>Foundations</span> of <span>Software Engineering</span></em>, 1459-62. Athens Greece:
      ACM. <a href=
      "https://doi.org/10.1145/3468264.3473129">https://doi.org/10.1145/3468264.3473129</a>.
    </div>
    </li>
    <li>
    <div id="ref-Bates1993" class="csl-entry" role="listitem">
      Bates, Samuel, and Susan Horwitz. 1993. <span>"Incremental Program Testing Using Program
      Dependence Graphs."</span> In <em>Proceedings of the 20th <span>ACM SIGPLAN-SIGACT</span>
      Symposium on <span>Principles</span> of Programming Languages - <span>POPL</span> '93</em>,
      384-96. Charleston, South Carolina, United States: ACM Press. <a href=
      "https://doi.org/10.1145/158511.158694">https://doi.org/10.1145/158511.158694</a>.
    </div>
    </li>
    <li>
    <div id="ref-Beheshtian2022" class="csl-entry" role="listitem">
      Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby. 2022. <span>"Software
      <span>Batch Testing</span> to <span>Save Build Test Resources</span> and to <span>Reduce
      Feedback Time</span>."</span> <em>IEEE Transactions on Software Engineering</em> 48 (8):
      2784-2801. <a href=
      "https://doi.org/10.1109/TSE.2021.3070269">https://doi.org/10.1109/TSE.2021.3070269</a>.
    </div>
    </li>
    <li>
    <div id="ref-Beller2022" class="csl-entry" role="listitem">
      Beller, Moritz, Georgios Gousios, and Andy Zaidman. 2022.
      <span>"<span>TravisTorrent</span>."</span> figshare. <a href=
      "https://doi.org/10.6084/M9.FIGSHARE.19314170.V1">https://doi.org/10.6084/M9.FIGSHARE.19314170.V1</a>.
    </div>
    </li>
    <li>
    <div id="ref-Bland2012" class="csl-entry" role="listitem">
      Bland, Mike. 2012. <span>"The <span>Chris</span>/<span>Jay Continuous Build</span>."</span>
      Personal {{Website}}. <em>Mike Bland's Blog</em>. <a href=
      "https://mike-bland.com/2012/06/21/chris-jay-continuous-%20build.html">https://mike-bland.com/2012/06/21/chris-jay-continuous-
      build.html</a>.
    </div>
    </li>
    <li>
    <div id="ref-Borg2019" class="csl-entry" role="listitem">
      Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
      <span>"<span>SZZ</span> Unleashed: An Open Implementation of the <span>SZZ</span> Algorithm -
      Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the
      <span>Jenkins</span> Project."</span> In <em>Proceedings of the 3rd <span>ACM SIGSOFT
      International Workshop</span> on <span>Machine Learning Techniques</span> for <span>Software
      Quality Evaluation</span> - <span>MaLTeSQuE</span> 2019</em>, 7-12. Tallinn, Estonia: ACM
      Press. <a href=
      "https://doi.org/10.1145/3340482.3342742">https://doi.org/10.1145/3340482.3342742</a>.
    </div>
    </li>
    <li>
    <div id="ref-Chen2020" class="csl-entry" role="listitem">
      Chen, Bihuan, Linlin Chen, Chen Zhang, and Xin Peng. 2020. <span>"<span>BUILDFAST</span>:
      <span class="nocase">History-aware</span> Build Outcome Prediction for Fast Feedback and
      Reduced Cost in Continuous Integration."</span> In <em>2020 35th
      <span>IEEE</span>/<span>ACM</span> International Conference on Automated Software Engineering
      (<span>ASE</span>)</em>, 42-53.
    </div>
    </li>
    <li>
    <div id="ref-Couder2008" class="csl-entry" role="listitem">
      Couder, Christian. 2008. <span>"Fighting Regressions with Git Bisect."</span> <em>The Linux
      Kernel Archives</em> 4 (5). <a href=
      "https://www.%20kernel.%20org/pub/software/scm/git/doc%20s/git-%20bisect-lk2009.html">https://www.
      kernel. org/pub/software/scm/git/doc s/git- bisect-lk2009.html</a>.
    </div>
    </li>
    <li>
    <div id="ref-DeCastro-Cabrera2020" class="csl-entry" role="listitem">
      De Castro-Cabrera, M. Del Carmen, Antonio García-Dominguez, and Inmaculada Medina-Bulo. 2020.
      <span>"Trends in Prioritization of Test Cases: 2017-2019."</span> <em>Proceedings of the ACM
      Symposium on Applied Computing</em>, 2005-11. <a href=
      "https://doi.org/10.1145/3341105.3374036">https://doi.org/10.1145/3341105.3374036</a>.
    </div>
    </li>
    <li>
    <div id="ref-DeS.CamposJunior2017" class="csl-entry" role="listitem">
      de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David, Regina Braga,
      Fernanda Campos, and Victor Ströele. 2017. <span>"Test <span>Case Prioritization</span>:
      <span>A Systematic Review</span> and <span>Mapping</span> of the
      <span>Literature</span>."</span> In <em>Proceedings of the 31st <span>Brazilian
      Symposium</span> on <span>Software Engineering</span></em>, 34-43. New York, NY, USA: ACM.
      <a href=
      "https://doi.org/10.1145/3131151.3131170">https://doi.org/10.1145/3131151.3131170</a>.
    </div>
    </li>
    <li>
    <div id="ref-Elbaum2000" class="csl-entry" role="listitem">
      Elbaum, Sebastian, Alexey G. Malishevsky, and Gregg Rothermel. 2000. <span>"Prioritizing Test
      Cases for Regression Testing."</span> In <em>Proceedings of the 2000 <span>ACM SIGSOFT</span>
      International Symposium on Software Testing and Analysis</em>, 102-12. Issta '00. New York,
      NY, USA: Association for Computing Machinery. <a href=
      "https://doi.org/10.1145/347324.348910">https://doi.org/10.1145/347324.348910</a>.
    </div>
    </li>
    <li>
    <div id="ref-Engstrom2010" class="csl-entry" role="listitem">
      Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. <span>"A Systematic Review on
      Regression Test Selection Techniques."</span> <em>Information and Software Technology</em> 52
      (1): 14-30. <a href=
      "https://doi.org/10.1016/j.infsof.2009.07.001">https://doi.org/10.1016/j.infsof.2009.07.001</a>.
    </div>
    </li>
    <li>
    <div id="ref-Fowler2006" class="csl-entry" role="listitem">
      Fowler, Martin. 2006. <span>"Continuous <span>Integration</span>."</span> <a href=
      "https://martinfowler.com/articles/%20continuousIntegration.html">https://martinfowler.com/articles/
      continuousIntegration.html</a>.
    </div>
    </li>
    <li>
    <div id="ref-Gligoric2014" class="csl-entry" role="listitem">
      Gligoric, Milos, Rupak Majumdar, Rohan Sharma, Lamyaa Eloussi, and Darko Marinov. 2014.
      <span>"Regression <span>Test Selection</span> for <span>Distributed Software
      Histories</span>."</span> In <em>Computer <span>Aided Verification</span></em>, edited by
      David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Alfred Kobsa, Friedemann
      Mattern, John C. Mitchell, et al., 8559:293-309. Cham: Springer International Publishing.
      <a href=
      "https://doi.org/10.1007/978-3-319-08867-9_19">https://doi.org/10.1007/978-3-319-08867-9_19</a>.
    </div>
    </li>
    <li>
    <div id="ref-Guillame-Bert2023" class="csl-entry" role="listitem">
      Guillame-Bert, Mathieu, Sebastian Bruch, Richard Stotz, and Jan Pfeifer. 2023.
      <span>"Yggdrasil <span>Decision Forests</span>: <span>A Fast</span> and <span>Extensible
      Decision Forests Library</span>."</span> In <em>Proceedings of the 29th <span>ACM SIGKDD
      Conference</span> on <span>Knowledge Discovery</span> and <span>Data Mining</span></em>,
      4068-77. Long Beach CA USA: ACM. <a href=
      "https://doi.org/10.1145/3580305.3599933">https://doi.org/10.1145/3580305.3599933</a>.
    </div>
    </li>
    <li>
    <div id="ref-Gupta2011" class="csl-entry" role="listitem">
      Gupta, Pooja, Mark Ivey, and John Penix. 2011. <span>"Testing at the Speed and Scale of
      <span>Google</span>."</span> <a href=
      "https://google-engtools.blogspot.com/2011/06/testing-at-%20speed-and-scale-of-google.html">https://google-engtools.blogspot.com/2011/06/testing-at-
      speed-and-scale-of-google.html</a>.
    </div>
    </li>
    <li>
    <div id="ref-Henderson2023" class="csl-entry" role="listitem">
      Henderson, Tim A. D., Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy. 2023.
      <span>"Flake <span>Aware Culprit Finding</span>."</span> In <em>2023 <span>IEEE
      Conference</span> on <span>Software Testing</span>, <span>Verification</span> and
      <span>Validation</span> (<span>ICST</span>)</em>. IEEE. <a href=
      "https://doi.org/10.1109/ICST57152.2023.00041">https://doi.org/10.1109/ICST57152.2023.00041</a>.
    </div>
    </li>
    <li>
    <div id="ref-Henderson2024" class="csl-entry" role="listitem">
      Henderson, Tim A. D., Avi Kondareddy, Sushmita Azad, and Eric Nickell. 2024.
      <span>"<span>SafeRevert</span>: <span>When Can Breaking Changes</span> Be <span>Automatically
      Reverted</span>?"</span> In <em>2024 <span>IEEE Conference</span> on <span>Software
      Testing</span>, <span>Verification</span> and <span>Validation</span>
      (<span>ICST</span>)</em>, 395-406. Toronto, ON, Canada: IEEE. <a href=
      "https://doi.org/10.1109/ICST60714.2024.00043">https://doi.org/10.1109/ICST60714.2024.00043</a>.
    </div>
    </li>
    <li>
    <div id="ref-Hoang2024" class="csl-entry" role="listitem">
      Hoang, Minh, and Adrian Berding. 2024. <span>"Presubmit <span>Rescue</span>:
      <span>Automatically Ignoring FlakyTest Executions</span>."</span> In <em>Proceedings of the
      1st <span>International Workshop</span> on <span>Flaky Tests</span></em>, 1-2. Lisbon
      Portugal: ACM. <a href=
      "https://doi.org/10.1145/3643656.3643896">https://doi.org/10.1145/3643656.3643896</a>.
    </div>
    </li>
    <li>
    <div id="ref-Hong2024" class="csl-entry" role="listitem">
      Hong, Yang, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Patanamon Thongtanunam, Arik
      Friedman, Xing Zhao, and Anton Krasikov. 2024. <span>"Practitioners' <span>Challenges</span>
      and <span>Perceptions</span> of <span>CI Build Failure Predictions</span> at
      <span>Atlassian</span>."</span> In <em>Companion <span>Proceedings</span> of the 32nd
      <span>ACM International Conference</span> on the <span>Foundations</span> of <span>Software
      Engineering</span></em>, 370-81. Porto de Galinhas Brazil: ACM. <a href=
      "https://doi.org/10.1145/3663529.3663856">https://doi.org/10.1145/3663529.3663856</a>.
    </div>
    </li>
    <li>
    <div id="ref-Horwitz1988" class="csl-entry" role="listitem">
      Horwitz, Susan, Jan Prins, and Thomas Reps. 1988. <span>"On the Adequacy of Program
      Dependence Graphs for Representing Programs."</span> <em>ACM SIGPLAN-SIGACT Symposium on
      Principles of Programming Languages (POPL)</em>, 146-57. <a href=
      "https://doi.org/10.1145/73560.73573">https://doi.org/10.1145/73560.73573</a>.
    </div>
    </li>
    <li>
    <div id="ref-Jin2024" class="csl-entry" role="listitem">
      Jin, Xianhao, Yifei Feng, Chen Wang, Yang Liu, Yongning Hu, Yufei Gao, Kun Xia, and Luchuan
      Guo. 2024. <span>"<span>PIPELINEASCODE</span>: <span>A CI</span>/<span>CD Workflow Management
      System</span> Through <span>Configuration Files</span> at <span>ByteDance</span>."</span> In
      <em>2024 <span>IEEE International Conference</span> on <span>Software Analysis</span>,
      <span>Evolution</span> and <span>Reengineering</span> (<span>SANER</span>)</em>, 1011-22.
      Rovaniemi, Finland: IEEE. <a href=
      "https://doi.org/10.1109/SANER60148.2024.00109">https://doi.org/10.1109/SANER60148.2024.00109</a>.
    </div>
    </li>
    <li>
    <div id="ref-Jin2023" class="csl-entry" role="listitem">
      Jin, Xianhao, and Francisco Servant. 2023. <span>"<span>HybridCISave</span>: <span>A Combined
      Build</span> and <span>Test Selection Approach</span> in <span>Continuous
      Integration</span>."</span> <em>ACM Transactions on Software Engineering and Methodology</em>
      32 (4): 1-39. <a href="https://doi.org/10.1145/3576038">https://doi.org/10.1145/3576038</a>.
    </div>
    </li>
    <li>
    <div id="ref-keenan2019" class="csl-entry" role="listitem">
      Keenan, James. 2019. <span>"James <span>E</span>. <span>Keenan</span> -
      "<span>Multisection</span>: <span>When Bisection Isn</span>'t <span>Enough</span> to
      <span>Debug</span> a <span>Problem</span>"."</span> <a href=
      "https://www.youtube.com/watch?v=05CwdTRt6AM">https://www.youtube.com/watch?v=05CwdTRt6AM</a>.
    </div>
    </li>
    <li>
    <div id="ref-Leon2003" class="csl-entry" role="listitem">
      Leon, D., and A. Podgurski. 2003. <span>"A Comparison of Coverage-Based and
      Distribution-Based Techniques for Filtering and Prioritizing Test Cases."</span> In <em>14th
      <span>International Symposium</span> on <span>Software Reliability Engineering</span>, 2003.
      <span>ISSRE</span> 2003.</em>, 2003-Janua:442-53. IEEE. <a href=
      "https://doi.org/10.1109/ISSRE.2003.1251065">https://doi.org/10.1109/ISSRE.2003.1251065</a>.
    </div>
    </li>
    <li>
    <div id="ref-Leong2019" class="csl-entry" role="listitem">
      Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John Micco. 2019.
      <span>"Assessing <span>Transition-Based Test Selection Algorithms</span> at
      <span>Google</span>."</span> In <em>2019 <span>IEEE</span>/<span>ACM</span> 41st
      <span>International Conference</span> on <span>Software Engineering</span>: <span>Software
      Engineering</span> in <span>Practice</span> (<span>ICSE-SEIP</span>)</em>, 101-10. IEEE.
      <a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00019">https://doi.org/10.1109/ICSE-SEIP.2019.00019</a>.
    </div>
    </li>
    <li>
    <div id="ref-Liu2023" class="csl-entry" role="listitem">
      Liu, Bohan, He Zhang, Weigang Ma, Gongyuan Li, Shanshan Li, and Haifeng Shen. 2023.
      <span>"The <span>Why</span>, <span>When</span>, <span>What</span>, and <span>How About
      Predictive Continuous Integration</span>: <span>A Simulation-Based
      Investigation</span>."</span> <em>IEEE Transactions on Software Engineering</em> 49 (12):
      5223-49. <a href=
      "https://doi.org/10.1109/TSE.2023.3330510">https://doi.org/10.1109/TSE.2023.3330510</a>.
    </div>
    </li>
    <li>
    <div id="ref-Machalica2019" class="csl-entry" role="listitem">
      Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra. 2019.
      <span>"Predictive <span>Test Selection</span>."</span> In <em>2019
      <span>IEEE</span>/<span>ACM</span> 41st <span>International Conference</span> on
      <span>Software Engineering</span>: <span>Software Engineering</span> in <span>Practice</span>
      (<span>ICSE-SEIP</span>)</em>, 91-100. IEEE. <a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00018">https://doi.org/10.1109/ICSE-SEIP.2019.00018</a>.
    </div>
    </li>
    <li>
    <div id="ref-Memon2017" class="csl-entry" role="listitem">
      Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob Siemborski, and John
      Micco. 2017. <span>"Taming <span class="nocase">Google-scale</span> Continuous
      Testing."</span> In <em>2017 <span>IEEE</span>/<span>ACM</span> 39th <span>International
      Conference</span> on <span>Software Engineering</span>: <span>Software Engineering</span> in
      <span>Practice Track</span> (<span>ICSE-SEIP</span>)</em>, 233-42. Piscataway, NJ, USA: IEEE.
      <a href=
      "https://doi.org/10.1109/ICSE-SEIP.2017.16">https://doi.org/10.1109/ICSE-SEIP.2017.16</a>.
    </div>
    </li>
    <li>
    <div id="ref-Micco2012" class="csl-entry" role="listitem">
      Micco, John. 2012. <span>"Tools for <span>Continuous Integration</span> at <span>Google
      Scale</span>."</span> Tech {{Talk}}. Google NYC. <a href=
      "https://youtu.be/KH2_sB1A6lA">https://youtu.be/KH2_sB1A6lA</a>.
    </div>
    </li>
    <li>
    <div id="ref-Micco2013" class="csl-entry" role="listitem">
      ---. 2013. <span>"Continuous <span>Integration</span> at <span>Google Scale</span>."</span>
      Lecture. EclipseCon 2013. <a href=
      "https://web.archive.org/web/20140705215747/https://%20www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-%2003-24%20Continuous%20Integration%20at%20Google%20Scale.pdf">
      https://web.archive.org/web/20140705215747/https://
      www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
      03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf</a>.
    </div>
    </li>
    <li>
    <div id="ref-Mondal2015" class="csl-entry" role="listitem">
      Mondal, Debajyoti, Hadi Hemmati, and Stephane Durocher. 2015. <span>"Exploring Test Suite
      Diversification and Code Coverage in Multi-Objective Test Case Selection."</span> <em>2015
      IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015
      - Proceedings</em>, 1-10. <a href=
      "https://doi.org/10.1109/ICST.2015.7102588">https://doi.org/10.1109/ICST.2015.7102588</a>.
    </div>
    </li>
    <li>
    <div id="ref-Musa2015" class="csl-entry" role="listitem">
      Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi Baharom. 2015.
      <span>"Regression <span>Test Cases</span> Selection for <span>Object-Oriented Programs</span>
      Based on <span>Affected Statements</span>."</span> <em>International Journal of Software
      Engineering and Its Applications</em> 9 (10): 91-108. <a href=
      "https://doi.org/10.14257/ijseia.2015.9.10.10">https://doi.org/10.14257/ijseia.2015.9.10.10</a>.
    </div>
    </li>
    <li>
    <div id="ref-Najafi2019a" class="csl-entry" role="listitem">
      Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. <span>"Bisecting Commits and Modeling
      Commit Risk During Testing."</span> In <em>Proceedings of the 2019 27th <span>ACM Joint
      Meeting</span> on <span>European Software Engineering Conference</span> and
      <span>Symposium</span> on the <span>Foundations</span> of <span>Software
      Engineering</span></em>, 279-89. New York, NY, USA: ACM. <a href=
      "https://doi.org/10.1145/3338906.3338944">https://doi.org/10.1145/3338906.3338944</a>.
    </div>
    </li>
    <li>
    <div id="ref-Najafi2019" class="csl-entry" role="listitem">
      Najafi, Armin, Weiyi Shang, and Peter C. Rigby. 2019. <span>"Improving <span>Test
      Effectiveness Using Test Executions History</span>: <span>An Industrial Experience
      Report</span>."</span> In <em>2019 <span>IEEE</span>/<span>ACM</span> 41st
      <span>International Conference</span> on <span>Software Engineering</span>: <span>Software
      Engineering</span> in <span>Practice</span> (<span>ICSE-SEIP</span>)</em>, 213-22. IEEE.
      <a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00031">https://doi.org/10.1109/ICSE-SEIP.2019.00031</a>.
    </div>
    </li>
    <li>
    <div id="ref-Ocariza2022" class="csl-entry" role="listitem">
      Ocariza, Frolin S. 2022. <span>"On the <span>Effectiveness</span> of <span>Bisection</span>
      in <span>Performance Regression Localization</span>."</span> <em>Empirical Software
      Engineering</em> 27 (4): 95. <a href=
      "https://doi.org/10.1007/s10664-022-10152-3">https://doi.org/10.1007/s10664-022-10152-3</a>.
    </div>
    </li>
    <li>
    <div id="ref-pan2022" class="csl-entry" role="listitem">
      Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand. 2022. <span>"Test Case
      Selection and Prioritization Using Machine Learning: A Systematic Literature Review."</span>
      <em>Empirical Software Engineering</em> 27 (2): 29. <a href=
      "https://doi.org/10.1007/s10664-021-10066-6">https://doi.org/10.1007/s10664-021-10066-6</a>.
    </div>
    </li>
    <li>
    <div id="ref-Podgurski1989" class="csl-entry" role="listitem">
      Podgurski, A, and L Clarke. 1989. <span>"The <span>Implications</span> of <span>Program
      Dependencies</span> for <span>Software Testing</span>, <span>Debugging</span>, and
      <span>Maintenance</span>."</span> In <em>Proceedings of the <span>ACM SIGSOFT</span> '89
      <span>Third Symposium</span> on <span>Software Testing</span>, <span>Analysis</span>, and
      <span>Verification</span></em>, 168-78. New York, NY, USA: ACM. <a href=
      "https://doi.org/10.1145/75308.75328">https://doi.org/10.1145/75308.75328</a>.
    </div>
    </li>
    <li>
    <div id="ref-Potvin2016" class="csl-entry" role="listitem">
      Potvin, Rachel, and Josh Levenberg. 2016. <span>"Why Google Stores Billions of Lines of Code
      in a Single Repository."</span> <em>Communications of the ACM</em> 59 (7): 78-87. <a href=
      "https://doi.org/10.1145/2854146">https://doi.org/10.1145/2854146</a>.
    </div>
    </li>
    <li>
    <div id="ref-Rodriguez-Perez2018" class="csl-entry" role="listitem">
      Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona. 2018.
      <span>"Reproducibility and Credibility in Empirical Software Engineering: <span>A</span> Case
      Study Based on a Systematic Literature Review of the Use of the <span>SZZ</span>
      Algorithm."</span> <em>Information and Software Technology</em> 99 (July): 164-76. <a href=
      "https://doi.org/10.1016/j.infsof.2018.03.009">https://doi.org/10.1016/j.infsof.2018.03.009</a>.
    </div>
    </li>
    <li>
    <div id="ref-Rothermel1997" class="csl-entry" role="listitem">
      Rothermel, Gregg, and Mary Jean Harrold. 1997. <span>"A Safe, Efficient Regression Test
      Selection Technique."</span> <em>ACM Transactions on Software Engineering and
      Methodology</em> 6 (2): 173-210. <a href=
      "https://doi.org/10.1145/248233.248262">https://doi.org/10.1145/248233.248262</a>.
    </div>
    </li>
    <li>
    <div id="ref-Sadowski2018" class="csl-entry" role="listitem">
      Sadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and Alberto Bacchelli. 2018.
      <span>"Modern Code Review: A Case Study at Google."</span> In <em>Proceedings of the 40th
      <span>International Conference</span> on <span>Software Engineering</span>: <span>Software
      Engineering</span> in <span>Practice</span></em>, 181-90. Gothenburg Sweden: ACM. <a href=
      "https://doi.org/10.1145/3183519.3183525">https://doi.org/10.1145/3183519.3183525</a>.
    </div>
    </li>
    <li>
    <div id="ref-Saha2017" class="csl-entry" role="listitem">
      Saha, Ripon, and Milos Gligoric. 2017. <span>"Selective <span>Bisection
      Debugging</span>."</span> In <em>Fundamental <span>Approaches</span> to <span>Software
      Engineering</span></em>, edited by Marieke Huisman and Julia Rubin, 10202:60-77. Berlin,
      Heidelberg: Springer Berlin Heidelberg. <a href=
      "https://doi.org/10.1007/978-3-662-54494-5_4">https://doi.org/10.1007/978-3-662-54494-5_4</a>.
    </div>
    </li>
    <li>
    <div id="ref-Saidani2020" class="csl-entry" role="listitem">
      Saidani, Islem, Ali Ouni, Moataz Chouchen, and Mohamed Wiem Mkaouer. 2020. <span>"Predicting
      Continuous Integration Build Failures Using Evolutionary Search."</span> <em>Information and
      Software Technology</em> 128 (December): 106392. <a href=
      "https://doi.org/10.1016/j.infsof.2020.106392">https://doi.org/10.1016/j.infsof.2020.106392</a>.
    </div>
    </li>
    <li>
    <div id="ref-Saidani2022" class="csl-entry" role="listitem">
      Saidani, Islem, Ali Ouni, and Mohamed Wiem Mkaouer. 2022a. <span>"Improving the Prediction of
      Continuous Integration Build Failures Using Deep Learning."</span> <em>Automated Software
      Engineering</em> 29 (1): 21. <a href=
      "https://doi.org/10.1007/s10515-021-00319-5">https://doi.org/10.1007/s10515-021-00319-5</a>.
    </div>
    </li>
    <li>
    <div id="ref-Saidani2022a" class="csl-entry" role="listitem">
      ---. 2022b. <span>"Detecting <span>Continuous Integration Skip Commits Using Multi-Objective
      Evolutionary Search</span>."</span> <em>IEEE Transactions on Software Engineering</em> 48
      (12): 4873-91. <a href=
      "https://doi.org/10.1109/TSE.2021.3129165">https://doi.org/10.1109/TSE.2021.3129165</a>.
    </div>
    </li>
    <li>
    <div id="ref-Singh2012" class="csl-entry" role="listitem">
      Singh, Yogesh, Arvinder Kaur, Bharti Suri, and Shweta Singhal. 2012. <span>"Systematic
      Literature Review on Regression Test Prioritization Techniques."</span> <em>Informatica
      (Slovenia)</em> 36 (4): 379-408. <a href=
      "https://doi.org/10.31449/inf.v36i4.420">https://doi.org/10.31449/inf.v36i4.420</a>.
    </div>
    </li>
    <li>
    <div id="ref-Sliwerski2005" class="csl-entry" role="listitem">
      Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005. <span>"When Do Changes Induce
      Fixes?"</span> <em>ACM SIGSOFT Software Engineering Notes</em> 30 (4): 1. <a href=
      "https://doi.org/10.1145/1082983.1083147">https://doi.org/10.1145/1082983.1083147</a>.
    </div>
    </li>
    <li>
    <div id="ref-Sun2024" class="csl-entry" role="listitem">
      Sun, Gengyi, Sarra Habchi, and Shane McIntosh. 2024. <span>"<span>RavenBuild</span>:
      <span>Context</span>, <span>Relevance</span>, and <span>Dependency Aware Build Outcome
      Prediction</span>."</span> <em>Proceedings of the ACM on Software Engineering</em> 1 (FSE):
      996-1018. <a href="https://doi.org/10.1145/3643771">https://doi.org/10.1145/3643771</a>.
    </div>
    </li>
    <li>
    <div id="ref-Wang2024" class="csl-entry" role="listitem">
      Wang, Guoqing, Zeyu Sun, Yizhou Chen, Yifan Zhao, Qingyuan Liang, and Dan Hao. 2024.
      <span>"Commit <span>Artifact Preserving Build Prediction</span>."</span> In <em>Proceedings
      of the 33rd <span>ACM SIGSOFT International Symposium</span> on <span>Software Testing</span>
      and <span>Analysis</span></em>, 1236-48. Vienna Austria: ACM. <a href=
      "https://doi.org/10.1145/3650212.3680356">https://doi.org/10.1145/3650212.3680356</a>.
    </div>
    </li>
    <li>
    <div id="ref-Wang2021" class="csl-entry" role="listitem">
      Wang, Kaiyuan, Daniel Rall, Greg Tener, Vijay Gullapalli, Xin Huang, and Ahmed Gad. 2021.
      <span>"Smart <span>Build Targets Batching Service</span> at <span>Google</span>."</span> In
      <em>2021 <span>IEEE</span>/<span>ACM</span> 43rd <span>International Conference</span> on
      <span>Software Engineering</span>: <span>Software Engineering</span> in <span>Practice</span>
      (<span>ICSE-SEIP</span>)</em>, 160-69. Madrid, ES: IEEE. <a href=
      "https://doi.org/10.1109/ICSE-SEIP52600.2021.00025">https://doi.org/10.1109/ICSE-SEIP52600.2021.00025</a>.
    </div>
    </li>
    <li>
    <div id="ref-Wang2020" class="csl-entry" role="listitem">
      Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and Daniel Rall. 2020.
      <span>"Scalable Build Service System with Smart Scheduling Service."</span> In
      <em>Proceedings of the 29th <span>ACM SIGSOFT International Symposium</span> on
      <span>Software Testing</span> and <span>Analysis</span></em>, 452-62. Virtual Event USA: ACM.
      <a href=
      "https://doi.org/10.1145/3395363.3397371">https://doi.org/10.1145/3395363.3397371</a>.
    </div>
    </li>
    <li>
    <div id="ref-Wen2019" class="csl-entry" role="listitem">
      Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi Cheung, and Zhendong
      Su. 2019. <span>"Exploring and Exploiting the Correlations Between Bug-Inducing and
      Bug-Fixing Commits."</span> In <em>Proceedings of the 2019 27th <span>ACM Joint
      Meeting</span> on <span>European Software Engineering Conference</span> and
      <span>Symposium</span> on the <span>Foundations</span> of <span>Software
      Engineering</span></em>, 326-37. Tallinn Estonia: ACM. <a href=
      "https://doi.org/10.1145/3338906.3338962">https://doi.org/10.1145/3338906.3338962</a>.
    </div>
    </li>
    <li>
    <div id="ref-Zeng2024" class="csl-entry" role="listitem">
      Zeng, Zhili, Tao Xiao, Maxime Lamothe, Hideaki Hata, and Shane Mcintosh. 2024. <span>"A
      <span>Mutation-Guided Assessment</span> of <span>Acceleration Approaches</span> for
      <span>Continuous Integration</span>: <span>An Empirical Study</span> of
      <span>YourBase</span>."</span> In <em>Proceedings of the 21st <span>International
      Conference</span> on <span>Mining Software Repositories</span></em>, 556-68. Lisbon Portugal:
      ACM. <a href=
      "https://doi.org/10.1145/3643991.3644914">https://doi.org/10.1145/3643991.3644914</a>.
    </div>
    </li>
    <li>
    <div id="ref-Zhou2010" class="csl-entry" role="listitem">
      Zhou, Zhi Quan. 2010. <span>"Using Coverage Information to Guide Test Case Selection in
      <span>Adaptive Random Testing</span>."</span> <em>Proceedings - International Computer
      Software and Applications Conference</em>, 208-13. <a href=
      "https://doi.org/10.1109/COMPSACW.2010.43">https://doi.org/10.1109/COMPSACW.2010.43</a>.
    </div>
    </li>
    <li>
    <div id="ref-Ziftci2013a" class="csl-entry" role="listitem">
      Ziftci, Celal, and Vivek Ramavajjala. 2013. <span>"Finding <span>Culprits
      Automatically</span> in <span>Failing Builds</span> - i.e. <span>Who Broke</span> the
      <span>Build</span>?"</span> <a href=
      "https://www.youtube.com/watch?v=SZLuBYlq3OM">https://www.youtube.com/watch?v=SZLuBYlq3OM</a>.
    </div>
    </li>
    <li>
    <div id="ref-Ziftci2017" class="csl-entry" role="listitem">
      Ziftci, Celal, and Jim Reardon. 2017. <span>"Who Broke the Build? <span>Automatically</span>
      Identifying Changes That Induce Test Failures in Continuous Integration at Google
      Scale."</span> <em>Proceedings - 2017 IEEE/ACM 39th International Conference on Software
      Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017</em>, 113-22. <a href=
      "https://doi.org/10.1109/ICSE-SEIP.2017.13">https://doi.org/10.1109/ICSE-SEIP.2017.13</a>.
    </div>
    </li>
  </div>
  </ul>
  <section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
    <hr />
    <ol>
      <li id="fn1">
        <p><a href="https://bazel.build/" class="uri">https://bazel.build/</a><a href="#fnref1"
        class="footnote-back" role="doc-backlink">↩︎</a></p>
      </li>
      <li id="fn2">
        <p>When a target breaks we perform culprit finding. The conclusion of the culprit finding
        is the culprit commit. We colloquially say the target is blaming the culprit.<a href=
        "#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p>
      </li>
    </ol>
  </section>
