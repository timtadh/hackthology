<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hackthology</title><link>http://hackthology.com/</link><description></description><atom:link href="http://hackthology.com/feeds/rss.xml" rel="self"></atom:link><lastBuildDate>Wed, 13 Nov 2013 00:00:00 -0500</lastBuildDate><item><title>Linear Hashing</title><link>http://hackthology.com/linear-hashing.html</link><description>&lt;p&gt;Tonight I am giving a talk&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; on Linear Hashing,&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt; a hash table suitable
for secondary storage. It is often used to implement hash indices in databases
and file systems. Linear Hashing was invented by Witold Litwin in 1980 and has
been in widespread use since that time. I implemented this file-structure
earlier this year. You can find &lt;a href="https://github.com/timtadh/file-structures/blob/master/linhash"&gt;my implementation on
github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hopefully this essay gives you a taste for the essentials of Linear Hashing.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Linear Hash" src="/images/lin-hash.png" /&gt;&lt;/p&gt;
&lt;h1&gt;Hashing, A Refresher&lt;/h1&gt;
&lt;p&gt;In order to understand Linear Hashing one should take a moment to review
Classical Hashing.&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; Briefly, a hash table is a "symbol table" which maps keys
to values. By "maps" I mean if you give it a key it will give the associated
value if one exists. Keys must be hashable, which means there must be some way
to turn them into integers. For a discussion on constructing such hash functions
I recommend the Hashing Tutorial.&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Abstract Data Type&lt;/h3&gt;
&lt;p&gt;If the above description was confusing perhaps this ADT will clarify what I
mean:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;HashTable&lt;/span&gt;
  &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt; &lt;span class="n"&gt;many&lt;/span&gt; &lt;span class="n"&gt;entries&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt;
  &lt;span class="n"&gt;has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt;
  &lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;KeyNotFound&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;associated&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
  &lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;associate&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;


  &lt;span class="n"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;KeyNotFound&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;remove&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;Hashable&lt;/span&gt;
  &lt;span class="n"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You may have used hash tables by another name in you programming language. For
instance in Python they are called dictionaries, in Ruby hashs, and in Java they
are called HashMaps.&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let's walk through a simple hash table implementation using separate chaining
(also called open hashing). We will do this in the Go programming language. &lt;/p&gt;
&lt;h3&gt;Structs&lt;/h3&gt;
&lt;p&gt;Here is how we are going to represent a hash table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;type&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;Equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt;
    &lt;span class="nx"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;type&lt;/span&gt; &lt;span class="nx"&gt;entry&lt;/span&gt; &lt;span class="kd"&gt;struct&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;


    &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="nx"&gt;next&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;type&lt;/span&gt; &lt;span class="nx"&gt;hash&lt;/span&gt; &lt;span class="kd"&gt;struct&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;table&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;


    &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A &lt;code&gt;hash&lt;/code&gt; is a struct with to elements. An array of pointers to &lt;code&gt;entry&lt;/code&gt;. The
entries hold our key value pairs. The way the table works is we convert the key
into a number which we then clamp to the size of our table. That number will be
the index of some entry in our table from which we can add, lookup, or remove
the key.&lt;/p&gt;
&lt;p&gt;The entry struct represents the key value pair and represents a linked list.
Since covering linked list operations is a bit beyond the scope of this paper,
let me just present operations on the &lt;code&gt;*entry&lt;/code&gt; but with no explanation. The
function should be obvious even if the implementation is obscure.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{})&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;has&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{})&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;



&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nb"&gt;panic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Errors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;list-not-found&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Operations on the Hash Table&lt;/h2&gt;
&lt;p&gt;Now for how to implement the different operations. As a reminder we are going to
convert our key to an index into the table as our first step. Let's make a
function for that and call it &lt;code&gt;bucket&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;


    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Insertion&lt;/h4&gt;
&lt;p&gt;Putting an object into a hash table is very simple. We grab the bucket and use
the associate &lt;code&gt;Put&lt;/code&gt; method to place our key value pair into the list. If it was
actually appended onto the list (rather than updating and existing entry) we
increment the &lt;code&gt;size&lt;/code&gt; field.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{})&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt;


    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now there is one more wrinkle I will return to in a moment which is resizing the
table when it gets too full.&lt;/p&gt;
&lt;h4&gt;Retrieval&lt;/h4&gt;
&lt;p&gt;Retrieval is just as easy. We grab the bucket and look in the linked list to see
if it is there or not. If it is, return it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;has&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;Get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="nx"&gt;has&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;Errors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;not-found&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Removal&lt;/h4&gt;
&lt;p&gt;Removal is almost the same as insertion except we call &lt;code&gt;Remove&lt;/code&gt; on the linked
list instead of &lt;code&gt;Put&lt;/code&gt; and update the head as before. We check to make sure it
is in the linked list first as this slightly simplifies the removal algorithm
above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;has&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;Get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;has&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;Errors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;not-found&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;Remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Expansion&lt;/h4&gt;
&lt;p&gt;The performance of a hash table degrades as it gets too full. Therefore, we have
to periodically expand the size of the hash table. As long we double the size
each time all of our operations are asymptotically linear (on average). To
double the size of the table, we allocate a new table and copy all of the
entries from the old table to the new. We must be careful when we do this and
rehash each element. If we don't, we will be unable to find the elements in the
new table since the &lt;code&gt;bucket&lt;/code&gt; function depends on the table size.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;table&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;

    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;make&lt;/span&gt;&lt;span class="p"&gt;([]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;E&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="k"&gt;range&lt;/span&gt; &lt;span class="nx"&gt;table&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;E&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;next&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Insert Revisited&lt;/h4&gt;
&lt;p&gt;So when should we expand the hash table? We should expand on insert when the
number of elements is over a certain threshold. The threshold is often set at
60% but this setting varies.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="p"&gt;{})&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;appended&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Adapting Hash Tables for Secondary Storage&lt;/h1&gt;
&lt;p&gt;&lt;a href="/images/cpu-arch.png"&gt;&lt;img alt="CPU and Storage" src="/images/cpu-arch.png" /&gt;&lt;/a&gt;
Figure 1. &lt;strong&gt;CPU and Storage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, the algorithm presented above does work well when using secondary
storage mediums like hard disks and solid state drives. There are several
reasons for this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Secondary Storage is slower than RAM&lt;/li&gt;
&lt;li&gt;The bus is slower&lt;/li&gt;
&lt;li&gt;Many peripherals hang off of the South Bridge&lt;/li&gt;
&lt;li&gt;Disks may be daisy chained causing bus contention&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To deal with these factors and others when using disks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read and write pages which are blocks of size 4096 bytes.&lt;/li&gt;
&lt;li&gt;Try and read contiguous runs and if writing more than one page write
   contiguous runs as well.&lt;/li&gt;
&lt;li&gt;Batch writes.&lt;/li&gt;
&lt;li&gt;Don't read one byte at a time, read several blocks and get the byte that you
   need.&lt;/li&gt;
&lt;li&gt;Employ caching at every layer.&lt;/li&gt;
&lt;li&gt;Measure performance in terms of number of disk accesses (eg. Block read and
   writes).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Back to Hashing&lt;/h2&gt;
&lt;p&gt;The first adjustment to make is to hash into blocks instead of hashing into
individual array buckets. Each block is then a sorted array of entries.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/block-file.png"&gt;&lt;img alt="Block File" src="/images/block-file.png" /&gt;&lt;/a&gt;
Figure 2. &lt;strong&gt;Block File&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We could make a fairly straight forward adaption of our separate chained hash
table above to this restriction. However, there is a problem: what do we do when
the table needs to be expanded? If the table is static then there is not
problem, we simply allocate the correct number of blocks right away. But, if we
have to expand the table every entry will need to be rehashed. This will cause
us to read from every block from our old table (N reads) and write to every
block in our new table (2*N writes) -- ouch.&lt;/p&gt;
&lt;p&gt;The solution is of course Linear Hashing.&lt;/p&gt;
&lt;h2&gt;Linear Hashing&lt;/h2&gt;
&lt;p&gt;How does Linear Hashing compare?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Small mean disk accesses&lt;ol&gt;
&lt;li&gt;Successful Search&lt;ol&gt;
&lt;li&gt;.75 utilization ~ 1.05 disk accesses&lt;/li&gt;
&lt;li&gt;.9 utilization ~ 1.35 disk accesses&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Unsuccessful Search&lt;ol&gt;
&lt;li&gt;.75 utilization ~ 1.27 disk accesses&lt;/li&gt;
&lt;li&gt;.9 utilization ~ 2.37 disk accesses&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Insert&lt;ol&gt;
&lt;li&gt;.75 utilization ~ 2.62 disk accesses&lt;/li&gt;
&lt;li&gt;.9 utilization ~ 3.73 disk accesses&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In comparison a B+Tree of reasonable size might need at least 4 disk access
   for a search. (Of course a B+Tree will can perform range queries but that
   isn't the point here)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;File grows at a linear rate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Little dynamic re-arrangement&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does not necessarily need address translation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple Algorithm esp. in comparison to B+Trees.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Explanation of the Algorithm&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;key&lt;/strong&gt; insight is to not use all the bits of the hash function "H(.)" all
the time. When the table is small we only use as much of the hash function as
we need. As the table grows we use more bits. As the table shrinks we use less.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/lin-hash-ex-1.png"&gt;&lt;img alt="Linear Hash" src="/images/lin-hash-ex-1.png" /&gt;&lt;/a&gt;
Figure 3. &lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the figure, &lt;code&gt;n&lt;/code&gt; is the number of blocks, &lt;code&gt;i&lt;/code&gt; is the number of bits of the
hash functions and &lt;code&gt;r&lt;/code&gt; is the number of records.&lt;/p&gt;
&lt;p&gt;So to find which bucket a key goes to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;bkt_idx&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kr"&gt;let&lt;/span&gt;
  &lt;span class="n"&gt;hash&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;a_1&lt;/span&gt; &lt;span class="n"&gt;a_2&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;a_i&lt;/span&gt; &lt;span class="cm"&gt;(* base 2 expansion of the hash of the key *)&lt;/span&gt;
  &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a_1&lt;/span&gt; &lt;span class="n"&gt;a_2&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;a_i&lt;/span&gt; &lt;span class="cm"&gt;(* just the first i bits *)&lt;/span&gt;
&lt;span class="kr"&gt;in&lt;/span&gt;
  &lt;span class="kr"&gt;if&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="kr"&gt;then&lt;/span&gt;
    &lt;span class="n"&gt;m&lt;/span&gt;
  &lt;span class="kr"&gt;else&lt;/span&gt;
    &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;^&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="cm"&gt;(* == 0 a_2 a_3 ... a_i *)&lt;/span&gt;
&lt;span class="kr"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In go&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt; &lt;span class="kt"&gt;uint&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kt"&gt;uint&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;m&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;hash&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;// last i bits of hash as&lt;/span&gt;
                           &lt;span class="c1"&gt;// bucket number m&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;m&lt;/span&gt; &lt;span class="p"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;n&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;m&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;m&lt;/span&gt; &lt;span class="p"&gt;^&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;// unset the top bit&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Insertion&lt;/h4&gt;
&lt;p&gt;Insertion is quite simple now that we know how to get the bucket (assuming we
have implemented the appropriate operations on our buckets). First we get the
bucket and we put the item into the bucket. If the bucket takes care of chaining
on an extra block if it full then the only thing that is left is checking
whether or not an expansion (called a split) is needed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="nx"&gt;LinearHash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;Insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt; &lt;span class="nx"&gt;Hashable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;hash&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nx"&gt;bkt_idx&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;bkt&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bkt_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;bkt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;r&lt;/span&gt; &lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;UTILIZATION&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;records_per_block&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As I mentioned above, if a bucket is full it should chain out an extra block for
itself. This can be handled transparently.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/lin-hash-ex-2.png"&gt;&lt;img alt="Linear Hash" src="/images/lin-hash-ex-2.png" /&gt;&lt;/a&gt;
Figure 4. &lt;strong&gt;Chaining Example&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;Splitting&lt;/h4&gt;
&lt;p&gt;The split mechanism is clever bit of the linear hash algorithm. When the table
is too full another block is added to the table:&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/lin-hash-ex-3.png"&gt;&lt;img alt="Linear Hash" src="/images/lin-hash-ex-3.png" /&gt;&lt;/a&gt;
Figure 5. &lt;strong&gt;Split Example Part 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note that the bucket we added in the example was &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;a_i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;a_1&lt;/span&gt; &lt;span class="n"&gt;a_2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are some keys in the old bucket &lt;code&gt;0&lt;/code&gt; which is now called &lt;code&gt;00&lt;/code&gt; which
actually belong to bucket &lt;code&gt;10&lt;/code&gt;. So in order to make the addition of the new
bucket correct we need to split bucket &lt;code&gt;00&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/lin-hash-ex-4.png"&gt;&lt;img alt="Linear Hash" src="/images/lin-hash-ex-4.png" /&gt;&lt;/a&gt;
Figure 5. &lt;strong&gt;Split Example Part 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In general if we add&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;a_2&lt;/span&gt; &lt;span class="n"&gt;a_3&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;a_i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We split&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;a_2&lt;/span&gt; &lt;span class="n"&gt;a_3&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;a_i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In code&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt; &lt;span class="nx"&gt;LinearHash&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="kt"&gt;error&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;bkt_idx&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nx"&gt;bkt_a&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_bucket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bkt_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;bkt_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;allocate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;nil&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;n&lt;/span&gt; &lt;span class="p"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;blk_a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;split_into&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bkt_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;// The split into function is left as&lt;/span&gt;
    &lt;span class="c1"&gt;// an exercise for the reader!&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Hopefully if you are still with me you have a grasp on what is going on in the
Linear Hashing algorithm. The key take aways are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You can slowly extend how much of the hash function you use.&lt;/li&gt;
&lt;li&gt;You don't have to rehash the whole file to add a bucket, just the bucket that
   collides with the new bucket.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you want to implement the algorithm I suggest reading the description in the
Garcia-Molina book and taking a look at the original paper. You can also take a
look at &lt;a href="https://github.com/timtadh/file-structures/blob/master/linhash"&gt;my implementation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Henderson, T. A. D. (2013)
&lt;a href="http://hackthology.com/pdfs/2013-11-13-linear-hashing-lecture.pdf"&gt;Linear Virtual Hashing&lt;/a&gt;.
CWRU Hacker Society. Lecture Notes. November, 2013.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Litwin, W. (1980).
&lt;a href="http://hackthology.com/pdfs/Litwin-1980-Linear_Hashing.pdf"&gt;Linear hashing: a new tool for file and table addressing&lt;/a&gt;. In Proceedings of the sixth
international conference on Very Large Data Bases - Volume 6 (pp.  212223).
VLDB Endowment. Retrieved from
&lt;a href="http://dl.acm.org/citation.cfm?id=1286887.1286911"&gt;http://dl.acm.org/citation.cfm?id=1286887.1286911&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Garcia-Molina, H., Ullman J. D., and Widom J. (2002)
&lt;a href="http://www.worldcat.org/title/database-systems-the-complete-book/oclc/47915796"&gt;Database Systems: The Complete Book&lt;/a&gt;.
Prentice Hall. Upper Saddle River, New Jersey. ISBN 0-13-031995-3. Section
13.4.7 "Linear Hash Tables"&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Shaffer, C. A. (2007)
&lt;a href="http://research.cs.vt.edu/AVresearch/hashing/"&gt;Hashing Tutorial&lt;/a&gt;
Virginia Tech Algorithm Research Group.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Ok, I am not that mean here is how to do the split_into in this file:
&lt;a href="https://github.com/timtadh/file-structures/blob/master/linhash/bucket/bucket.go"&gt;https://github.com/timtadh/file-structures/blob/master/linhash/bucket/bucket.go&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Henderson, T. A. D. (2013) &lt;a href="https://github.com/timtadh/file-structures/blob/master/linhash"&gt;Linear Hash
Implementation&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Wed, 13 Nov 2013 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2013-11-13:linear-hashing.html</guid></item><item><title>Cryptography and Complexity</title><link>http://hackthology.com/cryptography-and-complexity.html</link><description>&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct or the bibliography you should read the
&lt;a href="http://hackthology.com/pdfs/crypto-complexity.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cite as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Henderson, Tim A. D. &lt;strong&gt;Cryptography and Complexity&lt;/strong&gt;. Unpublished. Case Western Reserve University. MATH 408.  Spring 2012.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;hr/&gt;
&lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Modern cryptographic systems are built on problems which are assumed to be computationally infeasible. Computational infeasibility means a computation which although computable would take far too many resources to actually compute. Ideally in cryptography one would like to ensure an infeasible computation&amp;#8217;s cost is greater than the reward obtained by computing it. At first glance this seems to be an odd notion to base a cryptographic system on. Don&amp;#8217;t we want our cryptographic systems to be totally secure? They should be unbreakable! &amp;#8220;It may take a long time to break it,&amp;#8221; seems like a poor guarantee of security.&lt;/p&gt;

&lt;p&gt;However, it is the best guarantee which can exist in either an ideal world (from a mathematical perspective) or the physical world. As we shall see later in the survey, if several widely held assumptions turn out to be false we can not even make the guarantee of computational infeasibility.&lt;/p&gt;

&lt;h2&gt;Classical Security&lt;/h2&gt;

&lt;p&gt;In classical cryptographic systems, those known to the academic community prior to the publication of Diffie and Hellman&amp;#8217;s paper &lt;span class="citation"&gt;&lt;/span&gt;, security assumptions were based on the results of information theory. This approach is sometimes referred to as &lt;em&gt;information-theoretic&lt;/em&gt; and is concerned with whether there exists information in the &lt;em&gt;ciphertext&lt;/em&gt; which originated in the &lt;em&gt;plaintext&lt;/em&gt; or in the &lt;em&gt;key&lt;/em&gt;. We say a system has &lt;em&gt;perfect-secrecy&lt;/em&gt; if:&lt;/p&gt;

&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
  \forall_{m \in \mathcal{M}} \forall_{c \in \mathcal{C}} \text{ : } 
  Pr[\mathcal{M} = m | \mathcal{C} = c] = Pr[\mathcal{M} = m]
  \label{perfect-secrecy}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Intuitively this formula says an attacker gains no information about the contents of a message from the ciphertext of the message. Does this mean the attacker knows nothing about the message? Of course not! However, he doesn&amp;#8217;t &lt;em&gt;learn&lt;/em&gt; anything new about the message by closely examining the ciphertext. Therefore, the ciphertext of the message is essentially useless to an attacker. &lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn1" class="footnoteRef" id="fnref1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;However, any system with &lt;em&gt;perfect-secrecy&lt;/em&gt; requires the length of the key to be at least as large as the sum of the lengths of all messages encrypted with it. Since the key has to be at least as long as the messages sent such a system is of little value in practical modern situations.&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn2" class="footnoteRef" id="fnref2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;There are two practical problems with a system with &lt;em&gt;perfect-secrecy&lt;/em&gt; the first is &amp;#8220;Key Distribution.&amp;#8221; Since the sender and receiver must use the same key they must some how &lt;em&gt;secretly&lt;/em&gt; agree on a key beforehand. Therefore, there must exist some &amp;#8220;second channel&amp;#8221; by which the sender and receiver can communicate. The second problem has to do with the length of the key. Since it is as long as the message there seems to be only small utility in the system as the sender and receiver could conceivably securely exchange messages using their secure &amp;#8220;second channel&amp;#8221; they use for key distribution.&lt;/p&gt;

&lt;p&gt;These to problems make the system an unrealistic system for securing (for instance) internet communications. Internet communications do not require the parties to know each other before hand and allow for no secondary secure communications channel to exist. Therefore, some other encryption methodology must be used if one wants to secure communication in this setting.&lt;/p&gt;

&lt;h2&gt;Modern Security&lt;/h2&gt;

&lt;p&gt;In modern system we no longer discuss security in terms of whether a system provides &lt;em&gt;perfect-secrecy&lt;/em&gt;. Instead, we say if a ciphertext contains information leaked from the plaintext it should be computationally infeasible to extract that information. We can provide this property even in cases where the key is shorter than the message. This property can also be provided in cases where the attacker has access to the &lt;em&gt;encryption&lt;/em&gt; key (but not of course to the &lt;em&gt;decryption&lt;/em&gt; key).&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn3" class="footnoteRef" id="fnref3"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;In following section [secure-encryption] we shall unpack and rigorously define (using &lt;span class="citation"&gt;&lt;/span&gt;&amp;#8217;s definitions) the definition above. In particular we will look at the definition in the context of symmetric key systems. The difficulties of public key systems will also be briefly presented but without detailed exposition. However, before we get to the fun stuff we will first present complexity theory and one way functions.&lt;/p&gt;

&lt;h1&gt;A Tour of Computational Complexity Theory&lt;/h1&gt;

&lt;p&gt;In many ways Computability Theory, and its daughter field Complexity Theory, began with proof of the incompleteness of axiomatic systems in 1931.&lt;span class="citation"&gt;&lt;/span&gt; The proof is a tremendously important result in meta-mathematics stating: no recursively axiomatized mathematical system can be both complete and consistent. Thus, we cannot prove in a particular theory that the same particular theory is consistent. Indeed, if we did construct such a proof it would prove exactly the opposite. Thus, there are some mathematical sentences which are true for which no algorithm can decided on their truth value.&lt;/p&gt;

&lt;p&gt;In a similar result, Alan Turing in 1937 proved that for general programs one could not decide whether the programs would halt for all inputs.&lt;span class="citation"&gt;&lt;/span&gt; During the previous year Alonzo Church proved the exact same thing for evaluations of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;-Calculus expressions.&lt;span class="citation"&gt;&lt;/span&gt; Church and Turing later conjectured that the machine Turing defined (eg. Turing Machines) and Church&amp;#8217;s lambda calculus were equivalent. All though this is an unprovable conjecture it is largely accepted today.&lt;/p&gt;

&lt;p&gt;Once actual computational machines were produced (as opposed to the abstract machines of Turing and Church), programmers became interested in the notion of the &lt;em&gt;complexity&lt;/em&gt; of an algorithm. The complexity of an algorithm is an expression of how much time or space or other resources the algorithm will use. The representation of time and space is abstract and placed in terms of the size of the parameters to the algorithm. Today, we use asymptotic notation to express complexity assertions. The notation was standardized by Don Knuth in 1976 but in wide (although inconsistent) use before then. It was invented by Bachmann in 1894 for use in a different context.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The interest in the complexity of algorithms and work on linguistics (particularly formal language hierarchies) lead to work on classifying the &amp;#8220;hardness&amp;#8221; computational problems. For instance all of the language classes in the Chomsky hierarchy have hardness results. Type-0 Languages (all recursively enumerable languages) are recognizable but only non-deterministically. While, Type-3 languages (referred to as regular languages) can be recognized in linear time.&lt;sup&gt;&lt;a href="#fn4" class="footnoteRef" id="fnref4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This leads to defining complexity classes for problems (as opposed to algorithms). A complexity class typically refers to a bound on the amount time or space needed to solve the problem in the worst case. Thus, complexity classes describe how difficult a problem is to solve in general. The first general results in the theory were obtained in 1965 by Hartmanis and Stearns who defined the meaning computation complexity.&lt;/p&gt;

&lt;p&gt;In particular Hartmanis and Stearns modeled their definition using the computational model of an N-Tape Turing Machine. Any computational model could have been used, and today others are used. In particular the authors prove facts about the computability of particular binary strings, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; in the paper. They say &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is in a complexity class &lt;span class="math"&gt;\(S_T\)&lt;/span&gt; if &lt;span class="math"&gt;\(T: \mathbb{N} \rightarrow \mathbb{N}\)&lt;/span&gt; is a monotone increasing function and there exists a Turing machine &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; such that &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; computes the &lt;span class="math"&gt;\(n\)&lt;/span&gt;th term in &lt;span class="math"&gt;\(T(n)\)&lt;/span&gt; steps.&lt;/p&gt;

&lt;p&gt;What does this definition mean intuitively? Think of &lt;span class="math"&gt;\(T\)&lt;/span&gt; as a time function, where time is a function of the number bits generated. A string can belongs to those complexity classes which can compute the string according to the complexity class&amp;#8217;s specified time function. Thus, by specifying some general time functions such as (&lt;span class="math"&gt;\(1\)&lt;/span&gt;, &lt;span class="math"&gt;\(n\)&lt;/span&gt;, &lt;span class="math"&gt;\(n^2\)&lt;/span&gt; &amp;#8230;, &lt;span class="math"&gt;\(2^n\)&lt;/span&gt;) one can begin classifying bit-strings. The bit-strings correspond to problem solutions. For a simple example consider the bit-string which corresponds to all prime numbers. To compute it, one would need to actually compute exactly those numbers which are prime.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The type of problem Hartmanis and Stearns classified belongs to the class of problems known as Decision Problems. Informally, decision problems are problems to which there is a &amp;#8220;yes/no&amp;#8221; answer. For instance, deciding whether the first &lt;span class="math"&gt;\(n\)&lt;/span&gt; bits of a string is in a language, &lt;span class="math"&gt;\(\mathcal{L} \subseteq \{0,1\}^*\)&lt;/span&gt;, is a decision problem. Complexity classes are more general than just decision problems however, one can construct complexity classes for any type of computational problem, optimizations problems for instance.&lt;/p&gt;

&lt;h2&gt;The Class NP&lt;/h2&gt;

&lt;p&gt;Of particular importance to mathematics, computer science and this paper in particular is the complexity class NP (Non-Deterministic Polynomial Time). The class of NP is defined (intuitively) as those problems which have easily verifiable solutions. What does it mean for a solution to be &amp;#8220;easily verifiable.&amp;#8221; It means given the problem instance and the solution one can check the validity of the solution in &lt;span class="math"&gt;\(O(n^k)\)&lt;/span&gt; where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is parameterized by the problem and &lt;span class="math"&gt;\(k\)&lt;/span&gt; is a constant.&lt;/p&gt;

&lt;p&gt;More formally, the class NP is defined in terms of formal languages. Let &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; be an alphabet and &lt;span class="math"&gt;\(\Sigma_{0}\)&lt;/span&gt; be &lt;span class="math"&gt;\(\Sigma - \{*\}\)&lt;/span&gt; where &lt;span class="math"&gt;\(*\)&lt;/span&gt; is the empty symbol. Let &lt;span class="math"&gt;\(\Sigma_{0}^{*}\)&lt;/span&gt; be the closure of all finite strings made up of symbols in &lt;span class="math"&gt;\(\Sigma_{0}\)&lt;/span&gt;. We define a language, &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, as &lt;span class="math"&gt;\(\mathcal{L} \subseteq \Sigma_{0}^{*}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The Class NP&lt;sup&gt;&lt;a href="#fn5" class="footnoteRef" id="fnref5"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A language, &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, belongs to NP if there exists a Deterministic Turing Machine, &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt;, a polynomial, &lt;span class="math"&gt;\(p(n)\)&lt;/span&gt; &amp;#8211; such that &lt;span class="math"&gt;\(p(n)\)&lt;/span&gt; defines the complexity class of &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn6" class="footnoteRef" id="fnref6"&gt;6&lt;/a&gt;&lt;/sup&gt; &amp;#8211; and on any input &lt;span class="math"&gt;\(x
    \in \Sigma_{0}^{*}\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class="math"&gt;\(x \in \mathcal{L}\)&lt;/span&gt; then there exists a &lt;em&gt;certificate&lt;/em&gt;, &lt;span class="math"&gt;\(y
            \in \Sigma_{0}^{*} \text{ st. } |y| \leq p(|x|)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; accepts the input string &lt;span class="math"&gt;\(xy\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class="math"&gt;\(x \notin \mathcal{L}\)&lt;/span&gt; then for any string, &lt;span class="math"&gt;\(y \in
            \Sigma_{0}^{*}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; rejects the input string &lt;span class="math"&gt;\(xy\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;The given definition does not discuss Non-Determinism. To see the role of Non-Determinism consider constructing solutions (certificates) to problem instances (the string x in the definition). If certificates can be chosen and examined non-deterministically then it will take only polynomial time to find a solution. However, if we are testing every possible certificate deterministically it will take &lt;span class="math"&gt;\(\vert \Sigma_0\vert ^{p(\vert x\vert )}\)&lt;/span&gt; examinations, a combinatorial explosion. Thus, problems in NP have solutions which are easy to verify but not necessarily easy to construct.&lt;/p&gt;

&lt;h2&gt;The Class P&lt;/h2&gt;

&lt;p&gt;The complexity class P (Polynomial Time) is exactly those problems solvable in deterministic polynomial time. More formally,&lt;/p&gt;

&lt;p&gt;The Class P&lt;sup&gt;&lt;a href="#fn7" class="footnoteRef" id="fnref7"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt; be a decision problem. Let &lt;span class="math"&gt;\(L_{\Pi} = \{ x \in \Sigma_0^* |
    \text{x is an encoding of an instance of } \Pi \}\)&lt;/span&gt;, that is, &lt;span class="math"&gt;\(L_{\Pi}\)&lt;/span&gt; is the language of &lt;span class="math"&gt;\(\Pi\)&lt;/span&gt;. We can then define the class P as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      P = \{ L \subseteq \Sigma_0^* |&amp;amp; \text{ there is a Deterministic Turing} 
        \text{ Machine, } \mathcal{M} \text{, and a polynomial, } \\
        &amp;amp;p(n) \text{, } \text{such that } T_{\mathcal{M}} \leq p(n) 
        \text{ for all } n \geq 1 \}
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A language is in P if one can construct a Turing Machine which accepts it (and rejects all non-members) in time less than some polynomial (with respect to the size of the input).&lt;/p&gt;

&lt;p&gt;All those problems which belong to P are considered easily solvable, or tractable. While, they are &amp;#8220;easy&amp;#8221; one should not make the mistake of assuming they are simple. Given a polynomial time algorithm which solves a problem one can easily solve it. However, even if you know a polynomial time algorithm exists for a problem constructing the algorithm may be difficult.&lt;/p&gt;

&lt;h2&gt;P vs.&amp;#160;NP&lt;/h2&gt;

&lt;p&gt;What is the relationship between P and NP? It is known P is contained in NP (ie. &lt;span class="math"&gt;\(P \subseteq NP\)&lt;/span&gt;). However, whether &lt;span class="math"&gt;\(NP \subseteq P\)&lt;/span&gt; is true is one of the greatest open questions in applied mathematics. The class P containment inside of NP is obvious: if we can find a solution in polynomial time it is certainly verifiable in polynomial time. To show NP is contained within P one would need to show every problem in NP can be solved with a polynomial time algorithm.&lt;/p&gt;

&lt;p&gt;The methodology for solving P vs NP with the greatest impact relies on the idea of &lt;em&gt;reduction&lt;/em&gt;. We say problem, &lt;span class="math"&gt;\(\Pi_1\)&lt;/span&gt; is &lt;em&gt;reducible&lt;/em&gt; to another problem, &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt;, if one can find a mapping from every instance of &lt;span class="math"&gt;\(\Pi_1\)&lt;/span&gt; to equivalent instances of &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt; such that the solutions to the constructed instances of &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt; correspond to solutions of &lt;span class="math"&gt;\(\Pi_1\)&lt;/span&gt;. A reduction is a &lt;em&gt;polynomial reduction&lt;/em&gt; if the mapping can be done in polynomial time. A problem, &lt;span class="math"&gt;\(\Pi_1\)&lt;/span&gt;, is as &amp;#8220;hard&amp;#8221; as another problem, &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt;, if &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt; can be &lt;em&gt;reduced&lt;/em&gt; to &lt;span class="math"&gt;\(\Pi_2\)&lt;/span&gt;. Thus, hardness is a relational notion. Problems are not intrinsically hard, they are hard with respect to other problems.&lt;/p&gt;

&lt;p&gt;To prove P contains NP one could prove the hardest problem in NP is also in P. By the definition of hardness above, if a problem, &lt;span class="math"&gt;\(\Pi_h\)&lt;/span&gt;, is the hardest problem in NP than every other problem in NP is reducible to &lt;span class="math"&gt;\(\Pi_h\)&lt;/span&gt;. Problems which are at least as hard as every problem in NP are know as NP-Hard problems. A problem does not need to be in NP to be NP-Hard. However, if a problem is NP-Hard and it is in NP then it is called an NP-Complete problem.&lt;/p&gt;

&lt;p&gt;NP-Complete problems exist and their existence is one of the greatest results in complexity theory. It was proved by Stephen Cook in 1971 who found the first NP-Complete problem. The problem he found is known as SAT (for satisfiability of boolean formulas). He proved any problem solvable in polynomial time by a Nondeterministic Turing Machine can be reduced to finding whether or not a boolean formula is satisfiable.&lt;span class="citation"&gt;&lt;/span&gt; Cook&amp;#8217;s result launched a wave of research. The very next year Richard Karp proved 21 other problems were also NP-Complete.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;While there have many hundreds of problems proven to be NP-Complete since Cook proved SAT, there have been only fruitless attempts to prove P does or does not contain NP. The leading consensus in the complexity community is P does not contain NP. Furthermore, since it appears work on proving P contains NP is permanently stalled one can safely assume NP-Hard problems are in some platonic sense actually difficult to solve.&lt;/p&gt;

&lt;h2&gt;Infeasibility&lt;/h2&gt;

&lt;p&gt;A sharper definition of computational infeasibility can now be given with definitions of Complexity Classes, P, and NP in hand. Recall the opening statement on infeasibility, where we defined an infeasible computation to be one requiring too many resources to actually compute. If one has encrypted a message one would ideally like the ciphertext to be unreadable. If the message is a solution to an NP-Complete problem then the ciphertext could be the problem instance and therefore can only be decrypted by solving the NP-Complete problem. However, assuming NP is not contained in P, the NP-Complete problem will take time proportional to &lt;span class="math"&gt;\(\vert \Sigma_0^*\vert ^{\vert x\vert }\)&lt;/span&gt; (where x is the ciphertext) to solve.&lt;/p&gt;

&lt;p&gt;Therefore, a new working definition of an infeasible computation is a &amp;#8220;hard&amp;#8221; instance of an NP-Hard problem of sufficient size. What is sufficient size? Any size which leads to &lt;span class="math"&gt;\(\vert \Sigma_0^*\vert ^{\vert x\vert }\)&lt;/span&gt; to be so large as to be uncomputable. An example of such as size might be &lt;span class="math"&gt;\(160\)&lt;/span&gt; since trying &lt;span class="math"&gt;\(2^{160}\)&lt;/span&gt; possible solutions is not expected to ever be computable with classical computers in time less than the age of the universe. What is a &amp;#8220;hard&amp;#8221; instance? A hard instance is one in which there exists no better way to find a solution than trying all possible solutions. Not every instance of a hard problem is hard to solve. Specifying an infeasible computation requires a hard instance is a necessary restriction.&lt;/p&gt;

&lt;h2&gt;Probabilistic Infeasibility&lt;/h2&gt;

&lt;p&gt;In the previous section it was assumed all computations were exact. No computation &lt;em&gt;sometimes&lt;/em&gt; gave the right answer and sometimes did not. However, with an algorithm which mostly gives right answers could be very useful to the cryptanalyst. Therefore, we briefly turn our attention to probabilistic computations.&lt;/p&gt;

&lt;h3&gt;Probabilistic Turing Machines&lt;/h3&gt;

&lt;p&gt;A Probabilistic Turing Machine (PTM) is a Deterministic Turing Machine (DTM) with an extra input tape. The tape is called the &amp;#8220;coin flipping tape.&amp;#8221; The PTM can read one bit of information at a time from the coin flipping tape. Each bit is assured to be a random bit.&lt;sup&gt;&lt;a href="#fn8" class="footnoteRef" id="fnref8"&gt;8&lt;/a&gt;&lt;/sup&gt; Computation on the machine proceeds as before except at any time a random choice can be made. This allows us to construct algorithms which will &amp;#8220;probably&amp;#8221; but not necessarily produce the desired answer.&lt;/p&gt;

&lt;p&gt;Analyzing the running time of a PTM is a bit different than a DTM. While a DTM&amp;#8217;s running time only depends on its program and the initial configuration of the input tape, a PTM also depends on the random bits it reads during the computation. Therefore, the running time of a PTM is a random variable (we denote it as &lt;span class="math"&gt;\(t_{\mathcal{M}}(x)\)&lt;/span&gt;). Furthermore, whether a PTM halts or not on a fixed input is also a random variable. A &lt;em&gt;halting&lt;/em&gt; PTM is one which halts after a finite number of steps for all inputs and all configurations of the coin tossing tape.&lt;/p&gt;

&lt;p&gt;With the definition a &lt;em&gt;halting&lt;/em&gt; PTM in hand we are now prepared to reason about its running time. Worst case running time of a PTM, &lt;span class="math"&gt;\(T_{\mathcal{M}}(n)\)&lt;/span&gt;, is:&lt;/p&gt;

&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
  T_{\mathcal{M}}(n) = \text{max} \{ t \text{ } | \text{ there exists a } 
                       x \in \Sigma^{n}_{0} \text{ such that } 
                       \text{Pr}[t_{\mathcal{M}}(x) = t] &amp;gt; 0 \}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Informally, this definition states: the worst case running time of a PTM is the maximum running time, &lt;span class="math"&gt;\(t_{\mathcal{M}}(x)\)&lt;/span&gt;, for which the machine will run with some probability greater than zero. A polynomial PTM is one in which there exists some positive polynomial, &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, such that &lt;span class="math"&gt;\(T_{\mathcal{M}}(n) \le p(n)\)&lt;/span&gt; holds.&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn9" class="footnoteRef" id="fnref9"&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;BPP, Bounded Probability Polynomial Time&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A language &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is recognized by a polynomial PTM, &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt;, if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;for every &lt;span class="math"&gt;\(x \in \mathcal{L}\)&lt;/span&gt; it holds that&lt;/em&gt; Pr&lt;span class="math"&gt;\([\mathcal{M} \text{ accepts } x] \ge \frac{2}{3}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;for every &lt;span class="math"&gt;\(x \notin \mathcal{L}\)&lt;/span&gt; it holds that&lt;/em&gt; Pr&lt;span class="math"&gt;\([\mathcal{M} \text{ does not accept } x] \ge \frac{2}{3}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BPP is the class of languages recognized by a polynomial PTM.&lt;sup&gt;&lt;a href="#fn10" class="footnoteRef" id="fnref10"&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;&lt;a href="#fn11" class="footnoteRef" id="fnref11"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The class Bounded Probability Polynomial Time, sometimes called Bounded-&lt;em&gt;Error&lt;/em&gt; Probabilistic Polynomial Time, is somewhat analogous to the class P. Computations in BPP are considered feasible computations. The class P is contained within BPP, &lt;span class="math"&gt;\(P \subseteq BPP\)&lt;/span&gt;. However, the relationship between NP and BPP has not been established. In practice cryptographers assume &lt;span class="math"&gt;\(NP \nsubseteq BPP\)&lt;/span&gt; which implies &lt;span class="math"&gt;\(NP \neq P\)&lt;/span&gt;. All problems unsolvable by a polynomial PTM are considered infeasible, of which NP-Hard problems are a special case. As before, some instances of hard problems may in fact be easy to solve.&lt;/p&gt;

&lt;p&gt;Infeasible computations as defined above are nice formalisms but do not seem too useful. To utilize the previous definition one has to answer the following question: Given and instance of a problem is it a &lt;em&gt;hard&lt;/em&gt; instance? Unfortunately, we don&amp;#8217;t know how to answer this question.&lt;sup&gt;&lt;a href="#fn12" class="footnoteRef" id="fnref12"&gt;12&lt;/a&gt;&lt;/sup&gt; As we will see later, if we could easily find hard instances we could construct a simple and secure crypto-system by sampling hard instances. Therefore, cryptographers need better assurances than &lt;em&gt;worst-case&lt;/em&gt; assurances; a cryptographer needs to know a typical instance of a problem is hard.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1&gt;One Way Functions&lt;/h1&gt;

&lt;p&gt;With a firm grounding in Complexity Theory, we turn our attention to cryptography. First, by capturing the notion of exploitable computational difficulty as epitomized in the one way function. A one way function is a function which is &lt;em&gt;easy&lt;/em&gt; to compute but &lt;em&gt;hard&lt;/em&gt; to invert. More specifically:&lt;/p&gt;

&lt;p&gt;One Way Function [owf]&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;span class="math"&gt;\(\forall_x\)&lt;/span&gt; computing &lt;span class="math"&gt;\(f(x) = y\)&lt;/span&gt; is &lt;em&gt;easy&lt;/em&gt; to compute.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class="math"&gt;\(\forall_y\)&lt;/span&gt; computing &lt;span class="math"&gt;\(f^{-1}(y)\)&lt;/span&gt; such that &lt;span class="math"&gt;\(f(x) \in f^{-1}(y)\)&lt;/span&gt; is &lt;em&gt;hard&lt;/em&gt; to compute.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;The one way function in definition [owf] is more of a theoretical construct than an actual mathematical construct. Therefore, it uses the notion of &lt;em&gt;easy&lt;/em&gt; and &lt;em&gt;hard&lt;/em&gt; computations without grounding itself with exact definitions. One can think of this first definition as an abstract, or ideal, definition.&lt;/p&gt;

&lt;p&gt;Ignoring for the moment the definitional problems, what use is a one way function to the cryptographer? It turns out one can define secure cryptosystems with one way functions. Such a cryptosystem will be discussed in detail in section [secure-encryption]. For now consider this simple example of the power of the idea:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;One day while toiling away, Ian had a flash of insight which would put his mechanical workings to right. A machine danced in his mind, one which would make widgets faster and better than before. So clever his insight he knew no one else would easily come up with the same idea. Thus, he decided not to patent it. Instead, he wrote down his idea and ran it through a one way function producing &lt;span class="math"&gt;\(y\)&lt;/span&gt; his certificate of his idea. He then published &lt;span class="math"&gt;\(y\)&lt;/span&gt; widely, placing it in all the libraries around the country.&lt;/p&gt;
&lt;p&gt;Many years passed and Mallory stole Ian&amp;#8217;s idea. Mallory being very clever sought to undue Ian and patented the idea. Then, he sued Ian for patent infringement. But, since Ian had a certificate of his invention, &lt;span class="math"&gt;\(y\)&lt;/span&gt;, he could prove to the court he had invented and known about the idea long before Mallory had filed for the patent. The court agreed with Ian and invalidated Mallory&amp;#8217;s patent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The challenge in section [secure-encryption] will be transforming the one way function into a workable encryption device. For while a powerful concept, as demonstrated by the story above, it is non-obvious how a crypto-system can be constructed from it. But before crypto-systems, the definition must be tightened. Furthermore, one must be convinced one way functions can be reasonably expected to exist.&lt;/p&gt;

&lt;h2&gt;Strong One Way Functions&lt;/h2&gt;

&lt;p&gt;There are two vague terms used in definition [owf], &lt;em&gt;easy&lt;/em&gt; and &lt;em&gt;hard&lt;/em&gt; computations. Fortunately, we have already defined what an &lt;em&gt;easy&lt;/em&gt; computation is: an easy computation is on which can be done in (probabilistic) polynomial time. But what about inversion? What does it mean for a function to be hard to invert? A function, &lt;span class="math"&gt;\(f\)&lt;/span&gt;, is hard to invert if every probabilistic polynomial time algorithms will only invert &lt;span class="math"&gt;\(f\)&lt;/span&gt; with &lt;em&gt;negligible&lt;/em&gt; probability.&lt;/p&gt;

&lt;p&gt;Negligible&lt;sup&gt;&lt;a href="#fn13" class="footnoteRef" id="fnref13"&gt;13&lt;/a&gt;&lt;/sup&gt; [neg]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A function, &lt;span class="math"&gt;\(\mu : \mathbb{N} \rightarrow \mathbb{R}\)&lt;/span&gt;, is negligible if for every positive polynomial, &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, there exists an &lt;span class="math"&gt;\(N\)&lt;/span&gt; such that for all &lt;span class="math"&gt;\(n &amp;gt; N\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      \mu(n) &amp;lt; \frac{1}{p(n)}
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The definition of negligible is reminiscent of Asymptotic Notation used in the analysis of algorithms. It concerns itself with the behavior of the function, &lt;span class="math"&gt;\(\mu(n)\)&lt;/span&gt;, when &lt;span class="math"&gt;\(n\)&lt;/span&gt; grows large. An additional, and useful, feature of the definition is any negligible function remain negligible after multiplication with any polynomial &lt;span class="math"&gt;\(q(\cdot)\)&lt;/span&gt;. Therefore, any event which occurs with negligible probability will continue to occur with negligible probability even after polynomial repetitions. Thus, if &lt;span class="math"&gt;\(f\)&lt;/span&gt; is only invertible with polynomial time algorithm, &lt;span class="math"&gt;\(A\)&lt;/span&gt;, with negligible probability than no polynomial repetition of &lt;span class="math"&gt;\(A\)&lt;/span&gt; will be likely to invert &lt;span class="math"&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Strong One Way Functions&lt;sup&gt;&lt;a href="#fn14" class="footnoteRef" id="fnref14"&gt;14&lt;/a&gt;&lt;/sup&gt; [strong-owf]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A function, &lt;span class="math"&gt;\(f : \{0,1\}^* \rightarrow \{0,1\}^*\)&lt;/span&gt;, is &lt;strong&gt;strongly&lt;/strong&gt; one way if it is:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Easy to compute&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;There exists a (deterministic) polynomial time algorithm A such that on input x algorithm A outputs &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; (ie. &lt;span class="math"&gt;\(A(x) =
      f(x)\)&lt;/span&gt;).&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Hard to invert&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;For &lt;em&gt;every&lt;/em&gt; probabilistic polynomial time algorithm &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt;, every positive polynomial &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, and all sufficiently large &lt;span class="math"&gt;\(n\)&lt;/span&gt; the probability &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt; inverts &lt;span class="math"&gt;\(f\)&lt;/span&gt; is negligible. That is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
        \text{Pr}[A&amp;#39;(f(x)) \in f^{-1}(f(x))] &amp;lt; \frac{1}{p(n)}
      \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the above definition &amp;#8220;input x&amp;#8221; should be considered as a random variable drawn from a uniform distribution over &lt;span class="math"&gt;\(\{0,1\}^n\)&lt;/span&gt;. Thus, the second condition reads: for any random input of size &lt;span class="math"&gt;\(n\)&lt;/span&gt; the probability an arbitrary polynomial time algorithm will find a pre-image is negligible. If such a function could be found or constructed it would offer a strong assurance of computational difficulty.&lt;/p&gt;

&lt;h2&gt;Weak One Way Functions&lt;/h2&gt;

&lt;p&gt;While strong one way functions ensure any efficient inversion algorithm has only a negligible likelihood of succeeding; weak one way functions require efficient inversion algorithms will fail with a non-negligible probability.&lt;/p&gt;

&lt;p&gt;Weak One Way Functions&lt;sup&gt;&lt;a href="#fn15" class="footnoteRef" id="fnref15"&gt;15&lt;/a&gt;&lt;/sup&gt; [weak-owf]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A function, &lt;span class="math"&gt;\(f : \{0,1\}^* \rightarrow \{0,1\}^*\)&lt;/span&gt;, is &lt;strong&gt;weakly&lt;/strong&gt; one way if it is:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Easy to compute&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;There exists a (deterministic) polynomial time algorithm A such that on input x algorithm A outputs &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; (ie. &lt;span class="math"&gt;\(A(x) =
      f(x)\)&lt;/span&gt;).&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Slightly hard to invert&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;There exists a polynomial &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt; such that for every probabilistic polynomial time algorithm, &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt;, and a sufficiently large &lt;span class="math"&gt;\(n\)&lt;/span&gt;&amp;#8217;s,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
        \text{Pr}[A&amp;#39;(f(x)) \notin f^{-1}(f(x))] &amp;gt; \frac{1}{p(n)}
      \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/blockquote&gt;

&lt;p&gt;In definition [strong-owf] the probability that &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt; could invert &lt;span class="math"&gt;\(f\)&lt;/span&gt; has an upper bound of &lt;span class="math"&gt;\(p(\cdot)^{-1}\)&lt;/span&gt; for &lt;em&gt;every&lt;/em&gt; positive polynomial. In definition [weak-owf], there is a &lt;em&gt;single&lt;/em&gt; positive polynomial, &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, such that &lt;span class="math"&gt;\(p(\cdot)^{-1}\)&lt;/span&gt; is a lower bound on the failure of any efficient inversion algorithm. Unlike strong one way functions, weak one way functions are not hard for typical instances. However, they are hard for some percentage of instances.&lt;/p&gt;

&lt;h3&gt;Amplification of Weak One Way Functions&lt;/h3&gt;

&lt;p&gt;Since weak functions are hard for a non-negligible percentage of inputs they can be used to construct strong functions. The proof for this bold assertion is given by Goldreich.&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn16" class="footnoteRef" id="fnref16"&gt;16&lt;/a&gt;&lt;/sup&gt; Since one can convert a weak one way function into a strong one it suffices to find weak ones. While a strongly one way function may yield a more efficient cryptosystem a weak one will still allow a secure system (as discussed in section [secure-encryption]).&lt;/p&gt;

&lt;h2&gt;Hard Core Predicates&lt;/h2&gt;

&lt;p&gt;If Alice has a strong one way function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, computes &lt;span class="math"&gt;\(y = f(x)\)&lt;/span&gt;, and sends &lt;span class="math"&gt;\(y\)&lt;/span&gt; to Bob while Eve eavesdrops what can Eve learn about &lt;span class="math"&gt;\(x\)&lt;/span&gt;? Depending on the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; Eve may be able to learn a surprising amount. Since &lt;span class="math"&gt;\(f\)&lt;/span&gt; is hard to invert Eve cannot learn everything about &lt;span class="math"&gt;\(x\)&lt;/span&gt; but she may not need too. Is there some way to quantify which bits of &lt;span class="math"&gt;\(x\)&lt;/span&gt; Eve can learn about and which bits she can&amp;#8217;t?&lt;/p&gt;

&lt;p&gt;There is! The bits which are hard for a polynomial attacker (like Eve) to learn about are called the &amp;#8220;Hard Core&amp;#8221; of a one way function. A predicate is a yes/no question, for example: Does &lt;span class="math"&gt;\(x\)&lt;/span&gt; end with a 0? If a yes/no question is hard for Eve to answer it is called a Hard Core Predicate. Since a yes/no question only has 2 possible answers Eve can always guess the answer. Therefore, a predicate is only hard for her to answer if she can&amp;#8217;t do better than get it right about half the time. To be precise:&lt;/p&gt;

&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
  \text{Pr}[\text{EveGuess\_P}(y) = P(x)] \le \frac{1}{2} + neg(|x|)\end{aligned}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span class="math"&gt;\(neg(\vert x\vert )\)&lt;/span&gt; is a negligible function (as defined in definition [neg]). This description of Eve trying to guess something about &lt;span class="math"&gt;\(x\)&lt;/span&gt;, like whether it starts with 0, leads nicely into a formal definition:&lt;/p&gt;

&lt;p&gt;Hard-Core Predicates&lt;sup&gt;&lt;a href="#fn17" class="footnoteRef" id="fnref17"&gt;17&lt;/a&gt;&lt;/sup&gt; [hardcore]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A polynomial time computable predicate, &lt;span class="math"&gt;\(b : \{0,1\}^* \rightarrow
    \{0,1\}\)&lt;/span&gt;, is called a &lt;strong&gt;hard-core&lt;/strong&gt; of a function, &lt;span class="math"&gt;\(f\)&lt;/span&gt;, if for every probabilistic polynomial time algorithm &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt;, every positive polynomial &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, and all sufficiently large &lt;span class="math"&gt;\(|x|\)&lt;/span&gt;&amp;#8217;s,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      \text{Pr}[A&amp;#39;(f(x)) = b(x)] &amp;lt; \frac{1}{2} + \frac{1}{p(|x|)}
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given a hard to invert function, &lt;span class="math"&gt;\(f\)&lt;/span&gt;, one knows some of the bits in its input must be hard to predict from the output. How does one know which bits are the hard bits? In general deciding what bits are hard for a function is difficult but one can always construct a Hard-Core Predicate for any strong one way function. Since one can always construct a strong one way function from a weak function this poses no limitation to the framework.&lt;/p&gt;

&lt;h3&gt;Constructing Hard-Core Predicates&lt;/h3&gt;

&lt;p&gt;The following result was first proved in 1982 by Yao but we present a simplification due to Goldreich and Levin as presented by Talbot and Welsh.&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn18" class="footnoteRef" id="fnref18"&gt;18&lt;/a&gt;&lt;/sup&gt; A detailed proof is available as usual in the Goldreich book.&lt;span class="citation"&gt;&lt;/span&gt;&lt;sup&gt;&lt;a href="#fn19" class="footnoteRef" id="fnref19"&gt;19&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Hard-Core Predicates from Strong One Way Functions [con-hard]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(f\)&lt;/span&gt; be an arbitrary strong one way function. Let &lt;span class="math"&gt;\(g\)&lt;/span&gt; be defined as &lt;span class="math"&gt;\(g(x, r) = (f(x), r)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(|x| = |r|\)&lt;/span&gt;. Let &lt;span class="math"&gt;\(r\)&lt;/span&gt; be a random bit string. Then define &lt;span class="math"&gt;\(B(x, r)\)&lt;/span&gt; to be a Hard-Core Predicate of &lt;span class="math"&gt;\(g\)&lt;/span&gt; by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      B(x,r) &amp;amp;= \sum\limits^{|x|}_{i=1} x_i r_i \imod{2} \\
             &amp;amp;= \bar{x} \cdot \bar{r} \imod{2} 
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The theorem states, if &lt;span class="math"&gt;\(f\)&lt;/span&gt; is strongly one way then it will be hard to guess the result of taking an exclusive-or of a random subset of &lt;span class="math"&gt;\(x\)&lt;/span&gt; given &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; and the subset &lt;span class="math"&gt;\(r\)&lt;/span&gt;. If &lt;span class="math"&gt;\(B(x,r)\)&lt;/span&gt; is not a hard-core of &lt;span class="math"&gt;\(g\)&lt;/span&gt; then &lt;span class="math"&gt;\(f\)&lt;/span&gt; is easily invertible. The proof involves constructing an algorithm from the predictor for &lt;span class="math"&gt;\(B\)&lt;/span&gt;. For details on the construction once again see Goldreich.&lt;/p&gt;

&lt;p&gt;With the result of theorem [con-hard] and the ability to construct strong one way functions from weak one way functions one will always be able to construct a function where at least one predicate on &lt;span class="math"&gt;\(x\)&lt;/span&gt; is hard to compute. If one bit is not enough it turns out &lt;em&gt;hard-core functions&lt;/em&gt; are also constructable. However, their specific details are well out of the scope of this paper.&lt;/p&gt;

&lt;h2&gt;Constructing One Way Functions&lt;/h2&gt;

&lt;p&gt;It one is going to build a crypto-system based on hard computational problems (specifically strong one way functions) one should have some way of identifying such problems. From a practical perspective there are three number theoretic based problems which are assumed to be one way functions. The first is the discrete log problem: &lt;span class="math"&gt;\(g^x \equiv y \imod{p}\)&lt;/span&gt;, second finding square roots mod &lt;span class="math"&gt;\(N = pq\)&lt;/span&gt;, and third the &amp;#8220;RSA&amp;#8221; problem &lt;span class="math"&gt;\(c \equiv x^e \imod{N}\)&lt;/span&gt;. While these problems are likely to be used in practice none of them are suspected to be in the class NP-Hard. While, instances of problems in NP-Hard may be efficiently solvable there is good evidence they are not. In contrast these problems are potentially vulnerable to good approximation algorithms.&lt;/p&gt;

&lt;p&gt;Thus, an open problem for the aspiring cryptographer to tackle is to suggest a novel one way function. However, serious care needs to be exercised when suggesting such a function. It is not good enough for the function to be difficult in the &lt;em&gt;worst-case&lt;/em&gt; it must be difficult in the typical case. Average case complexity analysis relies heavily on the input distribution. Thus, the input distribution must be carefully characterized and uniform sampling techniques must be developed. Without exercising such care the aspiring cryptographer may fall into the trap of defining something which appears secure from a cursory theoretical glance but on close inspection is quite vulnerable.&lt;/p&gt;

&lt;h1&gt;Secure Encryption&lt;/h1&gt;

&lt;p&gt;[secure-encryption]&lt;/p&gt;

&lt;p&gt;Secure encryption schemes are naturally built on top of strong one way functions with hard-core predicates. However, before the encryption schemes can be defined a formal definition of security must be stated. Until now, our definition has been colloquial: information in the ciphertext should be computationally infeasible to extract. The informal definition is too vague for use in defining an encryption system because the security definition is more important than the cryptographic system itself. A proper definition ensures systems conforming to the definition will be more difficult to attack.&lt;/p&gt;

&lt;h2&gt;Security Definitions&lt;/h2&gt;

&lt;p&gt;[sec-def]&lt;/p&gt;

&lt;p&gt;Before rigorously defining a modern definition of security let us turn once again to classical security and &lt;em&gt;perfect-secrecy&lt;/em&gt;. Recall perfect secrecy says an attackers &lt;em&gt;uncertainty&lt;/em&gt; about a message should not be reduced when in possession of a corresponding ciphertext. As noted in the introduction, the obvious criticism of &lt;em&gt;perfect-secrecy&lt;/em&gt; is the implied key length. In such a system, the length of the key must be at least as long as the message. Making the definition impractical for most modern uses of cryptography. Therefore, a new definition is indeed necessary.&lt;/p&gt;

&lt;h3&gt;Polynomial Indistinguishability&lt;/h3&gt;

&lt;p&gt;The first definition we will consider is &lt;em&gt;polynomial-indistinguishability&lt;/em&gt;. Informally, if Alice has two messages, &lt;span class="math"&gt;\(M_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(M_2\)&lt;/span&gt; and she sends Bob a ciphertext, &lt;span class="math"&gt;\(C\)&lt;/span&gt;, Eve who has been given both messages and the ciphertext will have no easy way to determine which message it corresponds to. Something is easy for Eve if she can do it in probabilistic polynomial time. Indeed, it is assumed none of our characters can do any computations except easy ones. Formally,&lt;/p&gt;

&lt;p&gt;Polynomial Indistinguishability of Encryptions&lt;sup&gt;&lt;a href="#fn20" class="footnoteRef" id="fnref20"&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An encryption scheme, &lt;span class="math"&gt;\((G, E, D)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(G\)&lt;/span&gt; generates keys, &lt;span class="math"&gt;\(E\)&lt;/span&gt; encrypts messages, and &lt;span class="math"&gt;\(D\)&lt;/span&gt; decrypts messages has &lt;em&gt;indistinguishable encryptions&lt;/em&gt; if for every probabilistic polynomial time algorithm, &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt;, every polynomial &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, all sufficiently large &lt;span class="math"&gt;\(n\)&lt;/span&gt;, and every &lt;span class="math"&gt;\(x,y \in
    \{0,1\}^{\text{poly}(n)}\)&lt;/span&gt; with &lt;span class="math"&gt;\(|x| = |y|\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      | \text{Pr}[A&amp;#39;(E_{G(1^n)}(x)) = 1] - 
        \text{Pr}[A&amp;#39;(E_{G(1^n)}(y)) = 1] |  &amp;lt;  \frac{1}{p(n)}
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The above definition was written with a symmetric encryption and decryption keys. However, the public key version only has minor and unimportant complications. The importance of the definition is in the intuition. Eve, the attacker, knows both messages and she has a ciphertext. The only thing she does not know is the key used to create the ciphertext. If the system is polynomially indistinguishable then Eve can only guess which message the ciphertext corresponds to. Since there are two messages she will only get it right half the time. If she can get it right better than half the time then the system is &lt;em&gt;not&lt;/em&gt; polynomially indistinguishable.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;security&lt;/em&gt; of the definition is perhaps non-obvious but consider the case were Eve can distinguish which message the ciphertext corresponds too. If the system was supposed to have &lt;em&gt;perfect-secrecy&lt;/em&gt; then clearly the secrecy would have been violated. Some bit of information would be leaking from the message to the ciphertext. Therefore, what the definition is saying is no information is leaking from the message to the ciphertext which can be extracted in polynomial time.&lt;/p&gt;

&lt;h3&gt;Semantic Security&lt;/h3&gt;

&lt;p&gt;The intuitive explanation of polynomial indistinguishability is captured in an alternative definition: &lt;em&gt;semantic-security&lt;/em&gt;. A crypto-system is semantically secure if any piece of information Eve can compute given a ciphertext she could just as easily compute without the ciphertext. That is, the ciphertext provides Eve with no advantage for computing any piece of information of interest to her. Formally,&lt;/p&gt;

&lt;p&gt;Semantic Security&lt;sup&gt;&lt;a href="#fn21" class="footnoteRef" id="fnref21"&gt;21&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An encryption scheme, &lt;span class="math"&gt;\((G, E, D)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(G\)&lt;/span&gt; generates keys, &lt;span class="math"&gt;\(E\)&lt;/span&gt; encrypts messages, and &lt;span class="math"&gt;\(D\)&lt;/span&gt; decrypts messages is &lt;em&gt;semantically secure&lt;/em&gt; if for every probabilistic polynomial time algorithm, &lt;span class="math"&gt;\(A\)&lt;/span&gt;, there exists another probabilistic polynomial time algorithm, &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt;, such that for every message &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; of length &lt;span class="math"&gt;\(n\)&lt;/span&gt;, every pair of functions with polynomially bounded output &lt;span class="math"&gt;\(f,h : \{0,1\}^* \rightarrow \{0,1\}^*\)&lt;/span&gt;, every polynomial &lt;span class="math"&gt;\(p(\cdot)\)&lt;/span&gt;, and all sufficiently large &lt;span class="math"&gt;\(n\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      \text{Pr}[A(1^n, E_{G_1(1^n)}(\mathcal{M}), h(1^n, \mathcal{M})] =
                                                        f(1^n,\mathcal{M})] \\
                                  &amp;lt;
      \text{Pr}[A&amp;#39;(1^n, h(1^n, \mathcal{M})] = f(1^n,\mathcal{M})] +
                                                        \frac{1}{p(n)}
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the above definition, &lt;span class="math"&gt;\(f\)&lt;/span&gt; represents the information Eve would like to compute. The information Eve wants, &lt;span class="math"&gt;\(f\)&lt;/span&gt;, is a function of the message and the length of the message (encoded for technical reasons in unary). The output of &lt;span class="math"&gt;\(f\)&lt;/span&gt; is polynomial however it is not necessary for &lt;span class="math"&gt;\(f\)&lt;/span&gt; to be a &lt;em&gt;computable&lt;/em&gt; function. The algorithm &lt;span class="math"&gt;\(A\)&lt;/span&gt; guesses &lt;span class="math"&gt;\(f\)&lt;/span&gt; using the ciphertext, the length of the message, and &lt;span class="math"&gt;\(h\)&lt;/span&gt;. The algorithm &lt;span class="math"&gt;\(A&amp;#39;\)&lt;/span&gt; guesses &lt;span class="math"&gt;\(f\)&lt;/span&gt; using only the length of the message and &lt;span class="math"&gt;\(h\)&lt;/span&gt;. The function &lt;span class="math"&gt;\(h\)&lt;/span&gt; represents a polynomial amount of &lt;em&gt;a-priori&lt;/em&gt; knowledge about the output of &lt;span class="math"&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The definition of semantic security intuitive says the probability Eve can guess &lt;span class="math"&gt;\(f\)&lt;/span&gt; utilizing the ciphertext is at most negligibly greater than guessing &lt;span class="math"&gt;\(f\)&lt;/span&gt; without the ciphertext. The definition places no restrictions on what Eve might be guessing (other than an upper bound on its size). Eve could be guessing whether the message is an order to move troops, or the message is a bank account number; it makes no difference to the definition.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Semantic-security&lt;/em&gt; is therefore the complexity theory analog of &lt;em&gt;perfect-secrecy&lt;/em&gt;. It provides assurance to the cryptographer that a polynomially bound cryptanalyst will be able to gain no information from the ciphertext. In practice, one only cares about polynomially bound adversaries since exponential adversaries do not exist.&lt;/p&gt;

&lt;h3&gt;Equivalence of Definitions&lt;/h3&gt;

&lt;p&gt;In a potentially surprising result it turns out it doesn&amp;#8217;t matter which security definition one uses, they imply each other:&lt;/p&gt;

&lt;p&gt;Equivalence of Definitions&lt;sup&gt;&lt;a href="#fn22" class="footnoteRef" id="fnref22"&gt;22&lt;/a&gt;&lt;/sup&gt; [equiv-thm]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An encryption scheme is semantically secure if and only if it has indistinguishable encryptions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In practice, it is usually far easier to prove a scheme has indistinguishable ciphertexts. However, from a security perspective the property one actually wants is &lt;em&gt;semantic-security&lt;/em&gt;. Thus, theorem [equiv-thm] provides the cryptographer with an incredibly useful result.&lt;/p&gt;

&lt;h2&gt;A Secure Symmetric Key Encryption Scheme&lt;/h2&gt;

&lt;p&gt;To construct a perfectly secret symmetric key encryption scheme from an information theory perspective one first obtains a large amount of random information. One then takes a random bit for each bit of message and exclusive-ors them together. One now has the perfect cryptographic system. The construction of a semantically secure system is quite similar (in the case of stream ciphers). One takes a bit of random information, referred alternately as the seed of the key, stretches it to create a pseudo-random sequence the same length as the message. The message and the pseudo-random sequence are then xored together. This encryption scheme will clearly be semantically secure if no adversary can distinguish between the pseudo-random sequence and a truly random sequence.&lt;/p&gt;

&lt;h3&gt;Pseudo-Random Sequence Generators&lt;/h3&gt;

&lt;p&gt;A pseudo-random bit generator, &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; is defined as a deterministic polynomial time algorithm taking a bit-string, &lt;span class="math"&gt;\(x \in \{0,1\}^k\)&lt;/span&gt;, and outputting a longer string &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt;. In other words, the generator stretches the input. For the generator to be pseudo-random in nature, the output must be unpredictable if the input is random. Luckily, we already know how to produce bits which are essentially unguessable by a polynomial adversary. Hard-core predicates by construction cannot be guessed correctly better than half the time.&lt;/p&gt;

&lt;p&gt;A Pseudo-Random Generator can be Constructed from any One Way Permutation.&lt;sup&gt;&lt;a href="#fn23" class="footnoteRef" id="fnref23"&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;&lt;a href="#fn24" class="footnoteRef" id="fnref24"&gt;24&lt;/a&gt;&lt;/sup&gt; [onebitgen]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(f : \{0,1\}^* \rightarrow \{0,1\}^*\)&lt;/span&gt; be a one-way function length preserving permutation with a hard core predicate &lt;span class="math"&gt;\(B : \{0,1\}^*
    \rightarrow \{0,1\}\)&lt;/span&gt; then,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      &amp;amp;G : \{0,1\}^k \rightarrow \{0,1\}^{k+1} \\
      &amp;amp;G(x) = (f(x), B(x))
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is a pseudo-random generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a random string, and therefore drawn from a uniform distribution over &lt;span class="math"&gt;\(\{0,1\}^k\)&lt;/span&gt;, then &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; is also a random string. Therefore, if there is some test, &lt;span class="math"&gt;\(T\)&lt;/span&gt;, which can distinguish &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; from a random string of length &lt;span class="math"&gt;\(k+1\)&lt;/span&gt; it must be distinguishing the last bit, &lt;span class="math"&gt;\(B(x)\)&lt;/span&gt;. Since, it can distinguish &lt;span class="math"&gt;\(B(x)\)&lt;/span&gt; from a random bit then one must be able to guess it significantly better than half the time. However, this contradicts &lt;span class="math"&gt;\(B(x)\)&lt;/span&gt; being a hard-core predicate of &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;. Therefore, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is either not a one-way function or &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; is a pseudo-random generator.&lt;/p&gt;

&lt;p&gt;While, theorem [onebitgen] certainly constructs a pseudo-random number generator it is hardly a useful one. Recall, the issue with perfect secrecy was the key size. If one constructed a stream cipher from using theorem [onebitgen] one would only save 1 bit of key size over a one time pad. Luckily, the following extension also holds:&lt;/p&gt;

&lt;p&gt;An &lt;span class="math"&gt;\(l(k)\)&lt;/span&gt; Pseudo-Random Generator&lt;sup&gt;&lt;a href="#fn25" class="footnoteRef" id="fnref25"&gt;25&lt;/a&gt;&lt;/sup&gt; [gen]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(f : \{0,1\}^* \rightarrow \{0,1\}^*\)&lt;/span&gt; be a one-way function length preserving permutation with a hard core predicate &lt;span class="math"&gt;\(B : \{0,1\}^*
    \rightarrow \{0,1\}\)&lt;/span&gt;. If &lt;span class="math"&gt;\(l(\cdot)\)&lt;/span&gt; is a positive polynomial then,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
      &amp;amp;G : \{0,1\}^k \rightarrow \{0,1\}^{l(k)} \\
      &amp;amp;G(x) = (B(x), B(f(x)), B(f^2(x)), ..., B(f^{l(k)-1}(x)))
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is a pseudo-random generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With the construction in theorem [gen] one can now generate a strong pseudo-random sequence. If &lt;span class="math"&gt;\(f\)&lt;/span&gt; is a strong one way function with a hard-core then no polynomial adversary can discern between the output of the generator above and a truly random string.&lt;/p&gt;

&lt;p&gt;A Symmetric Key Stream Cipher&lt;sup&gt;&lt;a href="#fn26" class="footnoteRef" id="fnref26"&gt;26&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;dl&gt;
&lt;dt&gt;Setup&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Alice chooses a short random key &lt;span class="math"&gt;\(x \in_{R} \{0,1\}^k\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Key Distribution&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Alice secretly shares &lt;span class="math"&gt;\(x\)&lt;/span&gt; with Bob.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Encryption&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Alice encrypts an &lt;span class="math"&gt;\(m\)&lt;/span&gt;-bit message, &lt;span class="math"&gt;\(M\)&lt;/span&gt;, by generating a pseudo-random string:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\[\begin{aligned}
                          G(x) = (B(f(x)), B(f^2(x)), ..., B(f^m(x)))
                        \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and forming the cryptogram &lt;span class="math"&gt;\(C = G(x) \otimes M\)&lt;/span&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Decryption&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Bob creates the same string &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; an recovers the message via &lt;span class="math"&gt;\(M = G(x) \otimes C\)&lt;/span&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/blockquote&gt;

&lt;p&gt;The strength of the cipher relies on the strength of &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; and the strength of &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; relies on the underlying properties of the one way function, &lt;span class="math"&gt;\(f\)&lt;/span&gt;. The above stream cipher is clearly semantically secure since the ciphertexts are indistinguishable by Eve. If Eve could distinguish the ciphertexts than she could predict &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt;. If Eve can predict &lt;span class="math"&gt;\(G(x)\)&lt;/span&gt; than &lt;span class="math"&gt;\(f\)&lt;/span&gt; must not be a one way function.&lt;/p&gt;

&lt;p&gt;While the above stream cipher is semantically secure, it is not necessarily the construction one would use in practice. Often, one would instead want to use a block cipher. Luckily, one can also construct block based ciphers from pseudo-random generators. For these an many other complications I refer you to Oded Goldreich&amp;#8217;s 2004 book.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;Public Key Schemes&lt;/h2&gt;

&lt;p&gt;I will not discuss the public key schemes in detail. The definitions for security setup in section [sec-def] are implicitly for symmetric key systems. While, the modifications are fairly trivial they should be given proper treatment. In addition the public key systems deserve a thorough explanation. I will settle for some brief remarks.&lt;/p&gt;

&lt;p&gt;The RSA cryptographic system does not satisfy the property of polynomial indistinguishability. In particular, if Eve wants to tell whether &lt;span class="math"&gt;\(C\)&lt;/span&gt; corresponds to &lt;span class="math"&gt;\(M_1\)&lt;/span&gt; or to &lt;span class="math"&gt;\(M_2\)&lt;/span&gt; all she has to do is encrypt both messages can compare their ciphertexts. Eve and easily do this since the encryption algorithm in a public key system is public and therefore available to Eve.&lt;/p&gt;

&lt;p&gt;The encryption algorithm being publicly available seems to be an insurmountable obstacle at first, but it turns out to be possible to overcome it. In the case of the RSA algorithm one needs to introduce randomness (and thus uncertainty) into the encryption process. One such suggestion &lt;em&gt;Randomized RSA&lt;/em&gt; introduces random data into each encryption thus ensuring polynomial indistinguishability. However, Randomized RSA comes with a cost: one must believe a different strong assumption. One must assume RSA has a &amp;#8220;large&amp;#8221; hard-core of bits in the input. While, this may be a reasonable assumption it is a &lt;em&gt;different&lt;/em&gt; assumption and not implied by the usual RSA assumption. For details I once again commend you to Goldreich&amp;#8217;s 2004 book.&lt;span class="citation"&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1&gt;Concluding Remarks&lt;/h1&gt;

&lt;p&gt;Basing cryptographic security on computation complexity is a sound practice. It yields systems with strong and extensible security guarantees. However, it also requires strong assumptions. In particular, we must believe in &amp;#8220;one way functions.&amp;#8221; While, there is good evidence they exist, and several candidate functions appear to work, we do not &lt;em&gt;know&lt;/em&gt; they exist. But, until a better formalism comes along complexity theory is secure in its position as the basis of modern cryptography.&lt;/p&gt;

&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;See section 4.6.1&lt;a href="#fnref1"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See pages 2,3&lt;a href="#fnref2"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;See page 3&lt;a href="#fnref3"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;ie. time proportional to the size of the input&lt;a href="#fnref4"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;See &lt;span class="citation"&gt;&lt;/span&gt; page 41 at the bottom.&lt;a href="#fnref5"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;That is &lt;span class="math"&gt;\(\mathcal{M}\)&lt;/span&gt; computes in the &lt;span class="math"&gt;\(n\)&lt;/span&gt;th bit of output in &lt;span class="math"&gt;\(p(n)\)&lt;/span&gt; time.&lt;a href="#fnref6"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;See &lt;span class="citation"&gt;&lt;/span&gt; page 23.&lt;a href="#fnref7"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;Since a PTM is a theoretical construction rather than a physical construction we can do away with the nasty realities of life and assume these random bits are actually random! A nice change of pace.&lt;a href="#fnref8"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;See section 4.2&lt;a href="#fnref9"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn10"&gt;&lt;p&gt;Note, any constant greater than &lt;span class="math"&gt;\(\frac{1}{2}\)&lt;/span&gt; can be used here.&lt;a href="#fnref10"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn11"&gt;&lt;p&gt;Definition is a combination of Definition 1.3.4 from &lt;span class="citation"&gt;&lt;/span&gt; and the definition given in Section 4.5 of &lt;span class="citation"&gt;&lt;/span&gt;.&lt;a href="#fnref11"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn12"&gt;&lt;p&gt;From personal discussion with Prof. Harold Connamacher (harold.connamacher@cwru.edu)&lt;a href="#fnref12"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn13"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 1.3.5&lt;a href="#fnref13"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn14"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 2.2.1. Note, I simplified the definition slightly for clarity.&lt;a href="#fnref14"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn15"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 2.2.2&lt;a href="#fnref15"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn16"&gt;&lt;p&gt;See Theorem 2.3.2 for an impractical but demonstrative conversion and Section 2.6 for an efficient conversion in the case of one-way permutations.&lt;a href="#fnref16"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn17"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 2.5.1&lt;a href="#fnref17"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn18"&gt;&lt;p&gt;Theorem 10.8&lt;a href="#fnref18"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn19"&gt;&lt;p&gt;Section 2.5.2&lt;a href="#fnref19"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn20"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 5.2.3, simplified.&lt;a href="#fnref20"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn21"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see Def. 5.2.1, simplified.&lt;a href="#fnref21"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn22"&gt;&lt;p&gt;Theorem (and proof) due to &lt;span class="citation"&gt;&lt;/span&gt; Theorem 5.2.5.&lt;a href="#fnref22"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn23"&gt;&lt;p&gt;Theorem and proof due to &lt;span class="citation"&gt;&lt;/span&gt; see theorem 10.9. My proof is a summary of Talbot and Welsh&amp;#8217;s main argument&lt;a href="#fnref23"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn24"&gt;&lt;p&gt;A one way permutation is simply a one way function which is a bijection from the domain to the range. The existence of one way functions implies the existence of one way permutations.&lt;a href="#fnref24"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn25"&gt;&lt;p&gt;Theorem due to &lt;span class="citation"&gt;&lt;/span&gt; see Theorem 10.10&lt;a href="#fnref25"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn26"&gt;&lt;p&gt;Definition due to &lt;span class="citation"&gt;&lt;/span&gt; see page 216.&lt;a href="#fnref26"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Mon, 11 Nov 2013 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2013-11-11:cryptography-and-complexity.html</guid></item><item><title>Programming Never Gets Easier</title><link>http://hackthology.com/programming-never-gets-easier.html</link><description>&lt;p&gt;Another comment on
&lt;a href="http://www.cs.utexas.edu/users/EWD/transcriptions/EWD03xx/EWD340.html"&gt;E.W.Dijkstra Archive: The Humble Programmer (EWD 340)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dijkstra, in the essay, felt that at the current moment in time (1972) the
programmer was limited by the tools. That their thought processes were limited
by poor languages and poor environment. However, he expressed great hope that
this was transitory. Our tools would improve but he warns:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As an aside I would like to insert a warning to those who identify the
difficulty of the programming task with the struggle against the inadequacies
of our current tools, because they might conclude that, once our tools will be
much more adequate, programming will no longer be a problem. Programming will
remain very difficult, because once we have freed ourselves from the
circumstantial cumbersomeness, we will find ourselves free to tackle the
problems that are now well beyond our programming capacity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I believe he was right. Some might argue are tools have not improved and that is
why programming is not better. Indeed they may say it is worse than ever. But, I
think our tools have improved. They don't always improve linearly but they are
improving in fits and starts, with steps forwards and steps back. No matter how
much our tools may have improved the state of programming as a practice has
barely advanced at all! We still struggle against our own inadequacies, our own
failings. Even when the algorithms are clear, the problem well studied, the
programmer rarely gets it right the first time. Programming doesn't get easier
as the tools get better. Rather the scope of our ambition climbs. As it climbs
the problems get harder and the tried and true solution of yesteryear seems
quaint to the practicing programmer of today.&lt;/p&gt;
&lt;p&gt;So, we cannot rest on our laurels. We must strive on and construct new tools
which ease our current pains, even if inevitably new pains arise. For, in
learning, there is suffering.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Sun, 10 Nov 2013 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2013-11-10:programming-never-gets-easier.html</guid></item><item><title>New Site</title><link>http://hackthology.com/new-site.html</link><description>&lt;p&gt;It has been 6 months maybe more since Posterous shut down and I have just now
gotten around to recreating my site. I chose to do this on github pages with
pelican. Hosting on github is easy and simple. If github every stops doing these
pages I can easily self host. I have now ported most of my old posts thanks to
&lt;code&gt;pandoc&lt;/code&gt;. The posts I left behind were mostly terrible so all in all I think the
new site is a big improvement. I might even write some more posts.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Wed, 06 Nov 2013 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2013-11-06:new-site.html</guid></item><item><title>The Limits of Type Systems</title><link>http://hackthology.com/the-limits-of-type-systems.html</link><description>&lt;p&gt;&lt;a href="http://www.cs.utexas.edu/users/EWD/transcriptions/EWD03xx/EWD340.html"&gt;E.W.Dijkstra Archive: The Humble Programmer (EWD 340)&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Argument three is based on the constructive approach to the problem of program
correctness. Today a usual technique is to make a program and then to test it.
But: program testing can be a very effective way to show the presence of bugs,
but is hopelessly inadequate for showing their absence. The only effective way
to raise the confidence level of a program significantly is to give a
convincing proof of its correctness.  But one should not first make the
program and then prove its correctness, because then the requirement of
providing the proof would only increase the poor programmers burden. On the
contrary: the programmer should let correctness proof and program grow hand in
hand.  Argument three is essentially based on the following observation. If
one first asks oneself what the structure of a convincing proof would be and,
having found this, then constructs a program satisfying this proofs
requirements, then these correctness concerns turn out to be a very effective
heuristic guidance. By definition this approach is only applicable when we
restrict ourselves to intellectually manageable programs, but it provides us
with effective means for finding a satisfactory one among these.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Essentially what Dijkstra is advocating here is what the approach the most
advanced type theories are striving for. They aim to allow the programmer to
specify in the type system (a proof system) important semantic invariants of the
application. The type system is constructed in such a way the program is only
well typed if the specified invariants hold. The hope of the type theorists is
that with a sufficiently powerful type (proof) system most if the not all
properties one cares to prove are in fact provable (for certain programs).&lt;/p&gt;
&lt;p&gt;The difficulty for the practicing programmer is most type systems are not nearly
powerful enough to specify properties which are actually interesting.  This
leaves the programmer doing essentially the proof equivalent of book keeping
with no benefit. I should distinguish here between dynamically checked types
(properties) and statically checked types.  Dynamically checked, that is at run
time, are always enormously helpful to the programmer because they provide
runtime safety. However, statically checked types can be overly burdensome if
they require lots of book keeping without sufficiently powerful proofs.&lt;/p&gt;
&lt;p&gt;Unfortunately, for a wide variety of statically checked programming languages
the available proofs are uninteresting and the burden is high.  This is the
challenge to the Dijkstra-ist.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Tue, 03 Sep 2013 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2013-09-03:the-limits-of-type-systems.html</guid></item><item><title>We can't automate the programmer.</title><link>http://hackthology.com/we-cant-automate-the-programmer.html</link><description>&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=6317778"&gt;HN Comment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I find this comment thread on how programmers will "automate" our way out of
jobs to be especially pertinent as I work on my proposal. A contention I have,
as do many others, is the primary practical challenge standing between us and
robust systems is complexity. There are several on this thread who believe we
have essentially mastered complexity for a broad number important application
areas.&lt;/p&gt;
&lt;p&gt;I think I could be convinced of this argument as long as those application areas
are very narrow. As long as the creators of those applications are content with
relatively standardized solutions. The moment there are A) functional
requirements not covered by the standard case or B) differentiating requirements
(such as aesthetics etc...) the general tool stops working. This is what makes
complexity such a beast.  If you make a fully generic tool, you have created a
programming language. Perhaps a very high level (even graphical language like
LabView) but still a language. Such things require trained people to operate.&lt;/p&gt;
&lt;p&gt;We will never not need programmers.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Tue, 03 Sep 2013 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2013-09-03:we-cant-automate-the-programmer.html</guid></item><item><title>Always Pertinent - C a giant leap backwards.</title><link>http://hackthology.com/always-pertinent-c-a-giant-leap-backwards.html</link><description>&lt;blockquote&gt;
&lt;p&gt;From the point of view of software engineering, the rapid spread of C
represented a great leap backward. It revealed that the community at large had
hardly grasped the true meaning of the term 'high-level language' which became
an ill-understood buzzword. What, if anything, was to be 'high-level'? As this
issue lies at the core of software engineering, we need to elaborate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;from : Wirth, N. (2008). &lt;em&gt;A Brief History of Software Engineering&lt;/em&gt;. IEEE Annals
of the History of Computing, 30(3), 3239.  doi:10.1109/MAHC.2008.33&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Sun, 25 Aug 2013 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2013-08-25:always-pertinent-c-a-giant-leap-backwards.html</guid></item><item><title>Passmash - The Site Specific Password Munger</title><link>http://hackthology.com/passmash-the-site-specific-password-munger.html</link><description>&lt;p&gt;&lt;a href="https://github.com/timtadh/passmash"&gt;Passmash&lt;/a&gt; is a new commandline
password munger. It has been tested to work on Linux with X and on
MacOS. It should also work on Windows.&lt;/p&gt;
&lt;h2&gt;What is a Munger?&lt;/h2&gt;
&lt;p&gt;A munger takes a password and turns it into another password, "munging"
it. In particular &lt;code&gt;passmash&lt;/code&gt; takes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A password (supplied interactively at the prompt)&lt;/li&gt;
&lt;li&gt;A URL (or other identifier) (supplied as a command line argument)&lt;/li&gt;
&lt;li&gt;A secret key (kept at \~/.ssh/passmash.key)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and returns a password. It has the advantages of a password manager
without having to worry about syncing a password database. The key file
is static, so simply keep a (possibly encrypted) backup of it. If you
loose the key file, you will not be able to recover your passwords.&lt;/p&gt;
&lt;h2&gt;Example Usage&lt;/h2&gt;
&lt;p&gt;In most circumstances you will want to use the &lt;code&gt;pm&lt;/code&gt; command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;pm myurlhere.com
Password:
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command automatically generates and copies the password to you
clipboard. On Linux it uses &lt;code&gt;xclip -selection clipboard&lt;/code&gt;, on Mac OS X it
uses &lt;code&gt;pbcopy&lt;/code&gt; and on Windows it uses &lt;code&gt;clip&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If it is on another operating system (like OpenBSD) it will pretty print
the password for easy typing. eg.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;pm myurlhere.com
&lt;span class="c"&gt;## We don&amp;#39;t yet support OpenBSD for autoclipboard copying&lt;/span&gt;
Password:

5KrUw4pBgC89LGxggXEIFtjM41aPc+/GxH+cumCuTo4
5KrUw - 4pBgC - 89LGx - ggXEI - FtjM4 - 1aPc+ - /GxH+ - cumCu - To4
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Technical Details&lt;/h2&gt;
&lt;p&gt;Passmash uses a SHA256 based HMAC with &lt;a href="http://en.wikipedia.org/wiki/Key_stretching"&gt;key
strengthening&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hmac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sha256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;250000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On my machine (a 2.0 Ghz Core2) it takes around 1 second to derive a
password using this function. A more secure version of the same utility
could make use of &lt;code&gt;bcrypt&lt;/code&gt; or &lt;code&gt;scrypt&lt;/code&gt;. However, either would add an
external dependency.&lt;/p&gt;
&lt;p&gt;This password derivation function should provide strong defense against
an attacker who has&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A password generated from the function (perhaps obtained from a
    hacked website).&lt;/li&gt;
&lt;li&gt;The algorithm. (eg. they know you use this program to generate your
    passwords).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And optionally:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key file&lt;/li&gt;
&lt;li&gt;&lt;em&gt;or&lt;/em&gt; the "master" password (but not both)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If your "master" password has sufficient
&lt;a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29"&gt;entropy&lt;/a&gt;
then your other passwords generated with the same key should be
reasonably secure against a brute force attack.&lt;/p&gt;
&lt;h3&gt;&lt;a href="https://github.com/timtadh/passmash"&gt;Happy Munging!&lt;/a&gt;&lt;/h3&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Mon, 30 Jan 2012 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2012-01-30:passmash-the-site-specific-password-munger.html</guid></item><item><title>Ternary Search Tries for Fast Flexible String Search : Part 1</title><link>http://hackthology.com/ternary-search-tries-for-fast-flexible-string-search-part-1.html</link><description>&lt;p&gt;Searching a large corpus of strings is a problem many applications have
to solve, whether the application features autocomplete boxes or
full-text search. Efficient methods for conducting such searches are not
always readily apparent to the algorithm designer. In this series of
articles I will present a data structure known as the Ternary Search
Trie (TST) which is designed to assist in solving this problem. For this
introductory article I will not discuss algorithms in detail but only
provide a high level overview of the structure and algorithmic running
time for various operations. In the next article I will detail the
process of maintaining the structure with insertions and deletions. The
final article will discuss different flexible search algorithms and
their implementations.&lt;/p&gt;
&lt;h2&gt;Symbol Tables: A Short Review&lt;/h2&gt;
&lt;p&gt;A symbol table is a mapping between a string key and a value which can
be any type of object from an integer to a complex nested structure.
There are two classic symbol table implementations most programmers are
immediately familiar with: Binary Search Trees (BSTs) and hash tables.
Both of these structures work by exactly matching the search key to the
keys stored in the structure. If there does not exist an exact match
then there is a miss. Thus, neither of these structures can serve as
useful index for an autocomplete algorithm where only part of the key is
known. They may be useful for a full text index, but they will not be as
efficient as some of the other structures we will later discuss.&lt;/p&gt;
&lt;p&gt;While these simple structures are limited, other symbol table
implementations have properties more suited for modification for
flexible string search. This series will focus on the Trie category of
structures. These structures are more suited for partial match as we
will see. But first, why are hash tables not suited for this job? They
have such excellent running time characteristics, O(1) lookups! However,
one cannot modify the hash table algorithm to effectively serve the
purpose of a partial match or a range query. Why? Because hash functions
transform strings into numbers (which correspond to buckets in an
array). Good hash functions have wide variance in hashing strings and
will not hash a similar string or a sub string to the same bucket.&lt;/p&gt;
&lt;h2&gt;Introducing the Trie&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Example Binary Search Trie" src="/images/Trie1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Figure 1. &lt;strong&gt;An Example Binary Search Trie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In general a Trie is a special for of a tree. However, instead of
comparing entire key at each node during traversal, it only compares
parts of keys. The key/value pairs are kept in the leaves (like in a B+
Tree). We will first consider a Binary Search Trie. Like the Binary
Search Tree, each node in the Trie has two children, left and right. The
left child is defined as the 0'' child and the right as the 1'' child.
As a key is inserted, a node is created or visited for each bit in the
key. When visiting a node which already exists, the direction to descend
is based on the current bit. that is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;let there be a function bit(i, s) which returns the ith bit in the
    string s.&lt;/li&gt;
&lt;li&gt;let depth(r, n) return the depth of the node n in the tree rooted at
    r&lt;/li&gt;
&lt;li&gt;bit(depth(r, n), s) is the bit in the string used to make the
    decision on which of n's children to visit.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sedgewick gives the formal definition: "A [binary search] trie is a
binary tree that has keys associated with each of its leaves, defined
recursively as follows: The trie for an empty set of keys is a null
link; the trie for a single key is a leaf containing that key; and the
trie for a set of keys of cardinality greater than one is an internal
node with left link referring to the trie for the keys whose initial bit
is 0 and right link referring to the trie for the keys whose initial bit
is 1, with the leading bit considered to be removed for the purpose of
constructing the subtrees."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;A search using this structure is directed by the strings in the
database. However, since only one bit is considered at a time in the
search for a k-bit string the search will take in the worst case k bit
comparisons. This makes for a very tall structure when using string
keys, since single characters will be at least 8 bits long in ASCII and
much longer in Unicode. Another unfortunate implementation detail is
that modern processors typically work more efficiently when accessing
bytes or words. Thus, a more efficient structure might consider multiple
bits at once.&lt;/p&gt;
&lt;h2&gt;Multi-way Tries&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Example R-Way Trie" src="/images/Trie2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Figure 2. &lt;strong&gt;An Example R-Way Trie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If one considers multiple bits at once one has to increase the fanout
(number of children per node) of the tree. Consider figure 2, in which
each node has a fanout of 26.&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt; While useful if every
node has 26 children, the space to store the pointers becomes wasteful
for real data. However, despite the wasted space in comparison to the
binary version, searches on the R-Way Trie will perform faster than on
the Binary Trie. It will be faster for the CPU to compare the bytes
under consideration and there will be fewer comparisons over all. In
general, a Binary Trie will require log~2~(N) comparisons to perform a
search, and an R-way Trie it will take log~R~(N) comparisons.&lt;/p&gt;
&lt;p&gt;However, to produce a usable structure for our purpose (a large
in-memory string index) we need to cut down on the space wasted by the
extra pointers in each node. The tricky bit is to do this while still
maintaining the hard-fought gains in search speed. Simply using a
dynamic structure like a hash table in each node to hold the array won't
work either because hash tables are slower than an array access and if
the hash table becomes overly full it may actually use more space than
the array. Thus, a different structure is needed.&lt;/p&gt;
&lt;h2&gt;Ternary Search Tries&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Example Ternary Search Trie" src="/images/TST1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Figure 3. &lt;strong&gt;An Example Ternary Search Trie&lt;/strong&gt; with strings [abc, abs,
awe, and].&lt;/p&gt;
&lt;p&gt;The Ternary Search Trie helps avoid the unnecessary space needed by a
traditional multi-way trie while still maintaining many of its
advantages. In a Ternary Search Trie each node contains a character and
three pointers. The pointers correspond to the current character under
consideration being less than, greater than or equal to the character
held by the node. In a sense this structure is like taking the Multi-way
Trie and encoding it on to a Binary Search Tree with the keys as current
character and the values as another BST corresponding to the next
character.&lt;/p&gt;
&lt;p&gt;While a Multi-way Trie has about R*N/log~2~(R) pointers, a Ternary
Search Trie has R + c*N pointers where c is a small constant, perhaps
3. Consider the graph of their performance:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Multi-way Trie vs. Ternary Search Trie" src="/images/Multiway_vs_Trie.gif" /&gt;&lt;/p&gt;
&lt;p&gt;Figure 4. &lt;strong&gt;Links in a Multi-way Trie vs. a Ternary Search Trie&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When R is small, 2, 3, 4 Multi-way Tries and Ternary Tries have similar
a similar number of pointers, but a low branching factor destroys the
advantage of the Multi-way Trie. When R grows to a larger, more
reasonable size such as 256, the number of pointers explodes in
comparison to the Ternary Trie. Thus, the Ternary formulation will be
far more space efficient in the worst case than the Multi-way
formulation.&lt;/p&gt;
&lt;p&gt;What is the cost for better space efficiency? In the Multi-way Trie we
must traverse at most the length of the search key links. In a TST we
may need to traverse up to 3 times that many links in the worst case.
However, this pathological case is rare. In the average case the
situation can be made much better through a few small improvements to
the basic structure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An Improved Ternary Search Trie" src="/images/TST2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Figure 5. &lt;strong&gt;An Improved Ternary Search Trie&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The first improvement, illustrated below in Figure 5, involves
collapsing the leaf nodes. Instead of allowing long chains of nodes at
the leaves, we collapse them into a single node. This allows the final
check to be computed efficiently.&lt;/p&gt;
&lt;p&gt;The second improvement, also illustrated in Figure 5, combines the best
of the Multi-way Search Trie with the Ternary Search Trie. The root node
is an R-Way node like in the Multi-way Search Trie. The rest of the tree
is a Ternary Search Trie with the leaf nodes collapsed. In practice
these improvements result in an enormous speedup. The theory also
supports the practice; according to Sedgewick, these improvements cut
the number of comparisons needed in half.&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;There is one final improvement to consider not shown the above figure.
Similar to the first improvement, it involves collapsing nodes, but
instead of collapsing leaf nodes, internal nodes are collapsed. This
idea is similar to the Patricia Trie. When a group of strings shares the
same contiguous substring, instead of having a node for each character
shared, collapse the shared nodes into a single node.&lt;/p&gt;
&lt;h1&gt;Conclusion and Whats Next&lt;/h1&gt;
&lt;p&gt;In this post we discussed the theory behind Symbol Tables and the use of
Tries as a symbol table implementation. The Trie, and in particular the
TST, is an efficient way to implement a symbol table. A good
implementation of a TST has comparable performance to a Hash Table.
However, as we will see in the next post they allow much more flexible
search operations. Stay tuned.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Sedgewick R. &lt;em&gt;Algorithms&lt;/em&gt;. Third Edition. Definition 15.1.  &lt;a href="http://www.amazon.com/dp/0201314525/"&gt;&lt;/a&gt;&lt;a href="http://www.amazon.com/dp/0201314525/"&gt;http://www.amazon.com/dp/0201314525/&lt;/a&gt; Hereafter: &lt;em&gt;Sedgewick&lt;/em&gt; &amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Note: Usually an R-Way or Multi-Way Trie has a fanout equal to the numbers of characters in the character set or the number of bits in a machine word, half word or byte. So in practice a node in an R-Way Trie might have 256 children or perhaps 2\^16 children. As the fanout (the number of children per node) increases the space efficiency of the Trie decreases. However, the search speed increases. A classic time/space trace off.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;em&gt;Sedgewick&lt;/em&gt; Table 15.2 and related text.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Thu, 02 Jun 2011 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2011-06-02:ternary-search-tries-for-fast-flexible-string-search-part-1.html</guid></item><item><title>How To: Write Self Updating Python Programs Using Pip and Git</title><link>http://hackthology.com/how-to-write-self-updating-python-programs-using-pip-and-git.html</link><description>&lt;p&gt;If you are a pip&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; and virtualenv user you already know how easy it is
to install python packages. Unlike the bad old days when I started
programming in Python, 9 years ago, it is now easy to add, remove and
manage python modules. In fact we can leverage pip to create an &lt;code&gt;update&lt;/code&gt;
command for a python program, for example and ease of illustration, a
shell utility.&lt;/p&gt;
&lt;h3&gt;Table of Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Desired Features&lt;/li&gt;
&lt;li&gt;Basic Idea&lt;/li&gt;
&lt;li&gt;Implementation&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Desired Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uses a server controlled by the owner (instead of the Python Package
  Index).&lt;/li&gt;
&lt;li&gt;Install an arbitrary version of the program.&lt;/li&gt;
&lt;li&gt;Defaults to updating to the newest version of the major release one
  is tracking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I often want my programs to update themselves from a specific location.
For instance an internal server or perhaps my github account.
Fortunately pip already supports such nicities with the &lt;code&gt;-e&lt;/code&gt; for the
&lt;code&gt;install&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Additionally, when running a generic update you often want to stay on
the same major revision and simply get the bug fixes. However, it is
important to provide the option to update to any arbitrary release
including tracking the master branch.&lt;/p&gt;
&lt;h2&gt;Basic Idea&lt;/h2&gt;
&lt;p&gt;Use Pip and the &lt;code&gt;-e&lt;/code&gt; option plus a base URL to automatically update your
software. eg. Have you software run pip for the user.&lt;/p&gt;
&lt;p&gt;example command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;pip&lt;/span&gt; &lt;span class="nb"&gt;install&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;upgrade&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;$HOME/.src&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;-e&lt;/span&gt; &lt;span class="nx"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;REV&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="vi"&gt;#egg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;PACKAGE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Tracking Major Versions&lt;/h4&gt;
&lt;p&gt;To track major version updates some care must be taken in setting up the
repository. I use branches instead of tags to track major releases. This
allows me to push out bug fix updates for every one tracking that
release. I tag minor releases to allow users to install a specific
version.&lt;/p&gt;
&lt;p&gt;Branches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master&lt;/li&gt;
&lt;li&gt;stable&lt;/li&gt;
&lt;li&gt;r0.1&lt;/li&gt;
&lt;li&gt;r0.2&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;rN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tags&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;r0.1&lt;/li&gt;
&lt;li&gt;r0.1.1&lt;/li&gt;
&lt;li&gt;r0.1.x&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;rN&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pip Gotcha&lt;/h4&gt;
&lt;p&gt;When checking out branches using pip you have to supply
&lt;code&gt;origin/branchname&lt;/code&gt; ex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;upgrade&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;$HOME/.src&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//github.com/user/repo.git@origin/branch#egg=PACKAGE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While when checking out a commit you should not supply origin&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;upgrade&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;$HOME/.src&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//github.com/user/repo.git@COMMIT_ID#egg=PACKAGE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Why does pip work like this? Because of the commands it executes. For
the command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;pip&lt;/span&gt; &lt;span class="nb"&gt;install&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;upgrade&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;$HOME/.src&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;-e&lt;/span&gt; &lt;span class="nx"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nx"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//github.com/user/repo.git@&amp;lt;VERSION&amp;gt;#egg=PACKAGE_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;pip runs&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;git&lt;/span&gt; &lt;span class="nx"&gt;fetch&lt;/span&gt; &lt;span class="na"&gt;-q&lt;/span&gt; &lt;span class="nx"&gt;git&lt;/span&gt; &lt;span class="nb"&gt;reset&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nx"&gt;hard&lt;/span&gt; &lt;span class="na"&gt;-q&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;VERSION&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Store the tracked version in the source&lt;/h4&gt;
&lt;p&gt;To ensure the update command installs the correct updates I put which
release to checkout in the source code. This allows me to "release" a
version by creating a branch and then changing the RELEASE constant to
point the name of the branch.&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Note: This is example code only, you should modify for security and
stability of your enviroment.&lt;/p&gt;
&lt;p&gt;Note: I didn't include virtualenv support in this code but it is trivial
to add.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;subprocess&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;check_call&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;getopt&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;getopt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GetoptError&lt;/span&gt; 
&lt;span class="n"&gt;RELEASE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;master&amp;#39;&lt;/span&gt; &lt;span class="c"&gt;# default release &lt;/span&gt;
&lt;span class="n"&gt;SRC_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;$HOME/.src&amp;quot;&lt;/span&gt; &lt;span class="c"&gt;# checkout directory &lt;/span&gt;
&lt;span class="n"&gt;UPDATE_CMD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="c"&gt;# base command &lt;/span&gt;
&lt;span class="s"&gt;&amp;#39;pip install --src=&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; --upgrade -e &amp;#39;&lt;/span&gt; 
&lt;span class="s"&gt;&amp;#39;git://github.com/timtadh/swork.git@&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;#egg=swork&amp;#39;&lt;/span&gt; 
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@command&lt;/span&gt; 
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getopt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;sr:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sudo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;src=&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;release=&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;commit=&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;GetoptError&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error_codes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;option&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt; 
    &lt;span class="n"&gt;src_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SRC_DIR&lt;/span&gt; 
    &lt;span class="n"&gt;release&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RELEASE&lt;/span&gt; 
    &lt;span class="n"&gt;commit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; 
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;--sudo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
            &lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; 
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;--release&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
            &lt;span class="n"&gt;release&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt; 
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;--src&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,):&lt;/span&gt; 
            &lt;span class="n"&gt;src_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt; 
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;--commit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,):&lt;/span&gt; 
            &lt;span class="n"&gt;commit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;release&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isdigit&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt; &lt;span class="c"&gt;## Check if it is a version &lt;/span&gt;
        &lt;span class="n"&gt;release&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;release&lt;/span&gt; 
    &lt;span class="n"&gt;release&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;origin/&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;release&lt;/span&gt; &lt;span class="c"&gt;## assume it is a branch&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;commit&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c"&gt;## if a commit is supplied use that &lt;/span&gt;
        &lt;span class="n"&gt;cmd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;UPDATE_CMD&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;src_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;commit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;cmd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;UPDATE_CMD&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;src_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;release&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sudo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sudo &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
        &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://www.pip-installer.org/en/latest/index.html"&gt;http://www.pip-installer.org/en/latest/index.html&lt;/a&gt; "A Python package installer."&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Mon, 23 May 2011 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2011-05-23:how-to-write-self-updating-python-programs-using-pip-and-git.html</guid></item><item><title>Announcing swork - Simplify your Shell Configuration</title><link>http://hackthology.com/announcing-swork-simplify-your-shell-configuration.html</link><description>&lt;p&gt;If you are like me, and if you are reading this you may very well be,
you spend an inordinate amount of time juggling inane details, like
shell environment variables, while programming. Now there is nothing
wrong with setting, exporting, and then unsetting variables, mounting
and unmounting FUSE partitions, starting routine backups, and so on but
it does get tedious after a while. Eventually, you may have written a
host of scripts to solve these various problems. Today I present
&lt;a href="https://github.com/timtadh/swork"&gt;swork&lt;/a&gt; (or start work) a command line
utility to help manage these little one off scripts with ease.&lt;/p&gt;
&lt;h1&gt;Don't Repeat Yourself&lt;/h1&gt;
&lt;p&gt;A typical pattern seen in scripts, such as virtualenv's activate script,
is the storing of old environment variables such that the changes made
by the script can be easily undone. Every non-trivial script I write
seems to include this detail, and I am tired of it. It is boring, it is
simple, and it is abstract-able. So I have abstracted. swork frees you
from needing to write this code. When you want to go back the original
state of the shell, you simply type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;swork&lt;/span&gt; &lt;span class="n"&gt;restore&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As long as you have run swork at some point in the past on the current
shell (or rather the current bash process) swork will restore
environment of the shell to the state it originally found it.&lt;/p&gt;
&lt;h1&gt;Writing Configuration Scripts&lt;/h1&gt;
&lt;p&gt;While, swork saves you the trouble of saving and restoring variables,
you still have to write the scripts to run. Fortunately, this is very
easy. You simply write a bash script (or any executable) then you add it
to the \~/.sworkrc (located conveniently in your home directory).&lt;/p&gt;
&lt;h3&gt;Example setenv file:&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env bash&lt;/span&gt;

&lt;span class="nb"&gt;source &lt;/span&gt;env/bin/activate &lt;span class="c"&gt;# activate a virtualenv&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;SOMEVAR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;new value&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;some/new/stuff&amp;quot;&lt;/span&gt;:&lt;span class="nv"&gt;$PATH&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;more/new/stuff&amp;quot;&lt;/span&gt;:&lt;span class="nv"&gt;$PYTHONPATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;example .sworkrc file:&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;project1&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;root&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/path/to/project1/root&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;start_cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;source /path/to/project1/root/then/setenv&amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;teardown_cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;echo &amp;#39;project1 teardown&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;project2&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;root&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/path/to/project2/root&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;start_cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;source /path/to/scripts/project2_setenv&amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;teardown_cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;echo &amp;#39;project2 teardown&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Wrapping Up&lt;/h1&gt;
&lt;p&gt;swork makes it easy for you to manage the environment on you shell
allowing you to switch contexts with minimum fuss. It currently
implements the minimum functionality to be useful, but is just waiting
for your feature request!&lt;/p&gt;
&lt;p&gt;check it out on github:
&lt;a href="https://github.com/timtadh/swork"&gt;https://github.com/timtadh/swork&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Wed, 18 May 2011 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2011-05-18:announcing-swork-simplify-your-shell-configuration.html</guid></item><item><title>Grammars, Ambiguity, and Expressibility</title><link>http://hackthology.com/grammars-ambiguity-and-expressibility.html</link><description>&lt;p&gt;Last night I gave a talk at CWRU Hacker Society about formal languages.
This is the first talk in a series of lectures I will be giving on
compilers. Unfortunately, unlike my regular expression talk I did not
get a recording of the audio. I may do a write up of exactly what I
talked about later. Until then enjoy my "slides."&lt;/p&gt;
&lt;iframe
  src="http://crocodoc.com/04Z8BP?embedded=true" height="600" width="100%"&gt;
&lt;/iframe&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Thu, 17 Feb 2011 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2011-02-17:grammars-ambiguity-and-expressibility.html</guid></item><item><title>Interpreting the Free Software Movement as Religion</title><link>http://hackthology.com/interpreting-the-free-software-movement-as-religion.html</link><description>&lt;p&gt;&lt;em&gt;A person should aspire to live an upright life openly with pride, and
this means saying No to proprietary software.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;- RMS&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The Free Software movement which began in earnest twenty-five years ago
has become one of the most quietly influential movements of the Internet
age. Today, many social phenomenas occurring in our networked world,
such as Wikileaks, can be understood more completely by understanding
the Free Software movement. The Free Software movement can be usefully
analyzed from many perspectives however, this paper will use the lens of
religion. Specifically, the movement will be analyzed from the context
of selected writing from its founder, Richard M. Stallman, using the
categories defined by &lt;a href="http://en.wikipedia.org/wiki/Mircea_Eliade" title="Mircea Eliade"&gt;Mircea
Eliade&lt;/a&gt;.
Through the use of Eliade's categories one understands Stallman to be
demarcating the sacred from the profane in an attempt to return to an
archaic past.&lt;/p&gt;
&lt;h3&gt;Eliade's Terms and Categories&lt;/h3&gt;
&lt;p&gt;In his 1957 treatise, The Sacred and the Profane , Mircea Eliade creates
categories, such as sacred vs profane, and attempts to fit many
different religious traditions into the categories. Methodologically
this approach has a serious shortcoming, the supporting traditions do
not always fit well into the chosen categories. However, since Eliade
has defined broadly useful, even if not universally applicable,
categories for interpreting religious traditions. The utilization of
these categories highlights the religious nature of the Free Software.&lt;/p&gt;
&lt;p&gt;Eliade's primary purpose in his treatise is to discuss the experiential
demarcation between the sacred and the profane. Eliade defines the
sacred in two senses. The first is recursive: the sacred is the opposite
of the profane.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; In this sense one can place objects
into two categories, sacred and profane, as long as they don't overlap.
Eliade clarifies this slightly through use of Rudolf Otto's term, the
wholly other,&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt; indicating the sacred manifests itself as
wholly different than the profane. By using Otto's language, Eliade
indicates the sacred has an element to the divine.&lt;/p&gt;
&lt;p&gt;For the purposes of this paper we will stipulatively take the divine to
mean: that which seems to the individual to have a numinous quality. An
object which has a numinous quality is one which seems irreducible and
the individual thus feels a creature dependence
towards.&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt; Thus, the sacred is the manifestation of the
numinous into the corporeal. The profane, on the other hand, is the
common, that which seems to be understandable. These stipulative
definitions conform to Eliade's requirements: the sacred is opposite of
the profane, and the sacred is wholly different than the profane.&lt;/p&gt;
&lt;p&gt;In addition to his categories, sacred and profane, Eliade defines two
related categories: archaic society and modern society. The man who
lives in archaic society, homo religiosus, seeks to exist as much as
possible in or around the sacred.&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; In contrast the man
of modern societies, modern man, exists in a desacralized
environment.&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt; The modern man depending on his
temperament may look back to the religious man either romantically or
derisively. Thus, modern and archaic do not indicate, as they
traditionally do, an essential ordering or time line. It may be that
archaic and modern coexist with each look back to the other as a
mythical past while eagerly looking forward to a time when man is more
or less religious.&lt;/p&gt;
&lt;h3&gt;The Free Software Movement&lt;/h3&gt;
&lt;p&gt;What is the Free Software movement? How can it be understood as
religious, using the terms and categories defined by Eliade? The Free
Software movement was started in 1984 with the publication of the GNU
Manifesto by Richard M. Stallman. Stallman had become disgusted with the
unethical nature of software and computer usage and sought a return to
an earlier time where users freely shared and modified programs. To
enable this return, he set about to create an ecosystem of software
which was protected from being made proprietary. In many ways Stallman
succeeded: today there is large amount of Free Software available. Every
computer user unwittingly uses such software on a daily basis, and
companies such as Google and Facebook could not exist without Free
Software.&lt;/p&gt;
&lt;p&gt;What unethical nature was Stallman disgusted with? The answer lies in
the GNU Manifesto where Stallman states, I consider that the golden rule
requires that if I like a program I must share it with other people who
like it. Software sellers want to divide the users and conquer them,
making each user agree not to share with others. I refuse to break
solidarity with other users in this way.&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt; It seems to
Stallman he should obey the golden rule. One may guess from the text
Stallman would define the golden rule as a principle of reciprocity.
Thus, if an individual likes a program and would want another to share
it with him he is ethically required to share the program with another
as well. Furthermore, an individual must not restrict those he shares
software with from further sharing the software, or from modifying the
software to suit their needs better. To prevent such sharing and
modification would violate his golden rule.&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;It is important to note Stallman does not believe that all things should
be shared alike. He only considers such freedom ethical where there is
no harm done to the person who shares by sharing. Stallman states:
"Owners say that they suffer harm or economic loss when users copy
programs themselves. But the copying has no direct effect on the owner,
and it harms no one.&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt; Thus, Stallman believes that
copying a program does not harm the original owner, because the owner
does not loose the use of the program because a copy is made. Such
copies can be made indefinitely. In the same way Stallman defends the
right to modify software: whether you run or change a program I wrote
affects you directly and me only indirectly. Whether you give a copy to
your friend affects you and your friend much more than it affects me. I
shouldnt have the power to tell you not to do these things. No one
should.&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt; Thus, Stallman has constructed his own ethical
system based on how the golden rule seemed to him.&lt;/p&gt;
&lt;p&gt;To explain and understand his movement, Stallman constructed a founding
narrative. The narrative begins in ancient times when copyright as a
concept did not exist. Stallman explains that in those times the roles
of authors, copiers, scribes, and commentators were muddled. Everyone
who participated in written culture freely copied, improved, and
commented on previous works.&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt; Stallman holds this
copyright-free society up as the exemplar from our historical past for
how one should relate to written work.&lt;/p&gt;
&lt;p&gt;Stallman continues his narrative by connecting the experiences of early
computer programmers (his in particular) to the copyright-free society
detailed above. While copyright existed when computing culture began in
the 40's and 50's it was not yet universally applied to computer source
code. Stallman participated in this society when he joined the MIT
Artificial Intelligence Lab in 1971: When I started working at the MIT
Articial Intelligence Lab in 1971, I became part of a software-sharing
community that had existed for many years. Sharing of software was not
limited to our particular community; it is as old as computers, just as
sharing of recipes is as old as cooking. But we did it more than
most.&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt; Stallman holds his early experiences in the AI
Lab as a second exemplar for the proper orders of society, where
software is freely shared, edited, commented, and ported.&lt;/p&gt;
&lt;p&gt;However, Stallman's perfect society eventually fell into disrepair.
Programmers were asked to sign software licenses and non disclosure
agreements when the university purchased new equipment and software. To
Stallman these events had a chaogonic&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt; quality: This
meant that the first step in using a computer was to promise not to help
your neighbor. A cooperating community was forbidden. The rule made by
the owners of proprietary software was, 'If you share with your
neighbor, you are a pirate. If you want any changes, beg us to make
them.'&lt;sup id="fnref:13"&gt;&lt;a class="footnote-ref" href="#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt; Thus, the ideal of community which had obeyed
the golden rule began to unravel. No longer could a programmer freely
help his neighbor, no longer could a programmer freely fix bugs, no
longer could a programmer port software to new platforms. The
programmers were now at the mercy of contracts and legal agreements.&lt;/p&gt;
&lt;p&gt;In the depths of this disarray, Stallman experienced a heirophany,
Eliade's term for the sacred manifesting itself. It began with the AI
Lab being gifted a printer from Xerox. However, despite giving them the
printer, Xerox refused to share the code for the
driver.&lt;sup id="fnref:14"&gt;&lt;a class="footnote-ref" href="#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt; Unfortunately, the driver had bugs in it.
When Stallman offered to fix the bugs if they gave him their code, they
refused. The experience was transformative. Stallman could no longer
accept the status quo of license and non-disclosure agreements. He set
out to change the world, so he could return to an ideal society where
programmers helped their neighbors.&lt;sup id="fnref:15"&gt;&lt;a class="footnote-ref" href="#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Thus, according to Stallman's narrative detailed above, Stallman set out
to purge his life of the corrupting influence of proprietary software.
Unfortunately, to rid himself of proprietary software he needed to
create a new ecosystem of Free Software. So he quit his job at MIT and
began working on several Free Software programs. Stallman states: I
realized that I was elected to do the job.&lt;sup id="fnref:16"&gt;&lt;a class="footnote-ref" href="#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt; Thus, he
began the GNU project to create a Free operating system and ecosystem of
software. Without, such an ecosystem Stallman feels one cannot live an
upright life as a programmer.&lt;sup id="fnref:17"&gt;&lt;a class="footnote-ref" href="#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Therefore, the Free Software movement seeks to establish an alternative
reality where all software written is Free. Users are free to modify and
redistribute software. No one is free to limit another's use of
software. This movement can be understood using Eliade's categories of
sacred vs profane, and archaic society vs modern society.&lt;/p&gt;
&lt;h3&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;The copyright free societies Stallman references in his narrative
parallel the Eliadian concept of the archaic society. In both cases
these societies are held up as exemplars of what it means to be truly
religious, to be a homo religiosus. In Stallman's pre-copyright society,
programmers shared with each other freely, they modified programs
without hesitation; unwittingly they obeyed his golden rule and helped
their neighbors. In modern society programmers no longer share code and
modify programs. They are prevented from doing so. Thus, they no longer
obey the golden rule. By not obeying the golden rule they have become
corrupt.&lt;sup id="fnref:18"&gt;&lt;a class="footnote-ref" href="#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Stallman's narrative fits into Eliade's categories of the modern society
vs. the archaic society. Stallman represents himself as a truly
religious person living in the modern society seeking to return to the
archaic society. His method for returning to the archaic society is to
resacralize the world.&lt;/p&gt;
&lt;p&gt;To sacralize, one must have a concept of something that is sacred vs.
something that is profane. For to sacralize one make the the profane
sacred. For Stallman, Free Sofware itself is sacred. Free Software is
opposite the profane proprietary software. Proprietary software cannot
be shared and it cannot be modified. Free Software can be explicitly
shared and modified. Free Software also manifests a numinous quality.
Specifically, Free Software is the revelation of the ideal divine
society today. In the ideal society all software is Free Software. To
have Free Software in present society is to experience the revelation of
the ideal. Thus, Free Software is not just sharable and modifiable it
also sacred. Free Software is Sacred Software.&lt;/p&gt;
&lt;p&gt;Stallman desires to return to his ideal archaic society where
programmers were more religious and users could share and modify
programs at will. To bring about the return of the archaic society he
must resacralize the present profane society. To do so he created the
GNU Project to manifest Sacred Software into the present profane space.
Thus, Stallman marks off the sacred, Free Software, from the profane,
proprietary software, in an attempt to return humanity to the ideal
society.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The Free Software movement can be understood as a religious movement
using Eliade's terms and categories. Stallman basis his movement on his
understanding of the golden rule. He uses his understanding of the rule
to construct an ethical system for the production and use of software.
Stallman then constructs a narrative to explain how society has moved
from a religious ethical past to a profane present. To return society to
the ideal past, Stallman attempts resacralize the present society by
creating Free Software. Free Software is sacred. By introducing Free
Software into present society, society becomes more sacred. The
utilization of Eliade's categories clarified the religious aspects of
the Free Software movement.&lt;/p&gt;
&lt;hr /&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Eliade, M.  &lt;a href="http://www.amazon.com/Sacred-Profane-Nature-Religion/dp/015679201X?tag=dudugo-20"&gt;&lt;em&gt;The Sacred and the Profane: The Nature of Religion&lt;/em&gt;&lt;/a&gt; trans. Trask, W. Harcourt Inc. New York. 1957. pg 10. Hearafter: &lt;em&gt;Sacred and Profane&lt;/em&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;em&gt;Ibib&lt;/em&gt;. pg 2. and Otto, R.  &lt;a href="http://www.amazon.com/Idea-Holy-R-Otto/dp/0195002105/"&gt;&lt;em&gt;The Idea of the Holy&lt;/em&gt;&lt;/a&gt; trans.  Harvey, J. Oxford University Press, New York. 1958. pg 25.  Hereafter: &lt;em&gt;The Holy&lt;/em&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;em&gt;The Holy&lt;/em&gt; pg. 6-7&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;em&gt;Sacred and Profane&lt;/em&gt; pg. 12, 15&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;em&gt;Sacred and Profane&lt;/em&gt; pg. 17&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Stallman, R. M.  &lt;a href="http://shop.fsf.org/product/free-software-free-society/"&gt;&lt;em&gt;Free Software, Free Society: Selected Essays of Richard M. Stallman.&lt;/em&gt;&lt;/a&gt; GNU Press, Boston, 2002. pg. 34. Hereafter: &lt;em&gt;Free Software&lt;/em&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;See &lt;em&gt;Free Software&lt;/em&gt; pg. 43 for a discussion of the precise meaning of Free Software.&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" rev="footnote" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 48&amp;#160;&lt;a class="footnote-backref" href="#fnref:8" rev="footnote" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 49&amp;#160;&lt;a class="footnote-backref" href="#fnref:9" rev="footnote" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 39&amp;#160;&lt;a class="footnote-backref" href="#fnref:10" rev="footnote" title="Jump back to footnote 10 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 17&amp;#160;&lt;a class="footnote-backref" href="#fnref:11" rev="footnote" title="Jump back to footnote 11 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;Chaos creating, antonym of Cosmogonic. See Beal, T. K.  &lt;a href="http://www.amazon.com/Religion-Its-Monsters-Timothy-Beal/dp/0415925886"&gt;&lt;em&gt;Religion and its Monsters&lt;/em&gt;.&lt;/a&gt; Routledge, New York, 2002.&amp;#160;&lt;a class="footnote-backref" href="#fnref:12" rev="footnote" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:13"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 18&amp;#160;&lt;a class="footnote-backref" href="#fnref:13" rev="footnote" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:14"&gt;
&lt;p&gt;A driver is a piece of software which allow the computer to communicate with a piece of hardware. Every piece of hardware has a unique communication protocol, necessitating many different drivers.&amp;#160;&lt;a class="footnote-backref" href="#fnref:14" rev="footnote" title="Jump back to footnote 14 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:15"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 19&amp;#160;&lt;a class="footnote-backref" href="#fnref:15" rev="footnote" title="Jump back to footnote 15 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:16"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 19,20&amp;#160;&lt;a class="footnote-backref" href="#fnref:16" rev="footnote" title="Jump back to footnote 16 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:17"&gt;
&lt;p&gt;&lt;em&gt;Free Software&lt;/em&gt; pg. 57&amp;#160;&lt;a class="footnote-backref" href="#fnref:17" rev="footnote" title="Jump back to footnote 17 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:18"&gt;
&lt;p&gt;For an example of Stallman using such language see for instance &lt;em&gt;Free Software&lt;/em&gt; pg. 130&amp;#160;&lt;a class="footnote-backref" href="#fnref:18" rev="footnote" title="Jump back to footnote 18 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Wed, 08 Dec 2010 00:00:00 -0500</pubDate><guid>tag:hackthology.com,2010-12-08:interpreting-the-free-software-movement-as-religion.html</guid></item><item><title>Lessons Learned While Implementing a B+Tree</title><link>http://hackthology.com/lessons-learned-while-implementing-a-btree.html</link><description>&lt;p&gt;B+Trees are complex disk based trees used to index large amounts of
data. They are used in everything from file systems, to relation
databases, to new style databases gaining popularity today. Sometimes a
domain specific application needs to index a large amount of data, but
cannot use a traditional database, or one of the NoSQL databases. In
such instances the development team needs to roll their own indices.
Here is an introduction to the B+Tree (one of the indexes my team
created) and lessons I learned while implementing it.&lt;/p&gt;
&lt;h2&gt;Introduction to the B+Tree&lt;/h2&gt;
&lt;p&gt;B+Trees are one of the fundamental index structures used by databases
today. This includes new style SQL free databases. The B+Tree popularity
stems from their performance approaching optimal performance in terms of
disk reads for range queries in a 1 dimensional space. What is a 1
dimensional space when talking about computer data which could be
anything (not just numbers)? It is any collection of objects where the
user accesses the object using only one attribute at a time.&lt;/p&gt;
&lt;p&gt;For example if we have an object which has X, Y, and Z as attributes
queries would only take place on X, or Y, or Z, but never on XY, or YZ,
or XZ, or XYZ. A collection where multiple attributes are used to access
the data elements are known as multidimensional spaces. For these spaces
there are many other structures which have better performance than
B+Trees.&lt;/p&gt;
&lt;p&gt;B+Trees perform particularly well (in comparison to some other indices)
when executing range queries. A range query is typically expressed as
inequality such as "give me all strings between 'blossom' and 'brunet.'"&lt;/p&gt;
&lt;p&gt;When I say their performance is approaching optimal in number of disk
reads what do I mean? Why are we not measuring performance in number of
instructions executed (like we do when we analyze a binary search)? In
memory algorithms and structures like sorted arrays and binary searches
are largely bound by the number CPU cycles it takes to execute the
algorithm. We usually neglect CPU cache performance and memory locality
when analyzing them, arguing these are constant in terms of the
asymptotic performance of the algorithm. However, for a disk based
structure like B+Trees the time it takes to read (or write) to a disk
becomes the dominant term, since disks are extremely slow in comparison
to main memory. Therefore for disk structures we analyze their
performance in terms of disk reads/writes.&lt;/p&gt;
&lt;h2&gt;Basic Structure of the B+ Tree&lt;/h2&gt;
&lt;p&gt;While I will not give a through explanation of the exact structure and
properties of B+ Tree (I leave that to algorithm and database textbooks
by the likes of Knuth, Sedgewick, and Ullman), I will describe its basic
structure.&lt;/p&gt;
&lt;p&gt;A B+Tree is best thought of as a key-value store. It is structured as a
generalized tree. Instead of having only one key in each node it has N
keys in each node, where N is referred to as the &lt;em&gt;degree&lt;/em&gt; of the B+Tree.
In the B+Tree there are 2 kinds of nodes, interior nodes, and exterior
(leaf) nodes. The interior nodes hold keys and pointers to nodes. The
exterior nodes hold keys and their associated values. This indicates
that the interior nodes have a different (usually higher) degree than
the exterior nodes.&lt;/p&gt;
&lt;p&gt;The reason the tree is structured this way is because it is rooted in
the nature of disk access. Disks to not return 1 byte when you ask for 1
byte instead they return what is called the disk block to which that
byte belongs, the operating system then sorts out which byte it is that
you need. B+Tree exploit the situation by making their nodes fit exactly
into the size of one disk block. Since the degree of the interior nodes
is high, this makes the tree extremely wide, which is a good thing since
it means fewer disk reads to find the value associated with any one key.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
  &lt;a href="/images/bptree1.png"&gt;
    &lt;img
      alt="Example B+Tree"
      src="/images/bptree1.png"/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1. &lt;strong&gt;An Example B+Tree&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In figure 1 you can see an example B+Tree. For this illustration I
neglect showing the values, and have the order of the interior nodes
equal to the order of the exterior nodes. In general this will not be
the case. One thing to note in this simple example is how the exterior
nodes are chained together in order. This is why it is efficient to
execute a range query on the B+Tree. One can simply find the first key
in the range, and then traverse the leaf nodes until the last key has
been found.&lt;/p&gt;
&lt;h2&gt;Implementing the B+Tree&lt;/h2&gt;
&lt;p&gt;I made the decision to use TDD (Test Driven Development) for
implementing the B+Tree. TDD has a lot of pluses when trying to create a
data structure of any kind. When implementing a data structure one
typically knows exactly how the structure should function, what it
should do, and what it should never do. By writing tests first, you can
ensure that when you finish a method, it actually works. This speeds
development time especially since you already know how the structure
should function. It makes it quicker to find bugs, and to battle test
the B+Tree. Since I have released the B+Tree to the rest of my team to
use, there has not yet been a bug filled against it.&lt;/p&gt;
&lt;p&gt;So knowing that we are using TDD, and knowing what the structure is and
how it performs. What is the best way to begin implementing this complex
structure? The way I started was to create a general structure called a
block file. My block files abstracted the notion of reading and writing
blocks (and buffering them). I also created objects to model a block
that could contain either keys and pointers, or keys and records
(instead of values from here on I will use the term records). Actually
my blocks are even more general than that as I intend to reuse them for
other disk based index structures like linear hashing in the future.&lt;/p&gt;
&lt;p&gt;I also created what I called a ByteSlice. My ByteSlice was an array of
bytes of arbitrary length. I use it to represent, keys, records, and
pointers; everything in the B+Tree. My ByteSlice implemented a
comparator, so it could be sorted, and conversions from integer types of
various lengths to the ByteSlice and back again. By implementing this
general type my B+Trees can easily deal with any kind of data and
perform in exactly the same way.&lt;/p&gt;
&lt;p&gt;After the infrastructure was created I began working on my first
iteration of the B+Tree. The first iteration was based on the algorithms
give by Robert Sedgewick in his excellent book "Algorithms in C++." I
managed to get this implementation up, running, and fully tested in a
matter of days. However, the version given by Sedgewick which inspired
my implementation did not deal gracefully with duplicate keys. Thus, I
need to invent my own way of handling duplicate keys.&lt;/p&gt;
&lt;h2&gt;Approaches to Handling Duplicate Keys in B+Trees&lt;/h2&gt;
&lt;p&gt;There are several different ways of handling duplicate keys. One way is
use an unmodified insert algorithm which allows duplicate keys in blocks
but is otherwise unchanged. The issue with a structure such as this is
the search algorithm must be modified to take into account several
corner cases which arise. For instance one of the invariants of a B+Tree
may be violated in this structure. Specifically if there are many
duplicate keys, a copy of one of the keys may be in a non-leaf block.
However, the key may appear in blocks that which appear logically before
the block which is pointed at by the key in the internal block. Thus the
search algorithm must be modified to look in the previous blocks to the
one suggested by the unmodified search algorithm. This will slow down
the common case for search.&lt;/p&gt;
&lt;p&gt;There is another issue with this straight forward implementation, if
there are many duplicate keys in the index, the index size may be taller
than necessary. Consider a situation were for each unique key there are
perhaps hundreds of duplicates, the index size will be proportional to
the total number of keys in the main file, however, you only need to
index an index on the unique keys. One of the files indexed in our
program will be indexing has such characteristics to its data. It
indexes strings (as the keys) with associated instances where those
strings show up in our documents. There can be hundreds to thousands of
instances of each unique string.&lt;/p&gt;
&lt;p&gt;Therefore the approach I took was to store only the unique keys in the
index, and have the duplicates captured in overflow blocks in the main
file. An example of such a tree can be seen in figure 2. Consider key 6;
there are 5 instances of this key in the tree. The tree is order 3,
indicating the keys cannot all fit in one block. To handle this
situation an overflow block is chained to the block which is indexed by
the tree structure. The overflow block then points to the next relevant
block in the tree.&lt;/p&gt;
&lt;div style="text-align:center"&gt;
  &lt;a href="/images/bptree2.png"&gt;
    &lt;img
      alt="Example B+Tree with Duplicate Keys"
      src="/images/bptree2.png"/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Figure 2. &lt;strong&gt;A B+Tree with duplicate keys and overflow blocks.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To create a structure such as this, the insert algorithm had to be
modified. Like the previous version these modifications do not come
without a cost, in particular the invariant which states all block must
be at least half full has been relaxed. This is not true in this B+Tree,
some blocks like the one containing key number 7, are not half full.
This problem could be partially solved by using key rotations to balance
the tree better. However, there are still corner cases where there would
be a block which is under-full. One such corner case includes when a key
falls between two keys which have overflow blocks. It must then be in a
block by itself, since this B+Tree has the invariant which state that if
a block is overflowing it can only contain one unique key. In the future
we would like to implement key rotations to help partially alleviate the
problem of under-full blocks.&lt;/p&gt;
&lt;p&gt;The advantage of this approach to B+Trees with duplicate keys is the
index size is small no matter how the ratio of duplicate keys to the
total number of keys in the file. This property allows our searches to
be conducted quicker. Since the overflow blocks are chained into the
B+Tree structure we still have the property of being able to fast
sequential scans. One consequence is we have defined all queries on our
B+Trees to be range queries. This is fine because all of our queries
were already range queries. In conclusion we relax the condition that
all blocks must be at least half full to gain higher performance during
search.&lt;/p&gt;
&lt;h2&gt;The Lessons Learned&lt;/h2&gt;
&lt;p&gt;The biggest lessons learned through the journey:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The Value of Test Driven Development&lt;/em&gt; The impact TDD had on the
    development time of the B+Tree vs. other structures in the project
    cannot be understated. TDD dramatically reduced the time it took to
    develop the structure (from over a month for some of the other
    structures to under two weeks for the B+Tree), and has ensured
    reliability of the structure once it entered production.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The Value of the Iterative Approach&lt;/em&gt; By starting simple, testing,
    and then adding complexity I was able to get a better grasp on the
    problems posed by the modifications we needed to make to the
    structure. For instance I before I tried method 2 for duplicate
    keys, I modeled the data we would be putting into our tree and
    visualized the resulting structure. I found the structure would
    perform poorly. However, the same code allowed my to visualize
    method 2 and see that it would perform well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Visualizations as Part of Development&lt;/em&gt; Writing code allowing you to
    visualize the structure you are developing can really help you find
    bugs quicker. The best tool to do this with is graphviz. The
    pictures used in this blog where generated as part of unit test
    cases. My building a visualization framework early as part of your
    unit tests you can further reduce development time. When a bug a
    appears it can be enormously helpful to visualize the actual
    structure of the tree at the time the bug manifested.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;When the right choice for your project isn't DBMS, but you still need to
index large data, don't fear you can write the index structures
yourself. By using TDD, iterating, and visualizing as you go you can
ensure the index structure you create will perform well, and will never
get into an incorrect state. Databases are not a black box, and they are
not always the right answer. When required you can create you own
system.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Henderson</dc:creator><pubDate>Sat, 10 Apr 2010 00:00:00 -0400</pubDate><guid>tag:hackthology.com,2010-04-10:lessons-learned-while-implementing-a-btree.html</guid></item></channel></rss>